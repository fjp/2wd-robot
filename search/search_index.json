{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to DiffBot Documentation This project guides you on how to build an autonomous two wheel differential drive robot. The robot is equipped with a Raspberry Pi 4 B running ROS Noetic middleware on Ubuntu Mate 20.04. With a motor driver and two actuators it can drive autonomously to a desired location while sensing its environment using sensors, such as a camera and an ultrasonic ranger to avoid obstacles. Speed sensors combined with an inertial measurement unit (IMU) are used for localization. The project is split into multiple parts, to adress the following main aspects of the robot. Part list and the theory behind the parts. Assembly of the robot platform and the components. Raspberry Pi 4 B setup using ROS Noetic, which will be the brain of the robot. Modeling the Robot in Blender and URDF to simulate it in Gazebo. ROS packages and nodes: Hardware drivers to interact with the hardware components High level nodes for perception, navigation, localization and control. Use the menu on the left to learn more about the ROS packages and other components of the robot. Note Using a Jetson Nano instead of a Raspberry Pi is also possible. Source Code The source code for this project can be found in this GitHub repository . References Helpful resources to bring your own robots into ROS are: Understand ROS Concepts Follow ROS Tutorials such as Using ROS on your custom Robot Books: Robot Operating System (ROS) for Absolute Beginners from Apress by Lentin Joseph Programming Robots with ROS A Practical Introduction to the Robot Operating System from O'Reilly Media Mastering ROS for Robotics Programming Second Edition from Packt Elements of Robotics Robots and Their Applications from Springer","title":"Home"},{"location":"#welcome-to-diffbot-documentation","text":"This project guides you on how to build an autonomous two wheel differential drive robot. The robot is equipped with a Raspberry Pi 4 B running ROS Noetic middleware on Ubuntu Mate 20.04. With a motor driver and two actuators it can drive autonomously to a desired location while sensing its environment using sensors, such as a camera and an ultrasonic ranger to avoid obstacles. Speed sensors combined with an inertial measurement unit (IMU) are used for localization. The project is split into multiple parts, to adress the following main aspects of the robot. Part list and the theory behind the parts. Assembly of the robot platform and the components. Raspberry Pi 4 B setup using ROS Noetic, which will be the brain of the robot. Modeling the Robot in Blender and URDF to simulate it in Gazebo. ROS packages and nodes: Hardware drivers to interact with the hardware components High level nodes for perception, navigation, localization and control. Use the menu on the left to learn more about the ROS packages and other components of the robot. Note Using a Jetson Nano instead of a Raspberry Pi is also possible.","title":"Welcome to DiffBot Documentation"},{"location":"#source-code","text":"The source code for this project can be found in this GitHub repository .","title":"Source Code"},{"location":"#references","text":"Helpful resources to bring your own robots into ROS are: Understand ROS Concepts Follow ROS Tutorials such as Using ROS on your custom Robot Books: Robot Operating System (ROS) for Absolute Beginners from Apress by Lentin Joseph Programming Robots with ROS A Practical Introduction to the Robot Operating System from O'Reilly Media Mastering ROS for Robotics Programming Second Edition from Packt Elements of Robotics Robots and Their Applications from Springer","title":"References"},{"location":"DG01D-E-motor-with-encoder/","text":"Motor with Wheel Encoder The DG01D-E is a single hobby motor with a hall speed encoder. This motor requires a voltage between 3-9V, has a gearbox ratio of 1:48 and a speed of 90RPM at 4.5V. The voltage between positive and negative is determined according to the power supply voltage of the single chip microcomputer used, generally 3.3V or 5V is used. The hall sensor can sense the North and South poles of its magnetic plate. When the hall sensor senses the South of the magnetic plate, the hall output will result in a high level. Meanwhile the North is the inverse and, when sensed, the hall output will result a low level. Terminal Pin Layout The pins on the product are as follows, when looking at the connector on the housing, motor down/connector up, from right to left. The colors correspond to the included cable when plugged in to the connection slot. G (Blue): hall power negative H1 (Green): hall H1 output signal, square wave H2 (Yellow): hall H2 output signal, square wave V (Orange): hall power positive M+ (Red): motor positive pole M- (Brown): motor negative pole The following image shows the motor from its side with the corresponding pin descriptions: DG01D-E Motor with encoder pin description. Wheel Encoder Measurements This section shows oscilloscope waveform measurements of the quadrature encoder in the DG01D-E motor. The motor is connected to the Grove I2C Motor Driver that is powerd with 10 VDC. The motor_example.py applies 50-100% of the 10 VDC which leads to the following output voltages on the motor: Voltage sweep measurements - 0:00 Forward Speed 50: 6.5 VDC - 0:12 Back Speed 50: 6.5 VDC - 0:23 Forward Speed 60: 6.9 VDC - 0:34 Back Speed 60: 6.9 VDC - 0:46 Forward Speed 70: 7.2 VDC - 0:56 Back Speed 70: 7.2 VDC - 1:07 Forward 80: 7.3 VDC - 1:18 Back 80: 7.3 VDC - 1:29 Forward 90: 7.6 VDC - 1:41 Back 90: 7.6 VDC - 1:52 Forward 100: 7.9 VDC - 2:02 Back 100: 7.9 VDC At the bottom of the pico scope window the cycle time, duty cycle, high and low pulse width measurements are shown for both encoder signals. Oscilloscope is the PicoScope 3000 Series with 2 Channels. To download and install Pico Scope software on Linux refer to the documentation . Summary of installation instructions 1. Add repository to the updater 1 sudo bash -c 'echo \"deb https://labs.picotech.com/debian/ picoscope main\" >/etc/apt/sources.list.d/picoscope.list' 2. Import public key 1 wget -O - https://labs.picotech.com/debian/dists/picoscope/Release.gpg.key | sudo apt-key add - 3. Update package manager cache 1 sudo apt-get update 4. Install PicoScope 1 sudo apt-get install picoscope","title":"Motor and Encoder"},{"location":"DG01D-E-motor-with-encoder/#motor-with-wheel-encoder","text":"The DG01D-E is a single hobby motor with a hall speed encoder. This motor requires a voltage between 3-9V, has a gearbox ratio of 1:48 and a speed of 90RPM at 4.5V. The voltage between positive and negative is determined according to the power supply voltage of the single chip microcomputer used, generally 3.3V or 5V is used. The hall sensor can sense the North and South poles of its magnetic plate. When the hall sensor senses the South of the magnetic plate, the hall output will result in a high level. Meanwhile the North is the inverse and, when sensed, the hall output will result a low level.","title":"Motor with Wheel Encoder"},{"location":"DG01D-E-motor-with-encoder/#terminal-pin-layout","text":"The pins on the product are as follows, when looking at the connector on the housing, motor down/connector up, from right to left. The colors correspond to the included cable when plugged in to the connection slot. G (Blue): hall power negative H1 (Green): hall H1 output signal, square wave H2 (Yellow): hall H2 output signal, square wave V (Orange): hall power positive M+ (Red): motor positive pole M- (Brown): motor negative pole The following image shows the motor from its side with the corresponding pin descriptions: DG01D-E Motor with encoder pin description.","title":"Terminal Pin Layout"},{"location":"DG01D-E-motor-with-encoder/#wheel-encoder-measurements","text":"This section shows oscilloscope waveform measurements of the quadrature encoder in the DG01D-E motor. The motor is connected to the Grove I2C Motor Driver that is powerd with 10 VDC. The motor_example.py applies 50-100% of the 10 VDC which leads to the following output voltages on the motor: Voltage sweep measurements - 0:00 Forward Speed 50: 6.5 VDC - 0:12 Back Speed 50: 6.5 VDC - 0:23 Forward Speed 60: 6.9 VDC - 0:34 Back Speed 60: 6.9 VDC - 0:46 Forward Speed 70: 7.2 VDC - 0:56 Back Speed 70: 7.2 VDC - 1:07 Forward 80: 7.3 VDC - 1:18 Back 80: 7.3 VDC - 1:29 Forward 90: 7.6 VDC - 1:41 Back 90: 7.6 VDC - 1:52 Forward 100: 7.9 VDC - 2:02 Back 100: 7.9 VDC At the bottom of the pico scope window the cycle time, duty cycle, high and low pulse width measurements are shown for both encoder signals. Oscilloscope is the PicoScope 3000 Series with 2 Channels. To download and install Pico Scope software on Linux refer to the documentation . Summary of installation instructions 1. Add repository to the updater 1 sudo bash -c 'echo \"deb https://labs.picotech.com/debian/ picoscope main\" >/etc/apt/sources.list.d/picoscope.list' 2. Import public key 1 wget -O - https://labs.picotech.com/debian/dists/picoscope/Release.gpg.key | sudo apt-key add - 3. Update package manager cache 1 sudo apt-get update 4. Install PicoScope 1 sudo apt-get install picoscope","title":"Wheel Encoder Measurements"},{"location":"components/","text":"Part list and assembly of the robot platform and the components. Category Hardware Part Number Data Sheet & Info Accessories Case for Raspberry Pi 4 B Slim acrylic case for Raspberry Pi 4, stackable, rainbow/transparent BerryBase Micro SD Card SanDisk 64GB Class 10 SanDisk , Ubuntu 18.04 Image Robot Car Kit 2WD robot05 Instructions manual Power bank Intenso Powerbank S10000 Intenso Actuator (Deprecated) Gearbox motor DC Gearbox motor - \"TT Motor\" - 200RPM - 3 to 6VDC Adafruit DG01E-E Motor with encoder DG01E-E Hobby motor with quadrature encoder Sparkfun Board Raspberry Pi 4 B Raspberry Pi 4 B - 4 GB OEM Website Cables Jumper - Female to Female Jumper - Male to Male Micro USB - USB Cable Camera extension cable I2C 4 pin cable Electronics Fan Fan 30x30x7mm 5V DC with Dupont connector BerryBase I2C motor driver Grove - I2C Motor Driver Seeed Studio I2C Hub Grove - I2C Hub Seeed Studio Human Machine Interface OLED Display Grove OLED Display 0.96\" Seeed Studio LED Ring NeoPixel Ring 12x5050 RGB LED Adafruit Sensors Camera module Raspberry Pi - camera module v2.1 Raspberry Pi Ultrasonic ranger Grove - Ultrasonic Ranger Seeed Studio IMU Adafruit 9-DOF Absolute Orientation IMU Fusion Breakout - BNO055 Adafruit Odometry Joy-IT - LM393 Speed Sensor with H206 slot-type opto interrupter Joy-IT Order list Part Store Raspberry Pi 4 B (4 Gb) Amazon.com , Amazon.de SanDisk 64 GB SD Card Class 10 Amazon.com , Amazon.de Robot Smart Chassis Kit Amazon.com , Amazon.de SLAMTEC RPLidar A2M8 (12 m) Amazon.com , Amazon.de Grove Ultrasonic Ranger Amazon.com , Amazon.de Raspi Camera Module V2, 8 MP, 1080p Amazon.com , Amazon.de Grove Motor Driver seeedstudio.com , Amazon.de I2C Hub seeedstudio.com , Amazon.de Additional (Optional) Equipment Part Store PicoScope 3000 Series Oscilloscope 2CH Amazon.de VOLTCRAFT PPS-16005 Amazon.de Board - Raspberry Pi 4 B The main processing unit of the robot is a Raspberry Pi 4 B with 4 GB of RAM. Raspberry Pi 4 B - 4 GB RAM variant. Accessories and Electronics Case and Cooling To protect the Rasbperry Pi 4 B we choose a case that provides access to all its ports. The following images show a stackable acrylic case in rainbow colors. Stackable Rainbow Case for Raspberry Pi 4 B. With this case it is possible to install four heatsinks and apply a fan as cooling equipment for the electronics of the Raspberry Pi 4 B such as its ARM processor. Heatsinks and cooling fan for Raspberry Pi 4 B. SD Card The Raspberry Pi requires a medium to boot from. For this we will use a micro sd card because it is lightweight and easy to flash new operating systems. SanDisk Micro SD Card Class 10. Although a micro sd card won't last that long compared to an hard disk drive (HDD) or solid state disk (SSD) it is well suited for testing. Because sd cards are slower when reading and writing data you should make sure to choose a micro sd card with high performance ratings. For the Raspberry Pi a Class 10 micro sd card is recommended. Regarding speed, the Pi has a limited bus speed of approximately 20 MB/s ( source ) Robot Base The Robot Car Kit 2WD from Joy-IT (article no.: robot05) is used as the base for the autonomous mobile robot. Parts of the 2WD Robot Car Kit 05 from Joy-IT. The acrylic chassis has many holes which allow to mount a mounting plate that can hold different development boards. It allows also to mount a Raspberry Pi 4 B, which will be used in this project. Two wheels, hence 2WD, are included in the kit which can be attached to the motors that are provided too. A third caster wheel is provided which allows the robot to spin on the spot. This means the robot can be described by a holonomic model. The motors operate in a range between 3 to 6 Volts DC and make it possible to mount a punched disk for speed measurements. With that punched disk and additional speed sensors it is possible to implement odometry in ROS. To power the motors a battery compartment is available together with a switch to turn the robot on or off. Power Supplies As mentioned the robot will be equipped with a 5V/2.1A USB-C powerbank to supply the Raspberry Pi 4 B with 5 V. Power bank with 10.000 mAh from Intenso. To power the motors the provided battery compartment will be used, which holds four AA batteries \\(4 \\cdot 1.5\\text{V} = 6\\text{V}\\) . I2C Hub The Raspberry Pi provides just two I2C ports, which is why we will use a I2C hub. With the four port I2C hub from Grove it is possible to connect three I2C devices to a single I2C port of the Raspberry Pi Grove I2C Hub. Breadboard and GPIO Extension Cable Optional but helpful for testing is a breadboard and a GPIO extension cable suitable for the Raspberry Pi 4 B. Breadboard with GPIO extension cable. Sensors Sensors are used to sense the environment and to collect information of the current state. For this 2WD robot the sensors are categorized into perception and localization which are explained in the following two sections. Perception Perception sensors of the 2WD robot will be used to avoid collisions using ultrasonic rangers. Another use case is to detect and follow objects using a camera. Ultrasonic Ranger To avoid obstacles the robot will carry a Grove - Ultrasonic Ranger at the front. Grove Ultrasonic Ranger for obstacle avoidance. It is a non-contact distance measurement module which works at 40KHz and can be interfaced via a single GPIO . For example physical pin 11 of the Raspberry Pi connected to the SIG pin on the sensor can provide the PWM communication. Parameter Value/Range Operating voltage 3.2~5.2V Operating current 8mA Ultrasonic frequency 40kHz Measuring range 2-350cm Resolution 1cm Output PWM Size 50mm X 25mm X 16mm Weight 13g Measurement angle 15 degree Working temperature -10~60 degree C Trigger signal 10uS TTL Echo signal TTL The code that will be used to wrap this sensor as a ROS node can be found in the Grove Raspberry Pi repository on GitHub. As an alternative we could use the HC SR04 . Camera RPi Camera v2. Localization Inertial Measurement Unit An intertial measurement unit (IMU) measures the acceleration and orientation through gyroscopes directly. Other states such as the velocity can then be calculated. For this the Adafruit 9-DOF Absolute Orientation IMU Fusion Breakout - BNO055 is used. 9-DOF Absolute Orientation IMU Fusion Breakout - BNO055 from Adafruit. Odometry For the used odometry sensor see the section below Motor and Wheel Encoder Alternative Optical Sensor To estimate the change in position over time ([odometry](https://en.wikipedia.org/wiki/Odometry)) the robot will utilize an [optical speed sensor](https://en.wikipedia.org/wiki/Wheel_speed_sensor#Optical_sensor). Specifically the [Joy-IT Speed Sensor](https://joy-it.net/en/products/SEN-Speed) which combines a LM393 ([datasheet](http://www.ti.com/lit/ds/symlink/lm2903-n.pdf)) [comperator](https://en.wikipedia.org/wiki/Comparator) and a H206 slot-type opto interrupter. LM393 Speed Sensor from Joy-IT. Technical Specifications: - Dimensions: 32 x 14 x 7mm - Operating voltage: 3.3V to 5V (we will use 3.3V) - Two outputs: Digital (D0) and Analog (A0) References: https://dronebotworkshop.com/robot-car-with-speed-sensors/ Actuators Grove - I2C Motor Driver V1.3 Control To drive the two motors of the car kit we use the Grove - I2C Motor Driver V1.3 from Seeed Studio. Grove - I2C Motor Driver. Motor and Wheel Encoder The DG01E-E Hobby Motor has a quadrature encoder built in, which makes it easy to assemble the robot and saves space because of no additional (optical or magnetic) wheel encoders. DG01D-E Motor with wheel encoders. Alternative Brushed Gear Motor ### Brushed Gearbox Motor DC Gearbox motor - \"TT Motor\" - 200RPM - 3 to 6VDC. Human Machine Interface (HMI) The human machine interface is the layer between the user and the robot. OLED Display To update the user with status messages the robot has a 0.96 inch oled (organic ligth emitting diode) display. The oled display used is the Grove I2C 0.96 inch OLED display from Seeed Studio. Grove - I2C 0.96 inch OLED Display. The display is connected to the RPi via I2C on the physical pins 27 (scl) and 28 (sda), refere to the pinout . Library","title":"Components"},{"location":"components/#board-raspberry-pi-4-b","text":"The main processing unit of the robot is a Raspberry Pi 4 B with 4 GB of RAM. Raspberry Pi 4 B - 4 GB RAM variant.","title":"Board - Raspberry Pi 4 B"},{"location":"components/#accessories-and-electronics","text":"","title":"Accessories and Electronics"},{"location":"components/#case-and-cooling","text":"To protect the Rasbperry Pi 4 B we choose a case that provides access to all its ports. The following images show a stackable acrylic case in rainbow colors. Stackable Rainbow Case for Raspberry Pi 4 B. With this case it is possible to install four heatsinks and apply a fan as cooling equipment for the electronics of the Raspberry Pi 4 B such as its ARM processor. Heatsinks and cooling fan for Raspberry Pi 4 B.","title":"Case and Cooling"},{"location":"components/#sd-card","text":"The Raspberry Pi requires a medium to boot from. For this we will use a micro sd card because it is lightweight and easy to flash new operating systems. SanDisk Micro SD Card Class 10. Although a micro sd card won't last that long compared to an hard disk drive (HDD) or solid state disk (SSD) it is well suited for testing. Because sd cards are slower when reading and writing data you should make sure to choose a micro sd card with high performance ratings. For the Raspberry Pi a Class 10 micro sd card is recommended. Regarding speed, the Pi has a limited bus speed of approximately 20 MB/s ( source )","title":"SD Card"},{"location":"components/#robot-base","text":"The Robot Car Kit 2WD from Joy-IT (article no.: robot05) is used as the base for the autonomous mobile robot. Parts of the 2WD Robot Car Kit 05 from Joy-IT. The acrylic chassis has many holes which allow to mount a mounting plate that can hold different development boards. It allows also to mount a Raspberry Pi 4 B, which will be used in this project. Two wheels, hence 2WD, are included in the kit which can be attached to the motors that are provided too. A third caster wheel is provided which allows the robot to spin on the spot. This means the robot can be described by a holonomic model. The motors operate in a range between 3 to 6 Volts DC and make it possible to mount a punched disk for speed measurements. With that punched disk and additional speed sensors it is possible to implement odometry in ROS. To power the motors a battery compartment is available together with a switch to turn the robot on or off.","title":"Robot Base"},{"location":"components/#power-supplies","text":"As mentioned the robot will be equipped with a 5V/2.1A USB-C powerbank to supply the Raspberry Pi 4 B with 5 V. Power bank with 10.000 mAh from Intenso. To power the motors the provided battery compartment will be used, which holds four AA batteries \\(4 \\cdot 1.5\\text{V} = 6\\text{V}\\) .","title":"Power Supplies"},{"location":"components/#i2c-hub","text":"The Raspberry Pi provides just two I2C ports, which is why we will use a I2C hub. With the four port I2C hub from Grove it is possible to connect three I2C devices to a single I2C port of the Raspberry Pi Grove I2C Hub.","title":"I2C Hub"},{"location":"components/#breadboard-and-gpio-extension-cable","text":"Optional but helpful for testing is a breadboard and a GPIO extension cable suitable for the Raspberry Pi 4 B. Breadboard with GPIO extension cable.","title":"Breadboard and GPIO Extension Cable"},{"location":"components/#sensors","text":"Sensors are used to sense the environment and to collect information of the current state. For this 2WD robot the sensors are categorized into perception and localization which are explained in the following two sections.","title":"Sensors"},{"location":"components/#perception","text":"Perception sensors of the 2WD robot will be used to avoid collisions using ultrasonic rangers. Another use case is to detect and follow objects using a camera.","title":"Perception"},{"location":"components/#ultrasonic-ranger","text":"To avoid obstacles the robot will carry a Grove - Ultrasonic Ranger at the front. Grove Ultrasonic Ranger for obstacle avoidance. It is a non-contact distance measurement module which works at 40KHz and can be interfaced via a single GPIO . For example physical pin 11 of the Raspberry Pi connected to the SIG pin on the sensor can provide the PWM communication. Parameter Value/Range Operating voltage 3.2~5.2V Operating current 8mA Ultrasonic frequency 40kHz Measuring range 2-350cm Resolution 1cm Output PWM Size 50mm X 25mm X 16mm Weight 13g Measurement angle 15 degree Working temperature -10~60 degree C Trigger signal 10uS TTL Echo signal TTL The code that will be used to wrap this sensor as a ROS node can be found in the Grove Raspberry Pi repository on GitHub. As an alternative we could use the HC SR04 .","title":"Ultrasonic Ranger"},{"location":"components/#camera","text":"RPi Camera v2.","title":"Camera"},{"location":"components/#localization","text":"","title":"Localization"},{"location":"components/#inertial-measurement-unit","text":"An intertial measurement unit (IMU) measures the acceleration and orientation through gyroscopes directly. Other states such as the velocity can then be calculated. For this the Adafruit 9-DOF Absolute Orientation IMU Fusion Breakout - BNO055 is used. 9-DOF Absolute Orientation IMU Fusion Breakout - BNO055 from Adafruit.","title":"Inertial Measurement Unit"},{"location":"components/#odometry","text":"For the used odometry sensor see the section below Motor and Wheel Encoder Alternative Optical Sensor To estimate the change in position over time ([odometry](https://en.wikipedia.org/wiki/Odometry)) the robot will utilize an [optical speed sensor](https://en.wikipedia.org/wiki/Wheel_speed_sensor#Optical_sensor). Specifically the [Joy-IT Speed Sensor](https://joy-it.net/en/products/SEN-Speed) which combines a LM393 ([datasheet](http://www.ti.com/lit/ds/symlink/lm2903-n.pdf)) [comperator](https://en.wikipedia.org/wiki/Comparator) and a H206 slot-type opto interrupter. LM393 Speed Sensor from Joy-IT. Technical Specifications: - Dimensions: 32 x 14 x 7mm - Operating voltage: 3.3V to 5V (we will use 3.3V) - Two outputs: Digital (D0) and Analog (A0) References: https://dronebotworkshop.com/robot-car-with-speed-sensors/","title":"Odometry"},{"location":"components/#actuators","text":"Grove - I2C Motor Driver V1.3","title":"Actuators"},{"location":"components/#control","text":"To drive the two motors of the car kit we use the Grove - I2C Motor Driver V1.3 from Seeed Studio. Grove - I2C Motor Driver.","title":"Control"},{"location":"components/#motor-and-wheel-encoder","text":"The DG01E-E Hobby Motor has a quadrature encoder built in, which makes it easy to assemble the robot and saves space because of no additional (optical or magnetic) wheel encoders. DG01D-E Motor with wheel encoders. Alternative Brushed Gear Motor ### Brushed Gearbox Motor DC Gearbox motor - \"TT Motor\" - 200RPM - 3 to 6VDC.","title":"Motor and Wheel Encoder"},{"location":"components/#human-machine-interface-hmi","text":"The human machine interface is the layer between the user and the robot.","title":"Human Machine Interface (HMI)"},{"location":"components/#oled-display","text":"To update the user with status messages the robot has a 0.96 inch oled (organic ligth emitting diode) display. The oled display used is the Grove I2C 0.96 inch OLED display from Seeed Studio. Grove - I2C 0.96 inch OLED Display. The display is connected to the RPi via I2C on the physical pins 27 (scl) and 28 (sda), refere to the pinout . Library","title":"OLED Display"},{"location":"diffbot_base/","text":"DiffBot Base Package This package contains the so called hardware interface of DiffBot which represents the real hardware in software to work with ROS Control . ROS Control Overview. In the simpleste case all that is needed in this package is to write a class that inherits from hardware_interface::RobotHW and provide a launch file. The launch file will Load the robot description from diffbot_description to the paramter server Run the hardware interface of this package diffbot_base Load the controller configuration yaml from the diffbot_control package to the parameter server Load the controllers with the controller manager diffbot_base Package The diffbot_base package is created with catkin-tools : 1 2 3 4 5 6 7 fjp@diffbot:/home/fjp/catkin_ws/src$ catkin create pkg diffbot_base --catkin-deps diff_drive_controller hardware_interface roscpp sensor_msgs rosparam_shortcuts Creating package \"diffbot_base\" in \"/home/fjp/catkin_ws/src\"... Created file diffbot_base/package.xml Created file diffbot_base/CMakeLists.txt Created folder diffbot_base/include/diffbot_base Created folder diffbot_base/src Successfully created package files in /home/fjp/catkin_ws/src/diffbot_base. To work with this package the specified dependencies must be installed either using the available Ubuntu/Debian packages for ROS Noetic or they have to be built from source first. The following table lists the dependencies that we have to install because they are not already part of the ROS Noetic desktop full installation. Refer to the section ROS Noetic Setup for how this was done. Dependency Source Ubuntu/Debian Package rosparam_shortcuts https://github.com/PickNikRobotics/rosparam_shortcuts ros-noetic-rosparam-shortcuts hardware_interface https://github.com/ros-controls/ros_control ros-noetic-ros-control diff_drive_controller https://github.com/ros-controls/ros_controllers ros-noetic-ros-controllers To install a package from source clone (using git) or download the source files from where they are located (commonly hosted on GitHub) into the src folder of a ros catkin workspace and execute the catkin build command. Also make sure to source the workspace after building new packages with source devel/setup.bash . 1 2 3 4 cd /homw/fjp/git/diffbot/ros/ # Navigate to the workspace catkin build # Build all the packages in the workspace ls build # Show the resulting build space ls devel # Show the resulting devel space Make sure to clone/download the source files suitable for the ROS distribtion you are using. If the sources are not available for the distribution you are working with, it is worth to try building anyway. Chances are that the package you want to use is suitable for multiple ROS distros. For example if a package states in its docs, that it is only available for kinetic it is possible that it will work with a ROS noetic install. {: .notice } Hardware Interface See the include and src folders of this package and the details on the hardware interface implementation. For more details on the hardware interface also refer to the section ROS Integration: Control , it gives more details and also this overview article about ROS Control . The hardware interface provides an interface between the real robot hardware and the controllers provided by ROS Control (or even custom controllers). DiffBot works with the diff_drive_controller that is configured in the diffbot_control package, which is also relevant for the simulation in Gazebo. Remember that the simulation uses the gazebo_ros_control package to communicate with the diff_drive_controller . For the real robot hardware, ROS Control uses an instance of type hardware_interface::RobotHW that is passed to the controller_manager to handle the resources, meaning that the actuated robot joints are not in use by multiple controllers that might be loaded. The skeleton of DiffBot's hardware interface looks like following, where the constructor is used to read loaded configuration values from the robot's description from the ROS parameter server: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 namespace diffbot_base { DiffBotHWInterface :: DiffBotHWInterface ( ros :: NodeHandle & nh , urdf :: Model * urdf_model ) : name_ ( \"hardware_interface\" ) , nh_ ( nh ) { // Initialization of the robot's resources (joints, sensors, actuators) and // interfaces can be done here or inside init(). // E.g. parse the URDF for joint names & interfaces, then initialize them // Check if the URDF model needs to be loaded if ( urdf_model == NULL ) loadURDF ( nh , \"robot_description\" ); else urdf_model_ = urdf_model ; // Load rosparams ros :: NodeHandle rpnh ( nh_ , name_ ); std :: size_t error = 0 ; // Code API of rosparam_shortcuts: // http://docs.ros.org/en/noetic/api/rosparam_shortcuts/html/namespacerosparam__shortcuts.html#aa6536fe0130903960b1de4872df68d5d error += ! rosparam_shortcuts :: get ( name_ , rpnh , \"joints\" , joint_names_ ); error += ! rosparam_shortcuts :: get ( name_ , nh_ , \"mobile_base_controller/wheel_radius\" , wheel_radius_ ); error += ! rosparam_shortcuts :: get ( name_ , nh_ , \"mobile_base_controller/linear/x/max_velocity\" , max_velocity_ ); rosparam_shortcuts :: shutdownIfError ( name_ , error ); wheel_diameter_ = 2.0 * wheel_radius_ ; //max_velocity_ = 0.2; // m/s // ros_control RobotHW needs velocity in rad/s but in the config its given in m/s max_velocity_ = linearToAngular ( max_velocity_ ); // Setup publisher for the motor driver pub_left_motor_value_ = nh_ . advertise < std_msgs :: Int32 > ( \"motor_left\" , 1 ); pub_right_motor_value_ = nh_ . advertise < std_msgs :: Int32 > ( \"motor_right\" , 1 ); // Setup subscriber for the wheel encoders sub_left_encoder_ticks_ = nh_ . subscribe ( \"ticks_left\" , 1 , & DiffBotHWInterface :: leftEncoderTicksCallback , this ); sub_right_encoder_ticks_ = nh_ . subscribe ( \"ticks_right\" , 1 , & DiffBotHWInterface :: rightEncoderTicksCallback , this ); // Initialize the hardware interface init ( nh_ , nh_ ); } bool DiffBotHWInterface :: init ( ros :: NodeHandle & root_nh , ros :: NodeHandle & robot_hw_nh ) { ROS_INFO ( \"Initializing DiffBot Hardware Interface ...\" ); num_joints_ = joint_names_ . size (); ROS_INFO ( \"Number of joints: %d\" , ( int ) num_joints_ ); std :: array < std :: string , NUM_JOINTS > motor_names = { \"left_motor\" , \"right_motor\" }; for ( unsigned int i = 0 ; i < num_joints_ ; i ++ ) { // Create a JointStateHandle for each joint and register them with the // JointStateInterface. hardware_interface :: JointStateHandle joint_state_handle ( joint_names_ [ i ], & joint_positions_ [ i ], & joint_velocities_ [ i ], & joint_efforts_ [ i ]); joint_state_interface_ . registerHandle ( joint_state_handle ); // Create a JointHandle (read and write) for each controllable joint // using the read-only joint handles within the JointStateInterface and // register them with the JointVelocityInterface. hardware_interface :: JointHandle joint_handle ( joint_state_handle , & joint_velocity_commands_ [ i ]); velocity_joint_interface_ . registerHandle ( joint_handle ); // Initialize joint states with zero values joint_positions_ [ i ] = 0.0 ; joint_velocities_ [ i ] = 0.0 ; joint_efforts_ [ i ] = 0.0 ; // unused with diff_drive_controller joint_velocity_commands_ [ i ] = 0.0 ; // Initialize the pid controllers for the motors using the robot namespace std :: string pid_namespace = \"pid/\" + motor_names [ i ]; ROS_INFO_STREAM ( \"pid namespace: \" << pid_namespace ); ros :: NodeHandle nh ( root_nh , pid_namespace ); // TODO implement builder pattern to initialize values otherwise it is hard to see which parameter is what. pids_ [ i ]. init ( nh , 0.0 , 10.0 , 1.0 , 1.0 , 0.0 , 0.0 , false , - max_velocity_ , max_velocity_ ); pids_ [ i ]. setOutputLimits ( - max_velocity_ , max_velocity_ ); } // Register the JointStateInterface containing the read only joints // with this robot's hardware_interface::RobotHW. registerInterface ( & joint_state_interface_ ); // Register the JointVelocityInterface containing the read/write joints // with this robot's hardware_interface::RobotHW. registerInterface ( & velocity_joint_interface_ ); ROS_INFO ( \"... Done Initializing DiffBot Hardware Interface\" ); return true ; } // The read method is part of the control loop cycle (read, update, write) and is used to // populate the robot state from the robot's hardware resources (joints, sensors, actuators). // This method should be called before controller_manager::ControllerManager::update() and write. void DiffBotHWInterface :: read ( const ros :: Time & time , const ros :: Duration & period ) { ros :: Duration elapsed_time = period ; // Read from robot hw (motor encoders) // Fill joint_state_* members with read values double wheel_angles [ 2 ]; double wheel_angle_deltas [ 2 ]; for ( std :: size_t i = 0 ; i < num_joints_ ; ++ i ) { wheel_angles [ i ] = ticksToAngle ( encoder_ticks_ [ i ]); //double wheel_angle_normalized = normalizeAngle(wheel_angle); wheel_angle_deltas [ i ] = wheel_angles [ i ] - joint_positions_ [ i ]; joint_positions_ [ i ] += wheel_angle_deltas [ i ]; joint_velocities_ [ i ] = wheel_angle_deltas [ i ] / period . toSec (); joint_efforts_ [ i ] = 0.0 ; // unused with diff_drive_controller } } // The write method is part of the control loop cycle (read, update, write) and is used to // send out commands to the robot's hardware resources (joints, actuators). // This method should be called after read and controller_manager::ControllerManager::update. void DiffBotHWInterface :: write ( const ros :: Time & time , const ros :: Duration & period ) { ros :: Duration elapsed_time = period ; // Write to robot hw // joint velocity commands from ros_control's RobotHW are in rad/s // Convert the velocity command to a percentage value for the motor // This maps the velocity to a percentage value which is used to apply // a percentage of the highest possible battery voltage to each motor. std_msgs :: Int32 left_motor ; std_msgs :: Int32 right_motor ; double output_left = pids_ [ 0 ]( joint_velocities_ [ 0 ], joint_velocity_commands_ [ 0 ], period ); double output_right = pids_ [ 1 ]( joint_velocities_ [ 1 ], joint_velocity_commands_ [ 1 ], period ); left_motor . data = output_left / max_velocity_ * 100.0 ; right_motor . data = output_right / max_velocity_ * 100.0 ; // Publish the PID computed motor commands to the left and right motors pub_left_motor_value_ . publish ( left_motor ); pub_right_motor_value_ . publish ( right_motor ); } // Process updates from encoders using a subscriber void DiffBotHWInterface :: leftEncoderTicksCallback ( const std_msgs :: Int32 :: ConstPtr & msg ) { encoder_ticks_ [ 0 ] = msg -> data ; ROS_DEBUG_STREAM_THROTTLE ( 1 , \"Left encoder ticks: \" << msg -> data ); } void DiffBotHWInterface :: rightEncoderTicksCallback ( const std_msgs :: Int32 :: ConstPtr & msg ) { encoder_ticks_ [ 1 ] = msg -> data ; ROS_DEBUG_STREAM_THROTTLE ( 1 , \"Right encoder ticks: \" << msg -> data ); } double DiffBotHWInterface :: ticksToAngle ( const int & ticks ) const { // Convert number of encoder ticks to angle in radians double angle = ( double ) ticks * ( 2.0 * M_PI / 542.0 ); ROS_DEBUG_STREAM_THROTTLE ( 1 , ticks << \" ticks correspond to an angle of \" << angle ); return angle ; } }; The functions above are designed to give the controller manager (and the controllers inside the controller manager) access to the joint state of custom robot, and to command it. When the controller manager runs, the controllers will read from the pos, vel and eff variables of the custom robot hardware interface, and the controller will write the desired command into the cmd variable. It's mandatory to make sure the pos, vel and eff variables always have the latest joint state available, and to make sure that whatever is written into the cmd variable gets executed by the robot. This can be done by implementing hardware_interface::RobotHW::read() and a hardware_interface::RobotHW::write() methods. The main node that will be executed uses the controller_manager to operate the so called control loop. In the case of DiffBot a simple example looks like the following, refer to the diffbot_base.cpp for the complete implementation: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 #include <ros/ros.h> #include <diffbot_base/diffbot_hw_interface.h> #include <controller_manager/controller_manager.h> int main ( int argc , char ** argv ) { // Initialize the ROS node ros :: init ( argc , argv , \"diffbot_hw_interface\" ); ros :: NodeHandle nh ; // Create an instance of your robot so that this instance knows about all // the resources that are available. diffbot_base :: DiffBotHWInterface diffBot ( nh ); // Create an instance of the controller manager and pass it the robot, // so that it can handle its resources. controller_manager :: ControllerManager cm ( & diffBot ); // Setup a separate thread that will be used to service ROS callbacks. // NOTE: We run the ROS loop in a separate thread as external calls such // as service callbacks to load controllers can block the (main) control loop ros :: AsyncSpinner spinner ( 1 ); spinner . start (); // Setup for the control loop. ros :: Time prev_time = ros :: Time :: now (); ros :: Rate rate ( 10.0 ); // 10 Hz rate // Blocks until shutdown signal recieved while ( ros :: ok ()) { // Basic bookkeeping to get the system time in order to compute the control period. const ros :: Time time = ros :: Time :: now (); const ros :: Duration period = time - prev_time ; // Execution of the actual control loop. diffBot . read ( time , period ); // If needed, its possible to define transmissions in software by calling the // transmission_interface::ActuatorToJointPositionInterface::propagate() // after reading the joint states. cm . update ( time , period ); // In case of software transmissions, use // transmission_interface::JointToActuatorEffortHandle::propagate() // to convert from the joint space to the actuator space. diffBot . write ( time , period ); // All these steps keep getting repeated with the specified rate. rate . sleep (); } return 0 ; } As we can see, the basic steps are to initialize the node, instantiate the hardware interface, pass it to a new controller manager and run the control loop that does the following: Read joint states from the real robot hardware Update the diff_drive_controller with read values and compute the joint velocities using the target cmd_vel Write the computed values You may be wondering why the read values aren't returned from the diffbot.read() method and nothing is passed to the diffbot.write() . This is because the RobotHW::init() method, shown in the first code snippet, is used to register the actuated joint names (described in the diffbot_description ) to the joint_position , joint_velocity and joint_effort member variables of the custom robot hardware interface. The class that registers the variables of the controller with the hardware interface member variables and thereby gives read access to all joint values without conflicting with other controllers, is the hardware_interface::JointStateInterface . ROS Control uses the hardware_interface::VelocityJointInterface (part of the joint_command_interface.h ) that registers the command member variable of the controller with the hardware interface to provide it the command that should be written to the actuators. When the controller manager runs, the controllers will read from the joint_position , joint_velocity and joint_effort variables of the custom robot hardware interface, and the controller will write the desired command into the joint_velocity_command variable. It's mandatory to make sure the position, velocity and effort (effort is not needed in the case of the diff_drive_controller ) variables always have the latest joint state available, and to make sure that whatever is written into the joint_velocity_command variable gets executed by the robot. As mentioned this can be done by implementing hardware_interface::RobotHW::read() and a hardware_interface::RobotHW::write() methods. In the control loop the overriden hardware_interface::RobotHW::read() method of DiffBot is used to read the joint states. The diff_drive_controller works with a VelocityInterface which is why the joint_position , defined in rad, and joint_velocity , defined in rad/s, are calculated from the encoder ticks. PID Controller Note the two PID controllers inside the hardware interface, where each PID is passed the error between velocity measured by the encoders and the target velocity computed by the diff_drive_controller for a specific wheel joint. The diff_drive_controller doesn't have a PID controller integrated, and doesn't take care if the wheels of the robot are actually turning. As mentioned above, ROS Control expects that the commands sent by the controller are actually implemented on the real robot hardware and that the joint states are always up to date. This means that the diff_drive_controller just uses the twist_msg on the cmd_vel topic for example from the rqt_robot_steering and converts it to a velocity command for the motors. It doesn't take the actual velocity of the motors into account. See the code of diff_drive_controller where the joint_command_velocity is calculated. {: .notice :} This is why a PID controller is needed to avoid situations like the following where the robot moves not straigth although it is commanded to do so: The PID used here inherits from the ROS Control control_toolbox::Pid that provides Dynamic Reconfigure out of the box to tune the proportional, integral and derivative gains. The behaviour when using only the P, I and D gains is that the output can overshoot and even change between positive and negative motor percent values because of a P gain that is too high. To avoid this, a feed forward gain can help to reach the setpoint faster. To add this feed forward gain to the dynamic reconfigure parameters it is necessary to add a new parameter configuration file in this package inside a cfg folder. For more details on ROS dynamic reconfigure see the official tutorials . With the use of the PID controller the robot is able to drive straight: In case of using inexpensive motors like the DG01D-E of DiffBot, you have to take inaccurate driving behaviour into account. The straight driving behaviour can be improved with motors that start spinning at the same voltage levels. To find suitable motors do a voltage sweep test by slightly increasing the voltage and note the voltage level where each motor starts to rotate. Such a test was done on DiffBot's motors. Using six DG01D-E motors the following values were recorded (sorted by increasing voltage): Motor Voltage (V) 01 2.5 02 2.8 - 3.0 03 3.1 04 3.2 05 3.2 06 3.3 In the videos above, motors numbered 01 and 03 were used coincidencely and I wasn't aware of the remarkable differences in voltage levels. Using the motors 04 and 05 improved the driving behaviour significantly. {: .notice } To deal with significant differences in the motors it would also help to tune the two PIDs individually, which is not shown in the video above . Make also sure that the motor driver outputs the same voltage level on both channels when the robot is commanded to move straight. The used Grove i2c motor driver was tested to do this. Another problem of not driving straight can be weight distribution or the orientation of the caster wheel. {: .notice } A good test to check the accuracy is to fix two meters of adhesive tape on the floor in a straight line. Then, place the robot on one end oriented in the direction to the other end. Now command it to move straight along the line and stop it when it reaches the end of the tape. Record the lateral displacement from the tape. Measuring a value below 10 cm is considered precise for these motors. Launch File To run a single controller_manager, the one from the diffbot_base package defined inside difbot_base.cpp use the launch file from diffbot_base/launch/diffbot.launch : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 <!-- https://github.com/ros-controls/ros_controllers/tree/kinetic-devel/diff_drive_controller/test --> <launch> <!-- Load DiffBot model --> <param name= \"robot_description\" command= \"$(find xacro)/xacro '$(find diffbot_description)/urdf/diffbot.xacro'\" /> <node name= \"diffbot_base\" pkg= \"diffbot_base\" type= \"diffbot_base\" /> <!-- Load controller config to the parameter server --> <rosparam command= \"load\" file= \"$(find diffbot_control)/config/diffbot_control.yaml\" /> <!-- Load the controllers --> <node name= \"controller_spawner\" pkg= \"controller_manager\" type= \"spawner\" respawn= \"false\" output= \"screen\" ns= \"diffbot\" args= \"joint_state_controller mobile_base_controller\" /> </launch> This will load the DiffBot robot description onto the parameter server which is required for the hardware interface that gets created inside the next node diffbot_base . It creates the hardware interface and instantiates a new controller manager in the diffbot_base.cpp . Finally the spawner from the controller_manager package is used to initialize and start the controllers defined in the diffbot_control/config/diffbot_control.yaml . The last step in this launch file is required to get the controllers initialized and started. Another way would be to use controller_manager::ControllerManager::loadControllers() inside the diffbot_base.cpp . After launching this launch file on DiffBot (Raspberry Pi) with 1 roslaunch diffbot_base diffbot.launch the following parameters are stored on the parameter server: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 $ rosparam list /diffbot/hardware_interface/joints /diffbot/joint_state_controller/extra_joints /diffbot/joint_state_controller/publish_rate /diffbot/joint_state_controller/type /diffbot/mobile_base_controller/base_frame_id /diffbot/mobile_base_controller/left_wheel /diffbot/mobile_base_controller/pose_covariance_diagonal /diffbot/mobile_base_controller/publish_rate /diffbot/mobile_base_controller/right_wheel /diffbot/mobile_base_controller/twist_covariance_diagonal /diffbot/mobile_base_controller/type /diffbot/mobile_base_controller/wheel_radius /diffbot/mobile_base_controller/wheel_separation /robot_description /rosdistro /roslaunch/uris/host_tensorbook__46157 /roslaunch/uris/host_ubuntu__33729 /rosversion /run_id Additional Requirements Because the hardware interface subscribes to the encoders, that are connected to the Teensy MCU, and publishes to the motors via the motr driver node, another launch will be required to run these additional nodes. See the diffbot_bringup package for this setup. Simulation To have a simulation showing DiffBot, the second step is to use the diffbot_gazebo/launch/diffbot_base.launch on the work pc: 1 $ roslaunch diffbot_gazebo diffbot_base.launch This will launch the gazebo simulation, which can make use of the running controllers inside the controller manager too: ROS Control with Gazebo Overview. After launching the Gazebo simulation the controllers got uninitialized. (It is assumed that the gazebo_ros_control plugin that gets launched). Because of this the controllers have to be initialized and started again. For this the diffbot_base/launch/controllers.launch should be used. This launch file is just loading and starting all controllers again. Note that using the spawner from the controller_manager package, like in the diffbot_base/launch/diffbot.launch results in an error. (TODO this needs some more testing).","title":"Base Hardware Interface"},{"location":"diffbot_base/#diffbot-base-package","text":"This package contains the so called hardware interface of DiffBot which represents the real hardware in software to work with ROS Control . ROS Control Overview. In the simpleste case all that is needed in this package is to write a class that inherits from hardware_interface::RobotHW and provide a launch file. The launch file will Load the robot description from diffbot_description to the paramter server Run the hardware interface of this package diffbot_base Load the controller configuration yaml from the diffbot_control package to the parameter server Load the controllers with the controller manager","title":"DiffBot Base Package"},{"location":"diffbot_base/#diffbot_base-package","text":"The diffbot_base package is created with catkin-tools : 1 2 3 4 5 6 7 fjp@diffbot:/home/fjp/catkin_ws/src$ catkin create pkg diffbot_base --catkin-deps diff_drive_controller hardware_interface roscpp sensor_msgs rosparam_shortcuts Creating package \"diffbot_base\" in \"/home/fjp/catkin_ws/src\"... Created file diffbot_base/package.xml Created file diffbot_base/CMakeLists.txt Created folder diffbot_base/include/diffbot_base Created folder diffbot_base/src Successfully created package files in /home/fjp/catkin_ws/src/diffbot_base. To work with this package the specified dependencies must be installed either using the available Ubuntu/Debian packages for ROS Noetic or they have to be built from source first. The following table lists the dependencies that we have to install because they are not already part of the ROS Noetic desktop full installation. Refer to the section ROS Noetic Setup for how this was done. Dependency Source Ubuntu/Debian Package rosparam_shortcuts https://github.com/PickNikRobotics/rosparam_shortcuts ros-noetic-rosparam-shortcuts hardware_interface https://github.com/ros-controls/ros_control ros-noetic-ros-control diff_drive_controller https://github.com/ros-controls/ros_controllers ros-noetic-ros-controllers To install a package from source clone (using git) or download the source files from where they are located (commonly hosted on GitHub) into the src folder of a ros catkin workspace and execute the catkin build command. Also make sure to source the workspace after building new packages with source devel/setup.bash . 1 2 3 4 cd /homw/fjp/git/diffbot/ros/ # Navigate to the workspace catkin build # Build all the packages in the workspace ls build # Show the resulting build space ls devel # Show the resulting devel space Make sure to clone/download the source files suitable for the ROS distribtion you are using. If the sources are not available for the distribution you are working with, it is worth to try building anyway. Chances are that the package you want to use is suitable for multiple ROS distros. For example if a package states in its docs, that it is only available for kinetic it is possible that it will work with a ROS noetic install. {: .notice }","title":"diffbot_base Package"},{"location":"diffbot_base/#hardware-interface","text":"See the include and src folders of this package and the details on the hardware interface implementation. For more details on the hardware interface also refer to the section ROS Integration: Control , it gives more details and also this overview article about ROS Control . The hardware interface provides an interface between the real robot hardware and the controllers provided by ROS Control (or even custom controllers). DiffBot works with the diff_drive_controller that is configured in the diffbot_control package, which is also relevant for the simulation in Gazebo. Remember that the simulation uses the gazebo_ros_control package to communicate with the diff_drive_controller . For the real robot hardware, ROS Control uses an instance of type hardware_interface::RobotHW that is passed to the controller_manager to handle the resources, meaning that the actuated robot joints are not in use by multiple controllers that might be loaded. The skeleton of DiffBot's hardware interface looks like following, where the constructor is used to read loaded configuration values from the robot's description from the ROS parameter server: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 namespace diffbot_base { DiffBotHWInterface :: DiffBotHWInterface ( ros :: NodeHandle & nh , urdf :: Model * urdf_model ) : name_ ( \"hardware_interface\" ) , nh_ ( nh ) { // Initialization of the robot's resources (joints, sensors, actuators) and // interfaces can be done here or inside init(). // E.g. parse the URDF for joint names & interfaces, then initialize them // Check if the URDF model needs to be loaded if ( urdf_model == NULL ) loadURDF ( nh , \"robot_description\" ); else urdf_model_ = urdf_model ; // Load rosparams ros :: NodeHandle rpnh ( nh_ , name_ ); std :: size_t error = 0 ; // Code API of rosparam_shortcuts: // http://docs.ros.org/en/noetic/api/rosparam_shortcuts/html/namespacerosparam__shortcuts.html#aa6536fe0130903960b1de4872df68d5d error += ! rosparam_shortcuts :: get ( name_ , rpnh , \"joints\" , joint_names_ ); error += ! rosparam_shortcuts :: get ( name_ , nh_ , \"mobile_base_controller/wheel_radius\" , wheel_radius_ ); error += ! rosparam_shortcuts :: get ( name_ , nh_ , \"mobile_base_controller/linear/x/max_velocity\" , max_velocity_ ); rosparam_shortcuts :: shutdownIfError ( name_ , error ); wheel_diameter_ = 2.0 * wheel_radius_ ; //max_velocity_ = 0.2; // m/s // ros_control RobotHW needs velocity in rad/s but in the config its given in m/s max_velocity_ = linearToAngular ( max_velocity_ ); // Setup publisher for the motor driver pub_left_motor_value_ = nh_ . advertise < std_msgs :: Int32 > ( \"motor_left\" , 1 ); pub_right_motor_value_ = nh_ . advertise < std_msgs :: Int32 > ( \"motor_right\" , 1 ); // Setup subscriber for the wheel encoders sub_left_encoder_ticks_ = nh_ . subscribe ( \"ticks_left\" , 1 , & DiffBotHWInterface :: leftEncoderTicksCallback , this ); sub_right_encoder_ticks_ = nh_ . subscribe ( \"ticks_right\" , 1 , & DiffBotHWInterface :: rightEncoderTicksCallback , this ); // Initialize the hardware interface init ( nh_ , nh_ ); } bool DiffBotHWInterface :: init ( ros :: NodeHandle & root_nh , ros :: NodeHandle & robot_hw_nh ) { ROS_INFO ( \"Initializing DiffBot Hardware Interface ...\" ); num_joints_ = joint_names_ . size (); ROS_INFO ( \"Number of joints: %d\" , ( int ) num_joints_ ); std :: array < std :: string , NUM_JOINTS > motor_names = { \"left_motor\" , \"right_motor\" }; for ( unsigned int i = 0 ; i < num_joints_ ; i ++ ) { // Create a JointStateHandle for each joint and register them with the // JointStateInterface. hardware_interface :: JointStateHandle joint_state_handle ( joint_names_ [ i ], & joint_positions_ [ i ], & joint_velocities_ [ i ], & joint_efforts_ [ i ]); joint_state_interface_ . registerHandle ( joint_state_handle ); // Create a JointHandle (read and write) for each controllable joint // using the read-only joint handles within the JointStateInterface and // register them with the JointVelocityInterface. hardware_interface :: JointHandle joint_handle ( joint_state_handle , & joint_velocity_commands_ [ i ]); velocity_joint_interface_ . registerHandle ( joint_handle ); // Initialize joint states with zero values joint_positions_ [ i ] = 0.0 ; joint_velocities_ [ i ] = 0.0 ; joint_efforts_ [ i ] = 0.0 ; // unused with diff_drive_controller joint_velocity_commands_ [ i ] = 0.0 ; // Initialize the pid controllers for the motors using the robot namespace std :: string pid_namespace = \"pid/\" + motor_names [ i ]; ROS_INFO_STREAM ( \"pid namespace: \" << pid_namespace ); ros :: NodeHandle nh ( root_nh , pid_namespace ); // TODO implement builder pattern to initialize values otherwise it is hard to see which parameter is what. pids_ [ i ]. init ( nh , 0.0 , 10.0 , 1.0 , 1.0 , 0.0 , 0.0 , false , - max_velocity_ , max_velocity_ ); pids_ [ i ]. setOutputLimits ( - max_velocity_ , max_velocity_ ); } // Register the JointStateInterface containing the read only joints // with this robot's hardware_interface::RobotHW. registerInterface ( & joint_state_interface_ ); // Register the JointVelocityInterface containing the read/write joints // with this robot's hardware_interface::RobotHW. registerInterface ( & velocity_joint_interface_ ); ROS_INFO ( \"... Done Initializing DiffBot Hardware Interface\" ); return true ; } // The read method is part of the control loop cycle (read, update, write) and is used to // populate the robot state from the robot's hardware resources (joints, sensors, actuators). // This method should be called before controller_manager::ControllerManager::update() and write. void DiffBotHWInterface :: read ( const ros :: Time & time , const ros :: Duration & period ) { ros :: Duration elapsed_time = period ; // Read from robot hw (motor encoders) // Fill joint_state_* members with read values double wheel_angles [ 2 ]; double wheel_angle_deltas [ 2 ]; for ( std :: size_t i = 0 ; i < num_joints_ ; ++ i ) { wheel_angles [ i ] = ticksToAngle ( encoder_ticks_ [ i ]); //double wheel_angle_normalized = normalizeAngle(wheel_angle); wheel_angle_deltas [ i ] = wheel_angles [ i ] - joint_positions_ [ i ]; joint_positions_ [ i ] += wheel_angle_deltas [ i ]; joint_velocities_ [ i ] = wheel_angle_deltas [ i ] / period . toSec (); joint_efforts_ [ i ] = 0.0 ; // unused with diff_drive_controller } } // The write method is part of the control loop cycle (read, update, write) and is used to // send out commands to the robot's hardware resources (joints, actuators). // This method should be called after read and controller_manager::ControllerManager::update. void DiffBotHWInterface :: write ( const ros :: Time & time , const ros :: Duration & period ) { ros :: Duration elapsed_time = period ; // Write to robot hw // joint velocity commands from ros_control's RobotHW are in rad/s // Convert the velocity command to a percentage value for the motor // This maps the velocity to a percentage value which is used to apply // a percentage of the highest possible battery voltage to each motor. std_msgs :: Int32 left_motor ; std_msgs :: Int32 right_motor ; double output_left = pids_ [ 0 ]( joint_velocities_ [ 0 ], joint_velocity_commands_ [ 0 ], period ); double output_right = pids_ [ 1 ]( joint_velocities_ [ 1 ], joint_velocity_commands_ [ 1 ], period ); left_motor . data = output_left / max_velocity_ * 100.0 ; right_motor . data = output_right / max_velocity_ * 100.0 ; // Publish the PID computed motor commands to the left and right motors pub_left_motor_value_ . publish ( left_motor ); pub_right_motor_value_ . publish ( right_motor ); } // Process updates from encoders using a subscriber void DiffBotHWInterface :: leftEncoderTicksCallback ( const std_msgs :: Int32 :: ConstPtr & msg ) { encoder_ticks_ [ 0 ] = msg -> data ; ROS_DEBUG_STREAM_THROTTLE ( 1 , \"Left encoder ticks: \" << msg -> data ); } void DiffBotHWInterface :: rightEncoderTicksCallback ( const std_msgs :: Int32 :: ConstPtr & msg ) { encoder_ticks_ [ 1 ] = msg -> data ; ROS_DEBUG_STREAM_THROTTLE ( 1 , \"Right encoder ticks: \" << msg -> data ); } double DiffBotHWInterface :: ticksToAngle ( const int & ticks ) const { // Convert number of encoder ticks to angle in radians double angle = ( double ) ticks * ( 2.0 * M_PI / 542.0 ); ROS_DEBUG_STREAM_THROTTLE ( 1 , ticks << \" ticks correspond to an angle of \" << angle ); return angle ; } }; The functions above are designed to give the controller manager (and the controllers inside the controller manager) access to the joint state of custom robot, and to command it. When the controller manager runs, the controllers will read from the pos, vel and eff variables of the custom robot hardware interface, and the controller will write the desired command into the cmd variable. It's mandatory to make sure the pos, vel and eff variables always have the latest joint state available, and to make sure that whatever is written into the cmd variable gets executed by the robot. This can be done by implementing hardware_interface::RobotHW::read() and a hardware_interface::RobotHW::write() methods. The main node that will be executed uses the controller_manager to operate the so called control loop. In the case of DiffBot a simple example looks like the following, refer to the diffbot_base.cpp for the complete implementation: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 #include <ros/ros.h> #include <diffbot_base/diffbot_hw_interface.h> #include <controller_manager/controller_manager.h> int main ( int argc , char ** argv ) { // Initialize the ROS node ros :: init ( argc , argv , \"diffbot_hw_interface\" ); ros :: NodeHandle nh ; // Create an instance of your robot so that this instance knows about all // the resources that are available. diffbot_base :: DiffBotHWInterface diffBot ( nh ); // Create an instance of the controller manager and pass it the robot, // so that it can handle its resources. controller_manager :: ControllerManager cm ( & diffBot ); // Setup a separate thread that will be used to service ROS callbacks. // NOTE: We run the ROS loop in a separate thread as external calls such // as service callbacks to load controllers can block the (main) control loop ros :: AsyncSpinner spinner ( 1 ); spinner . start (); // Setup for the control loop. ros :: Time prev_time = ros :: Time :: now (); ros :: Rate rate ( 10.0 ); // 10 Hz rate // Blocks until shutdown signal recieved while ( ros :: ok ()) { // Basic bookkeeping to get the system time in order to compute the control period. const ros :: Time time = ros :: Time :: now (); const ros :: Duration period = time - prev_time ; // Execution of the actual control loop. diffBot . read ( time , period ); // If needed, its possible to define transmissions in software by calling the // transmission_interface::ActuatorToJointPositionInterface::propagate() // after reading the joint states. cm . update ( time , period ); // In case of software transmissions, use // transmission_interface::JointToActuatorEffortHandle::propagate() // to convert from the joint space to the actuator space. diffBot . write ( time , period ); // All these steps keep getting repeated with the specified rate. rate . sleep (); } return 0 ; } As we can see, the basic steps are to initialize the node, instantiate the hardware interface, pass it to a new controller manager and run the control loop that does the following: Read joint states from the real robot hardware Update the diff_drive_controller with read values and compute the joint velocities using the target cmd_vel Write the computed values You may be wondering why the read values aren't returned from the diffbot.read() method and nothing is passed to the diffbot.write() . This is because the RobotHW::init() method, shown in the first code snippet, is used to register the actuated joint names (described in the diffbot_description ) to the joint_position , joint_velocity and joint_effort member variables of the custom robot hardware interface. The class that registers the variables of the controller with the hardware interface member variables and thereby gives read access to all joint values without conflicting with other controllers, is the hardware_interface::JointStateInterface . ROS Control uses the hardware_interface::VelocityJointInterface (part of the joint_command_interface.h ) that registers the command member variable of the controller with the hardware interface to provide it the command that should be written to the actuators. When the controller manager runs, the controllers will read from the joint_position , joint_velocity and joint_effort variables of the custom robot hardware interface, and the controller will write the desired command into the joint_velocity_command variable. It's mandatory to make sure the position, velocity and effort (effort is not needed in the case of the diff_drive_controller ) variables always have the latest joint state available, and to make sure that whatever is written into the joint_velocity_command variable gets executed by the robot. As mentioned this can be done by implementing hardware_interface::RobotHW::read() and a hardware_interface::RobotHW::write() methods. In the control loop the overriden hardware_interface::RobotHW::read() method of DiffBot is used to read the joint states. The diff_drive_controller works with a VelocityInterface which is why the joint_position , defined in rad, and joint_velocity , defined in rad/s, are calculated from the encoder ticks.","title":"Hardware Interface"},{"location":"diffbot_base/#pid-controller","text":"Note the two PID controllers inside the hardware interface, where each PID is passed the error between velocity measured by the encoders and the target velocity computed by the diff_drive_controller for a specific wheel joint. The diff_drive_controller doesn't have a PID controller integrated, and doesn't take care if the wheels of the robot are actually turning. As mentioned above, ROS Control expects that the commands sent by the controller are actually implemented on the real robot hardware and that the joint states are always up to date. This means that the diff_drive_controller just uses the twist_msg on the cmd_vel topic for example from the rqt_robot_steering and converts it to a velocity command for the motors. It doesn't take the actual velocity of the motors into account. See the code of diff_drive_controller where the joint_command_velocity is calculated. {: .notice :} This is why a PID controller is needed to avoid situations like the following where the robot moves not straigth although it is commanded to do so: The PID used here inherits from the ROS Control control_toolbox::Pid that provides Dynamic Reconfigure out of the box to tune the proportional, integral and derivative gains. The behaviour when using only the P, I and D gains is that the output can overshoot and even change between positive and negative motor percent values because of a P gain that is too high. To avoid this, a feed forward gain can help to reach the setpoint faster. To add this feed forward gain to the dynamic reconfigure parameters it is necessary to add a new parameter configuration file in this package inside a cfg folder. For more details on ROS dynamic reconfigure see the official tutorials . With the use of the PID controller the robot is able to drive straight: In case of using inexpensive motors like the DG01D-E of DiffBot, you have to take inaccurate driving behaviour into account. The straight driving behaviour can be improved with motors that start spinning at the same voltage levels. To find suitable motors do a voltage sweep test by slightly increasing the voltage and note the voltage level where each motor starts to rotate. Such a test was done on DiffBot's motors. Using six DG01D-E motors the following values were recorded (sorted by increasing voltage): Motor Voltage (V) 01 2.5 02 2.8 - 3.0 03 3.1 04 3.2 05 3.2 06 3.3 In the videos above, motors numbered 01 and 03 were used coincidencely and I wasn't aware of the remarkable differences in voltage levels. Using the motors 04 and 05 improved the driving behaviour significantly. {: .notice } To deal with significant differences in the motors it would also help to tune the two PIDs individually, which is not shown in the video above . Make also sure that the motor driver outputs the same voltage level on both channels when the robot is commanded to move straight. The used Grove i2c motor driver was tested to do this. Another problem of not driving straight can be weight distribution or the orientation of the caster wheel. {: .notice } A good test to check the accuracy is to fix two meters of adhesive tape on the floor in a straight line. Then, place the robot on one end oriented in the direction to the other end. Now command it to move straight along the line and stop it when it reaches the end of the tape. Record the lateral displacement from the tape. Measuring a value below 10 cm is considered precise for these motors.","title":"PID Controller"},{"location":"diffbot_base/#launch-file","text":"To run a single controller_manager, the one from the diffbot_base package defined inside difbot_base.cpp use the launch file from diffbot_base/launch/diffbot.launch : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 <!-- https://github.com/ros-controls/ros_controllers/tree/kinetic-devel/diff_drive_controller/test --> <launch> <!-- Load DiffBot model --> <param name= \"robot_description\" command= \"$(find xacro)/xacro '$(find diffbot_description)/urdf/diffbot.xacro'\" /> <node name= \"diffbot_base\" pkg= \"diffbot_base\" type= \"diffbot_base\" /> <!-- Load controller config to the parameter server --> <rosparam command= \"load\" file= \"$(find diffbot_control)/config/diffbot_control.yaml\" /> <!-- Load the controllers --> <node name= \"controller_spawner\" pkg= \"controller_manager\" type= \"spawner\" respawn= \"false\" output= \"screen\" ns= \"diffbot\" args= \"joint_state_controller mobile_base_controller\" /> </launch> This will load the DiffBot robot description onto the parameter server which is required for the hardware interface that gets created inside the next node diffbot_base . It creates the hardware interface and instantiates a new controller manager in the diffbot_base.cpp . Finally the spawner from the controller_manager package is used to initialize and start the controllers defined in the diffbot_control/config/diffbot_control.yaml . The last step in this launch file is required to get the controllers initialized and started. Another way would be to use controller_manager::ControllerManager::loadControllers() inside the diffbot_base.cpp . After launching this launch file on DiffBot (Raspberry Pi) with 1 roslaunch diffbot_base diffbot.launch the following parameters are stored on the parameter server: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 $ rosparam list /diffbot/hardware_interface/joints /diffbot/joint_state_controller/extra_joints /diffbot/joint_state_controller/publish_rate /diffbot/joint_state_controller/type /diffbot/mobile_base_controller/base_frame_id /diffbot/mobile_base_controller/left_wheel /diffbot/mobile_base_controller/pose_covariance_diagonal /diffbot/mobile_base_controller/publish_rate /diffbot/mobile_base_controller/right_wheel /diffbot/mobile_base_controller/twist_covariance_diagonal /diffbot/mobile_base_controller/type /diffbot/mobile_base_controller/wheel_radius /diffbot/mobile_base_controller/wheel_separation /robot_description /rosdistro /roslaunch/uris/host_tensorbook__46157 /roslaunch/uris/host_ubuntu__33729 /rosversion /run_id","title":"Launch File"},{"location":"diffbot_base/#additional-requirements","text":"Because the hardware interface subscribes to the encoders, that are connected to the Teensy MCU, and publishes to the motors via the motr driver node, another launch will be required to run these additional nodes. See the diffbot_bringup package for this setup.","title":"Additional Requirements"},{"location":"diffbot_base/#simulation","text":"To have a simulation showing DiffBot, the second step is to use the diffbot_gazebo/launch/diffbot_base.launch on the work pc: 1 $ roslaunch diffbot_gazebo diffbot_base.launch This will launch the gazebo simulation, which can make use of the running controllers inside the controller manager too: ROS Control with Gazebo Overview. After launching the Gazebo simulation the controllers got uninitialized. (It is assumed that the gazebo_ros_control plugin that gets launched). Because of this the controllers have to be initialized and started again. For this the diffbot_base/launch/controllers.launch should be used. This launch file is just loading and starting all controllers again. Note that using the spawner from the controller_manager package, like in the diffbot_base/launch/diffbot.launch results in an error. (TODO this needs some more testing).","title":"Simulation"},{"location":"diffbot_bringup/","text":"DiffBot Bring Up Package The bringup package is used to initialize the real hardware of DiffBot and to actually drive the robot around. First the package is created using catkin-tools : 1 2 3 4 5 fjp@diffbot:~/git/diffbot/ros/src$ catkin create pkg diffbot_bringup Creating package \"diffbot_bringup\" in \"/home/fjp/git/diffbot/ros/src\"... Created file diffbot_bringup/package.xml Created file diffbot_bringup/CMakeLists.txt Successfully created package files in /home/fjp/git/diffbot/ros/src/diffbot_bringup. The package provides a launch folder which includes minimal.launch and bringup.launch . The minimal.launch is used to load DiffBot's robot descripton and the controller configuration onto the ROS parameter server using the launch file from the diffbot_base package . It will also setup the ROS controller manager with DiffBot's hardware interface . For the motor driver the node motor_driver.py from the grove_motor_driver package is started. And for the encoders rosserial communicates with the Teensy microcontroller to publish the encoder ticks. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 <launch> <!-- Including the following launch file from diffbot_base package will --> <!-- Load the robot description onto the parameter server --> <!-- Run the controller manager with DiffBot's hardware interface --> <!-- Load the controller config onto the parameter server --> <include file= \"$(find diffbot_base)/launch/diffbot.launch\" > <!-- arg name=\"model\" value=\"$(arg model)\" /--> </include> <!-- Motors --> <!-- --> <node name= \"motor_driver\" pkg= \"grove_motor_driver\" type= \"motor_driver.py\" respawn= \"false\" output= \"screen\" ns= \"diffbot\" /> <!-- Encoders --> <!-- Run rosserial to connect with the Teensy 3.2 board connected to the motor encoders --> <node name= \"rosserial_teensy\" pkg= \"rosserial_python\" type= \"serial_node.py\" respawn= \"false\" output= \"screen\" ns= \"diffbot\" args= \"_port:=/dev/ttyACM0 _baud:=115200\" /> </launch> As mentioned, the ROS controller used for DiffBot is the diff_drive_controller . This controller publishes a transform message (see its published topics ), via the /tf topic, between the odom frame and the frame configured in the controller's configuration specified by the base_frame_id . In the case of DiffBot this is the base_footprint , a conventional link, defined in REP-120 , for mobile robots that specifies the robot's footprint. Because this is the only transform published by diff_drive_controller another node is needed to publish rest of the link transformations. It is the well known robot_state_publisher , which uses the joint states published by the ROS controller joint_state_controller (not to be confused with joint_state_publisher - it is not used here, see this answer for the difference) to create the transforms between the links. To do this the bringup.launch includes the minimal.launch and then runs the robot_state_publisher : 1 2 3 4 5 6 7 8 9 10 11 12 <launch> <include file= \"$(find diffbot_bringup)/launch/minimal.launch\" > <!-- arg name=\"model\" value=\"$(arg model)\" /--> </include> <!-- Starting robot state publish which will publish tf --> <!-- This is needed to publish transforms between all links --> <!-- diff_drive_controller publishes only a single transfrom between odom and base_footprint --> <!-- The robot_state_publisher reads the joint states published by ros control's joint_state_controller --> <node name= \"robot_state_publisher\" pkg= \"robot_state_publisher\" type= \"robot_state_publisher\" output= \"screen\" ns= \"diffbot\" /> </launch>","title":"Hardware Bringup"},{"location":"diffbot_bringup/#diffbot-bring-up-package","text":"The bringup package is used to initialize the real hardware of DiffBot and to actually drive the robot around. First the package is created using catkin-tools : 1 2 3 4 5 fjp@diffbot:~/git/diffbot/ros/src$ catkin create pkg diffbot_bringup Creating package \"diffbot_bringup\" in \"/home/fjp/git/diffbot/ros/src\"... Created file diffbot_bringup/package.xml Created file diffbot_bringup/CMakeLists.txt Successfully created package files in /home/fjp/git/diffbot/ros/src/diffbot_bringup. The package provides a launch folder which includes minimal.launch and bringup.launch . The minimal.launch is used to load DiffBot's robot descripton and the controller configuration onto the ROS parameter server using the launch file from the diffbot_base package . It will also setup the ROS controller manager with DiffBot's hardware interface . For the motor driver the node motor_driver.py from the grove_motor_driver package is started. And for the encoders rosserial communicates with the Teensy microcontroller to publish the encoder ticks. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 <launch> <!-- Including the following launch file from diffbot_base package will --> <!-- Load the robot description onto the parameter server --> <!-- Run the controller manager with DiffBot's hardware interface --> <!-- Load the controller config onto the parameter server --> <include file= \"$(find diffbot_base)/launch/diffbot.launch\" > <!-- arg name=\"model\" value=\"$(arg model)\" /--> </include> <!-- Motors --> <!-- --> <node name= \"motor_driver\" pkg= \"grove_motor_driver\" type= \"motor_driver.py\" respawn= \"false\" output= \"screen\" ns= \"diffbot\" /> <!-- Encoders --> <!-- Run rosserial to connect with the Teensy 3.2 board connected to the motor encoders --> <node name= \"rosserial_teensy\" pkg= \"rosserial_python\" type= \"serial_node.py\" respawn= \"false\" output= \"screen\" ns= \"diffbot\" args= \"_port:=/dev/ttyACM0 _baud:=115200\" /> </launch> As mentioned, the ROS controller used for DiffBot is the diff_drive_controller . This controller publishes a transform message (see its published topics ), via the /tf topic, between the odom frame and the frame configured in the controller's configuration specified by the base_frame_id . In the case of DiffBot this is the base_footprint , a conventional link, defined in REP-120 , for mobile robots that specifies the robot's footprint. Because this is the only transform published by diff_drive_controller another node is needed to publish rest of the link transformations. It is the well known robot_state_publisher , which uses the joint states published by the ROS controller joint_state_controller (not to be confused with joint_state_publisher - it is not used here, see this answer for the difference) to create the transforms between the links. To do this the bringup.launch includes the minimal.launch and then runs the robot_state_publisher : 1 2 3 4 5 6 7 8 9 10 11 12 <launch> <include file= \"$(find diffbot_bringup)/launch/minimal.launch\" > <!-- arg name=\"model\" value=\"$(arg model)\" /--> </include> <!-- Starting robot state publish which will publish tf --> <!-- This is needed to publish transforms between all links --> <!-- diff_drive_controller publishes only a single transfrom between odom and base_footprint --> <!-- The robot_state_publisher reads the joint states published by ros control's joint_state_controller --> <node name= \"robot_state_publisher\" pkg= \"robot_state_publisher\" type= \"robot_state_publisher\" output= \"screen\" ns= \"diffbot\" /> </launch>","title":"DiffBot Bring Up Package"},{"location":"diffbot_control/","text":"DiffBot Control Package As described in the ROS Integration and Gazebo Simulation sections, DiffBot makes use of ROS Control repositories. Specifically the diff_drive_controller package from the ros_controllers meta package. To leverage ROS Control for the simulation with Gazebo the robot description and the controller configuration (usually a MYROBOT_control.yaml file) is required. For the real hardware its required to implement a class derived from hardware_interface::RobotHW . The convention to control a robot (in simulation and in the real world) is to have a package named MYROBOT_control . In case of DiffBot its called diffbot_control and created with catkin create pkg PKG_NAME [--catkin-deps [DEP [DEP ...]]] : 1 2 3 4 5 6 7 catkin create pkg diffbot_control --catkin-deps diff_drive_controller roscpp sensor_msgs Creating package \"diffbot_control\" in \"/home/fjp/git/diffbot/ros/src\"... Created file diffbot_control/CMakeLists.txt Created file diffbot_control/package.xml Created folder diffbot_control/include/diffbot_control Created folder diffbot_control/src Successfully created package files in /home/fjp/git/diffbot/ros/src/diffbot_control. To work with this package the specified dependencies must be installed either using the available Ubuntu/Debian packages for ROS Noetic or have to be built from source first. The following table lists the dependencies that we have to install because they are not already part of the ROS Noetic desktop full installation. Refer to the section ROS Noetic Setup for how this was done. Dependency Source Ubuntu/Debian Package diff_drive_controller https://github.com/ros-controls/ros_controllers ros-noetic-ros-controllers To install a package from source clone (using git) or download the source files from where they are located (commonly hosted on GitHub) into the src folder of a ros catkin workspace and execute the catkin build command. Also make sure to source the workspace after building new packages with source devel/setup.bash . 1 2 3 4 cd /home/fjp/git/diffbot/ros/ # Navigate to the workspace catkin build # Build all the packages in the workspace ls build # Show the resulting build space ls devel # Show the resulting devel space Make sure to clone/download the source files suitable for the ROS distribtion you are using. If the sources are not available for the distribution you are working with, it is worth to try building anyway. Chances are that the package you want to use is suitable for multiple ROS distros. For example if a package states in its docs, that it is only available for kinetic it is possible that it will work with a ROS noetic install. {: .notice } ROS Control in Gazebo Two great resources to get the diff_drive_controller working inside Gazebo is the Gazebo ROS Control Tutorial of rrbot and the R2D2 ROS URDF Tutorial , especially the last section, The Wheels on the Droid Go Round and Round . To spawn DiffBot inside Gazebo, RViz and control it with the rqt_robot_steering plugin, launch the diffbot.launch inside the diffbot_control package: 1 roslaunch diffbot_control diffbot.launch This launch file makes use of diffbot_gazebo/launch/diffbot.launch , diffbot_control/launch/diffbot_control.launch to run gazebo and the diff_drive_controller . It also opens RViz with the configuration stored in diffbot_control/rviz/diffbot.rviz . The following video shows the result of launching. Note the video may be outdated when you read this and the model has improved. ROS Control on the Real Hardware As mentioned above the its required to implement a class derived from hardware_interface::RobotHW . Let's call it DiffBotHW and create it inside the diffbot_control/src folder.","title":"Control"},{"location":"diffbot_control/#diffbot-control-package","text":"As described in the ROS Integration and Gazebo Simulation sections, DiffBot makes use of ROS Control repositories. Specifically the diff_drive_controller package from the ros_controllers meta package. To leverage ROS Control for the simulation with Gazebo the robot description and the controller configuration (usually a MYROBOT_control.yaml file) is required. For the real hardware its required to implement a class derived from hardware_interface::RobotHW . The convention to control a robot (in simulation and in the real world) is to have a package named MYROBOT_control . In case of DiffBot its called diffbot_control and created with catkin create pkg PKG_NAME [--catkin-deps [DEP [DEP ...]]] : 1 2 3 4 5 6 7 catkin create pkg diffbot_control --catkin-deps diff_drive_controller roscpp sensor_msgs Creating package \"diffbot_control\" in \"/home/fjp/git/diffbot/ros/src\"... Created file diffbot_control/CMakeLists.txt Created file diffbot_control/package.xml Created folder diffbot_control/include/diffbot_control Created folder diffbot_control/src Successfully created package files in /home/fjp/git/diffbot/ros/src/diffbot_control. To work with this package the specified dependencies must be installed either using the available Ubuntu/Debian packages for ROS Noetic or have to be built from source first. The following table lists the dependencies that we have to install because they are not already part of the ROS Noetic desktop full installation. Refer to the section ROS Noetic Setup for how this was done. Dependency Source Ubuntu/Debian Package diff_drive_controller https://github.com/ros-controls/ros_controllers ros-noetic-ros-controllers To install a package from source clone (using git) or download the source files from where they are located (commonly hosted on GitHub) into the src folder of a ros catkin workspace and execute the catkin build command. Also make sure to source the workspace after building new packages with source devel/setup.bash . 1 2 3 4 cd /home/fjp/git/diffbot/ros/ # Navigate to the workspace catkin build # Build all the packages in the workspace ls build # Show the resulting build space ls devel # Show the resulting devel space Make sure to clone/download the source files suitable for the ROS distribtion you are using. If the sources are not available for the distribution you are working with, it is worth to try building anyway. Chances are that the package you want to use is suitable for multiple ROS distros. For example if a package states in its docs, that it is only available for kinetic it is possible that it will work with a ROS noetic install. {: .notice }","title":"DiffBot Control Package"},{"location":"diffbot_control/#ros-control-in-gazebo","text":"Two great resources to get the diff_drive_controller working inside Gazebo is the Gazebo ROS Control Tutorial of rrbot and the R2D2 ROS URDF Tutorial , especially the last section, The Wheels on the Droid Go Round and Round . To spawn DiffBot inside Gazebo, RViz and control it with the rqt_robot_steering plugin, launch the diffbot.launch inside the diffbot_control package: 1 roslaunch diffbot_control diffbot.launch This launch file makes use of diffbot_gazebo/launch/diffbot.launch , diffbot_control/launch/diffbot_control.launch to run gazebo and the diff_drive_controller . It also opens RViz with the configuration stored in diffbot_control/rviz/diffbot.rviz . The following video shows the result of launching. Note the video may be outdated when you read this and the model has improved.","title":"ROS Control in Gazebo"},{"location":"diffbot_control/#ros-control-on-the-real-hardware","text":"As mentioned above the its required to implement a class derived from hardware_interface::RobotHW . Let's call it DiffBotHW and create it inside the diffbot_control/src folder.","title":"ROS Control on the Real Hardware"},{"location":"diffbot_gazebo/","text":"Simulate DiffBot in Gazebo As described in the Creating your own Gazebo ROS Package , it is common in ROS to create a package that contains all the world files and launch files used with Gazebo. These files are located in a ROS package named /MYROBOT_gazebo . For DiffBot the package is named diffbot_gazebo . Another example that follows best pratices is rrbot which can be found in the gazebo_ros_demos repository. 1 2 3 4 5 fjp@ubuntu:~/git/diffbot/ros/src$ catkin create pkg diffbot_gazebo Creating package \"diffbot_gazebo\" in \"/home/fjp/git/diffbot/ros/src\"... Created file diffbot_gazebo/package.xml Created file diffbot_gazebo/CMakeLists.txt Successfully created package files in /home/fjp/git/diffbot/ros/src/diffbot_gazebo. The diffbot_gazebo package contains a launch file to lauch a world in Gazebo and spawn the robot model, which is defined in the previously created diffbot_description package. For the launch files the convention is to have a folder named launch and for Gazebo world files a folder named world inside a package. 1 fjp@ubuntu:~/git/diffbot/ros/src/diffbot_gazebo$ mkdir launch world Inside the launch folder is the diffbot.launch . ```xml 1 2 3 4 5 6 7 8 9 <!-- We resume the logic in empty_world.launch, changing only the name of the world to be launched --> <include file=\"$(find gazebo_ros)/launch/empty_world.launch\"> <arg name=\"world_name\" value=\"$(find diffbot_gazebo)/worlds/diffbot.world\"/> <arg name=\"debug\" value=\"$(arg debug)\" /> <arg name=\"gui\" value=\"$(arg gui)\" /> <arg name=\"paused\" value=\"$(arg paused)\"/> <arg name=\"use_sim_time\" value=\"$(arg use_sim_time)\"/> <arg name=\"headless\" value=\"$(arg headless)\"/> </include> In the `world` folder of the `diffbot_gazebo` package is the `diffbot.world` file: xml <?xml version=\"1.0\" ?> model://ground_plane model://sun model://gas_station gas_station -2.0 7.0 0 0 0 0 ``` With these files build the catkin workspace and source it to make the new diffbot_gazebo package visible to roslaunch : 1 2 catkin build source devel/setup.zsh Then its possible to launch the diffbot.launch with: 1 roslaunch diffbot_gazebo diffbot.launch This will lead to the following output: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 roslaunch diffbot_gazebo diffbot.launch ... logging to /home/fjp/.ros/log/6be90ef2-fdd8-11ea-9cb3-317fd602d5f2/roslaunch-tensorbook-393333.log Checking log directory for disk usage. This may take a while. Press Ctrl-C to interrupt Done checking log file disk usage. Usage is <1GB. started roslaunch server http://tensorbook:32837/ SUMMARY ======== PARAMETERS * /gazebo/enable_ros_network: True * /rosdistro: noetic * /rosversion: 1.15.8 * /use_sim_time: True NODES / gazebo (gazebo_ros/gzserver) gazebo_gui (gazebo_ros/gzclient) ROS_MASTER_URI=http://localhost:11311 process[gazebo-1]: started with pid [393352] process[gazebo_gui-2]: started with pid [393357] [ INFO] [1600950165.494721382]: Finished loading Gazebo ROS API Plugin. [ INFO] [1600950165.495515766]: waitForService: Service [/gazebo/set_physics_properties] has not been advertised, waiting... [ INFO] [1600950165.649461740]: Finished loading Gazebo ROS API Plugin. [ INFO] [1600950165.650277038]: waitForService: Service [/gazebo_gui/set_physics_properties] has not been advertised, waiting... [ INFO] [1600950166.640929113]: waitForService: Service [/gazebo/set_physics_properties] is now available. [ INFO] [1600950166.659917502, 0.007000000]: Physics dynamic reconfigure ready. Also, the Gazebo simulator will open a new window with the objects defined in diffbot.world except for the Gas station because it is a model that has to be downloaded first, which is happening in the background. TODO: gas station is not showing this way. Empty world of DiffBot. To get the Gas station or other available models it is possible to clone the gazebo models repository into your /home/your_username/.gazebo folder, e.g.: 1 /home/fjp/.gazeb$ git clone osrf/gazebo_models Then add this path inside Gazebo to insert these models into your world file. ### Using ROS launch to Spawn URDF Robots According to the Gazebo roslaunch tutorial the recommended way to spawn a robot into Gazebo is to use a launch file. Therefore, edit the diffbot.launch inside the diffbot_gazebo package by adding the following inside the <launch> </launch tag: 1 2 3 4 5 6 7 <!-- Load the URDF into the ROS Parameter Server --> <param name=\"robot_description\" command=\"$(find xacro)/xacro --inorder '$(find diffbot_description)/urdf/diffbot.xacro'\" /> <!-- Run a python script to the send a service call to gazebo_ros to spawn a URDF robot --> <node name=\"urdf_spawner\" pkg=\"gazebo_ros\" type=\"spawn_model\" respawn=\"false\" output=\"screen\" args=\"-urdf -model diffbot -param robot_description\"/> See also the complete diffbot.launch file. This will open Gazebo simulator and show the DiffBot model: Empty world including DiffBot. Moving the Robot Note that the robot cannot be moved without having either a Gazebo plugin loaded or making use of ROS Control and its Gazebo plugin gazebo_ros_control , see also the Gazebo ROS Control Tutorial . Using the ROS Control and its Gazebo plugin is done in case of DiffBot. An alternative would be to use the existing differential_drive_controller Gazebo plugin without having to rely on ROS Control. The next section explains the diffbot_control package in more detail and how to setup the diff_drive_controller from the ros_controllers package. Adding Sensors To add sensors to a robot model make use of link and joint tags to define the desired location and shape, possibly using meshes. For the simulation of these sensor there exist common Gazebo plugins that can be used. See, the [Tutorial: Using Gazebo plugins with ROS] for existing plugins and more details how to use them. For a full list of plugins see also gazebo_ros_pkgs which is a package or interface for using ROS with the Gazebo simulator. Camera This section follows Gazebo tutorial Adding a Camera . Laser (Lidar) This section follows Gazebo tutorial Adding a Laser GPU . Ultrasonic Ranger See the source of the gazebo_ros_range plugin. Inertial Measurement Unit (IMU) This section follows Gazebo tutorial [Adding an IMU](http://gazebosim.org/tutorials?tut=ros_gzplugins#IMU(GazeboRosImu). Note use GazeboRosImuSensor? Troubleshooting A quick way to verify if the conversion from xacro to urdf to sdf is working is the following ( source: Tutorial URDF in Gazebo ): First convert the xacro model to a urdf model with the xacro command: 1 xacro src/diffbot_description/urdf/diffbot.xacro -o diffbot.urdf This will output the urdf into a file named diffbot.urdf in the current working directory. Then use the gz command to create a sdf: 1 2 # gazebo3 and above gz sdf -p MODEL.urdf DiffBot sdf. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 <sdf version= '1.7' > <model name= 'diffbot' > <link name= 'base_footprint' > <inertial> <pose> -0.012273 0 0.040818 0 -0 0 </pose> <mass> 5.5 </mass> <inertia> <ixx> 0.0387035 </ixx> <ixy> 0 </ixy> <ixz> 0.000552273 </ixz> <iyy> 0.0188626 </iyy> <iyz> 0 </iyz> <izz> 0.0561591 </izz> </inertia> </inertial> <collision name= 'base_footprint_collision' > <pose> 0 0 0 0 -0 0 </pose> <geometry> <box> <size> 0.001 0.001 0.001 </size> </box> </geometry> <surface> <contact> <ode/> </contact> <friction> <ode/> </friction> </surface> </collision> <collision name= 'base_footprint_fixed_joint_lump__base_link_collision_1' > <pose> 0 0 0.04 0 -0 0 </pose> <geometry> <box> <size> 0.3 0.15 0.02 </size> </box> </geometry> <surface> <contact> <ode/> </contact> <friction> <ode/> </friction> </surface> </collision> <collision name= 'base_footprint_fixed_joint_lump__caster_link_collision_2' > <pose> -0.135 0 0.029 0 -0 0 </pose> <geometry> <sphere> <radius> 0.025 </radius> </sphere> </geometry> <surface> <contact> <ode/> </contact> <friction> <ode/> </friction> </surface> </collision> <visual name= 'base_footprint_fixed_joint_lump__base_link_visual' > <pose> 0 0 0.04 0 -0 0 </pose> <geometry> <box> <size> 0.3 0.15 0.02 </size> </box> </geometry> <material> <script> <name> Gazebo/White </name> <uri> file://media/materials/scripts/gazebo.material </uri> </script> </material> </visual> <visual name= 'base_footprint_fixed_joint_lump__caster_link_visual_1' > <pose> -0.115 0 0.029 0 -0 0 </pose> <geometry> <sphere> <radius> 0.025 </radius> </sphere> </geometry> </visual> <velocity_decay/> <velocity_decay/> <gravity> 1 </gravity> <velocity_decay/> </link> <joint name= 'front_left_wheel_joint' type= 'revolute' > <pose relative_to= 'base_footprint' > 0.105 -0.085 0.04 0 -0 0 </pose> <parent> base_footprint </parent> <child> front_left_wheel </child> <axis> <xyz> 0 1 0 </xyz> <limit> <lower> -1e+16 </lower> <upper> 1e+16 </upper> </limit> <dynamics> <spring_reference> 0 </spring_reference> <spring_stiffness> 0 </spring_stiffness> </dynamics> </axis> </joint> <link name= 'front_left_wheel' > <pose relative_to= 'front_left_wheel_joint' > 0 0 0 0 -0 0 </pose> <inertial> <pose> 0 0 0 0 -0 0 </pose> <mass> 2.5 </mass> <inertia> <ixx> 0.00108333 </ixx> <ixy> 0 </ixy> <ixz> 0 </ixz> <iyy> 0.00108333 </iyy> <iyz> 0 </iyz> <izz> 0.002 </izz> </inertia> </inertial> <collision name= 'front_left_wheel_collision' > <pose> 0 0 0 1.5708 -0 0 </pose> <geometry> <cylinder> <length> 0.02 </length> <radius> 0.04 </radius> </cylinder> </geometry> <surface> <contact> <ode> <kp> 1e+07 </kp> <kd> 1 </kd> </ode> </contact> <friction> <ode> <mu> 1 </mu> <mu2> 1 </mu2> <fdir1> 1 0 0 </fdir1> </ode> </friction> </surface> </collision> <visual name= 'front_left_wheel_visual' > <pose> 0 0 0 1.5708 -0 0 </pose> <geometry> <cylinder> <length> 0.02 </length> <radius> 0.04 </radius> </cylinder> </geometry> <material> <script> <name> Gazebo/Grey </name> <uri> file://media/materials/scripts/gazebo.material </uri> </script> </material> </visual> <gravity> 1 </gravity> <velocity_decay/> </link> <joint name= 'front_right_wheel_joint' type= 'revolute' > <pose relative_to= 'base_footprint' > 0.105 0.085 0.04 0 -0 0 </pose> <parent> base_footprint </parent> <child> front_right_wheel </child> <axis> <xyz> 0 1 0 </xyz> <limit> <lower> -1e+16 </lower> <upper> 1e+16 </upper> </limit> <dynamics> <spring_reference> 0 </spring_reference> <spring_stiffness> 0 </spring_stiffness> </dynamics> </axis> </joint> <link name= 'front_right_wheel' > <pose relative_to= 'front_right_wheel_joint' > 0 0 0 0 -0 0 </pose> <inertial> <pose> 0 0 0 0 -0 0 </pose> <mass> 2.5 </mass> <inertia> <ixx> 0.00108333 </ixx> <ixy> 0 </ixy> <ixz> 0 </ixz> <iyy> 0.00108333 </iyy> <iyz> 0 </iyz> <izz> 0.002 </izz> </inertia> </inertial> <collision name= 'front_right_wheel_collision' > <pose> 0 0 0 1.5708 -0 0 </pose> <geometry> <cylinder> <length> 0.02 </length> <radius> 0.04 </radius> </cylinder> </geometry> <surface> <contact> <ode> <kp> 1e+07 </kp> <kd> 1 </kd> </ode> </contact> <friction> <ode> <mu> 1 </mu> <mu2> 1 </mu2> <fdir1> 1 0 0 </fdir1> </ode> </friction> </surface> </collision> <visual name= 'front_right_wheel_visual' > <pose> 0 0 0 1.5708 -0 0 </pose> <geometry> <cylinder> <length> 0.02 </length> <radius> 0.04 </radius> </cylinder> </geometry> <material> <script> <name> Gazebo/Grey </name> <uri> file://media/materials/scripts/gazebo.material </uri> </script> </material> </visual> <gravity> 1 </gravity> <velocity_decay/> </link> <plugin name= 'gazebo_ros_control' filename= 'libgazebo_ros_control.so' > <robotNamespace> /rrbot </robotNamespace> <robotSimType> gazebo_ros_control/DefaultRobotHWSim </robotSimType> </plugin> <static> 0 </static> <plugin name= 'differential_drive_controller' filename= 'libgazebo_ros_diff_drive.so' > <legacyMode> 1 </legacyMode> <rosDebugLevel> Debug </rosDebugLevel> <publishWheelTF> 0 </publishWheelTF> <robotNamespace> / </robotNamespace> <publishTf> 1 </publishTf> <publishWheelJointState> 0 </publishWheelJointState> <alwaysOn> 1 </alwaysOn> <updateRate> 100.0 </updateRate> <leftJoint> front_left_wheel_joint </leftJoint> <rightJoint> front_right_wheel_joint </rightJoint> <wheelSeparation> 0.3 </wheelSeparation> <wheelDiameter> 0.08 </wheelDiameter> <broadcastTF> 1 </broadcastTF> <wheelTorque> 30 </wheelTorque> <wheelAcceleration> 1.8 </wheelAcceleration> <commandTopic> cmd_vel </commandTopic> <odometryFrame> odom </odometryFrame> <odometryTopic> odom </odometryTopic> <robotBaseFrame> base_footprint </robotBaseFrame> </plugin> </model> </sdf>","title":"Simulation"},{"location":"diffbot_gazebo/#simulate-diffbot-in-gazebo","text":"As described in the Creating your own Gazebo ROS Package , it is common in ROS to create a package that contains all the world files and launch files used with Gazebo. These files are located in a ROS package named /MYROBOT_gazebo . For DiffBot the package is named diffbot_gazebo . Another example that follows best pratices is rrbot which can be found in the gazebo_ros_demos repository. 1 2 3 4 5 fjp@ubuntu:~/git/diffbot/ros/src$ catkin create pkg diffbot_gazebo Creating package \"diffbot_gazebo\" in \"/home/fjp/git/diffbot/ros/src\"... Created file diffbot_gazebo/package.xml Created file diffbot_gazebo/CMakeLists.txt Successfully created package files in /home/fjp/git/diffbot/ros/src/diffbot_gazebo. The diffbot_gazebo package contains a launch file to lauch a world in Gazebo and spawn the robot model, which is defined in the previously created diffbot_description package. For the launch files the convention is to have a folder named launch and for Gazebo world files a folder named world inside a package. 1 fjp@ubuntu:~/git/diffbot/ros/src/diffbot_gazebo$ mkdir launch world Inside the launch folder is the diffbot.launch . ```xml 1 2 3 4 5 6 7 8 9 <!-- We resume the logic in empty_world.launch, changing only the name of the world to be launched --> <include file=\"$(find gazebo_ros)/launch/empty_world.launch\"> <arg name=\"world_name\" value=\"$(find diffbot_gazebo)/worlds/diffbot.world\"/> <arg name=\"debug\" value=\"$(arg debug)\" /> <arg name=\"gui\" value=\"$(arg gui)\" /> <arg name=\"paused\" value=\"$(arg paused)\"/> <arg name=\"use_sim_time\" value=\"$(arg use_sim_time)\"/> <arg name=\"headless\" value=\"$(arg headless)\"/> </include> In the `world` folder of the `diffbot_gazebo` package is the `diffbot.world` file: xml <?xml version=\"1.0\" ?> model://ground_plane model://sun model://gas_station gas_station -2.0 7.0 0 0 0 0 ``` With these files build the catkin workspace and source it to make the new diffbot_gazebo package visible to roslaunch : 1 2 catkin build source devel/setup.zsh Then its possible to launch the diffbot.launch with: 1 roslaunch diffbot_gazebo diffbot.launch This will lead to the following output: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 roslaunch diffbot_gazebo diffbot.launch ... logging to /home/fjp/.ros/log/6be90ef2-fdd8-11ea-9cb3-317fd602d5f2/roslaunch-tensorbook-393333.log Checking log directory for disk usage. This may take a while. Press Ctrl-C to interrupt Done checking log file disk usage. Usage is <1GB. started roslaunch server http://tensorbook:32837/ SUMMARY ======== PARAMETERS * /gazebo/enable_ros_network: True * /rosdistro: noetic * /rosversion: 1.15.8 * /use_sim_time: True NODES / gazebo (gazebo_ros/gzserver) gazebo_gui (gazebo_ros/gzclient) ROS_MASTER_URI=http://localhost:11311 process[gazebo-1]: started with pid [393352] process[gazebo_gui-2]: started with pid [393357] [ INFO] [1600950165.494721382]: Finished loading Gazebo ROS API Plugin. [ INFO] [1600950165.495515766]: waitForService: Service [/gazebo/set_physics_properties] has not been advertised, waiting... [ INFO] [1600950165.649461740]: Finished loading Gazebo ROS API Plugin. [ INFO] [1600950165.650277038]: waitForService: Service [/gazebo_gui/set_physics_properties] has not been advertised, waiting... [ INFO] [1600950166.640929113]: waitForService: Service [/gazebo/set_physics_properties] is now available. [ INFO] [1600950166.659917502, 0.007000000]: Physics dynamic reconfigure ready. Also, the Gazebo simulator will open a new window with the objects defined in diffbot.world except for the Gas station because it is a model that has to be downloaded first, which is happening in the background. TODO: gas station is not showing this way. Empty world of DiffBot. To get the Gas station or other available models it is possible to clone the gazebo models repository into your /home/your_username/.gazebo folder, e.g.: 1 /home/fjp/.gazeb$ git clone osrf/gazebo_models Then add this path inside Gazebo to insert these models into your world file. ### Using ROS launch to Spawn URDF Robots According to the Gazebo roslaunch tutorial the recommended way to spawn a robot into Gazebo is to use a launch file. Therefore, edit the diffbot.launch inside the diffbot_gazebo package by adding the following inside the <launch> </launch tag: 1 2 3 4 5 6 7 <!-- Load the URDF into the ROS Parameter Server --> <param name=\"robot_description\" command=\"$(find xacro)/xacro --inorder '$(find diffbot_description)/urdf/diffbot.xacro'\" /> <!-- Run a python script to the send a service call to gazebo_ros to spawn a URDF robot --> <node name=\"urdf_spawner\" pkg=\"gazebo_ros\" type=\"spawn_model\" respawn=\"false\" output=\"screen\" args=\"-urdf -model diffbot -param robot_description\"/> See also the complete diffbot.launch file. This will open Gazebo simulator and show the DiffBot model: Empty world including DiffBot.","title":"Simulate DiffBot in Gazebo"},{"location":"diffbot_gazebo/#moving-the-robot","text":"Note that the robot cannot be moved without having either a Gazebo plugin loaded or making use of ROS Control and its Gazebo plugin gazebo_ros_control , see also the Gazebo ROS Control Tutorial . Using the ROS Control and its Gazebo plugin is done in case of DiffBot. An alternative would be to use the existing differential_drive_controller Gazebo plugin without having to rely on ROS Control. The next section explains the diffbot_control package in more detail and how to setup the diff_drive_controller from the ros_controllers package.","title":"Moving the Robot"},{"location":"diffbot_gazebo/#adding-sensors","text":"To add sensors to a robot model make use of link and joint tags to define the desired location and shape, possibly using meshes. For the simulation of these sensor there exist common Gazebo plugins that can be used. See, the [Tutorial: Using Gazebo plugins with ROS] for existing plugins and more details how to use them. For a full list of plugins see also gazebo_ros_pkgs which is a package or interface for using ROS with the Gazebo simulator.","title":"Adding Sensors"},{"location":"diffbot_gazebo/#camera","text":"This section follows Gazebo tutorial Adding a Camera .","title":"Camera"},{"location":"diffbot_gazebo/#laser-lidar","text":"This section follows Gazebo tutorial Adding a Laser GPU .","title":"Laser (Lidar)"},{"location":"diffbot_gazebo/#ultrasonic-ranger","text":"See the source of the gazebo_ros_range plugin.","title":"Ultrasonic Ranger"},{"location":"diffbot_gazebo/#inertial-measurement-unit-imu","text":"This section follows Gazebo tutorial [Adding an IMU](http://gazebosim.org/tutorials?tut=ros_gzplugins#IMU(GazeboRosImu). Note use GazeboRosImuSensor?","title":"Inertial Measurement Unit (IMU)"},{"location":"diffbot_gazebo/#troubleshooting","text":"A quick way to verify if the conversion from xacro to urdf to sdf is working is the following ( source: Tutorial URDF in Gazebo ): First convert the xacro model to a urdf model with the xacro command: 1 xacro src/diffbot_description/urdf/diffbot.xacro -o diffbot.urdf This will output the urdf into a file named diffbot.urdf in the current working directory. Then use the gz command to create a sdf: 1 2 # gazebo3 and above gz sdf -p MODEL.urdf DiffBot sdf. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 <sdf version= '1.7' > <model name= 'diffbot' > <link name= 'base_footprint' > <inertial> <pose> -0.012273 0 0.040818 0 -0 0 </pose> <mass> 5.5 </mass> <inertia> <ixx> 0.0387035 </ixx> <ixy> 0 </ixy> <ixz> 0.000552273 </ixz> <iyy> 0.0188626 </iyy> <iyz> 0 </iyz> <izz> 0.0561591 </izz> </inertia> </inertial> <collision name= 'base_footprint_collision' > <pose> 0 0 0 0 -0 0 </pose> <geometry> <box> <size> 0.001 0.001 0.001 </size> </box> </geometry> <surface> <contact> <ode/> </contact> <friction> <ode/> </friction> </surface> </collision> <collision name= 'base_footprint_fixed_joint_lump__base_link_collision_1' > <pose> 0 0 0.04 0 -0 0 </pose> <geometry> <box> <size> 0.3 0.15 0.02 </size> </box> </geometry> <surface> <contact> <ode/> </contact> <friction> <ode/> </friction> </surface> </collision> <collision name= 'base_footprint_fixed_joint_lump__caster_link_collision_2' > <pose> -0.135 0 0.029 0 -0 0 </pose> <geometry> <sphere> <radius> 0.025 </radius> </sphere> </geometry> <surface> <contact> <ode/> </contact> <friction> <ode/> </friction> </surface> </collision> <visual name= 'base_footprint_fixed_joint_lump__base_link_visual' > <pose> 0 0 0.04 0 -0 0 </pose> <geometry> <box> <size> 0.3 0.15 0.02 </size> </box> </geometry> <material> <script> <name> Gazebo/White </name> <uri> file://media/materials/scripts/gazebo.material </uri> </script> </material> </visual> <visual name= 'base_footprint_fixed_joint_lump__caster_link_visual_1' > <pose> -0.115 0 0.029 0 -0 0 </pose> <geometry> <sphere> <radius> 0.025 </radius> </sphere> </geometry> </visual> <velocity_decay/> <velocity_decay/> <gravity> 1 </gravity> <velocity_decay/> </link> <joint name= 'front_left_wheel_joint' type= 'revolute' > <pose relative_to= 'base_footprint' > 0.105 -0.085 0.04 0 -0 0 </pose> <parent> base_footprint </parent> <child> front_left_wheel </child> <axis> <xyz> 0 1 0 </xyz> <limit> <lower> -1e+16 </lower> <upper> 1e+16 </upper> </limit> <dynamics> <spring_reference> 0 </spring_reference> <spring_stiffness> 0 </spring_stiffness> </dynamics> </axis> </joint> <link name= 'front_left_wheel' > <pose relative_to= 'front_left_wheel_joint' > 0 0 0 0 -0 0 </pose> <inertial> <pose> 0 0 0 0 -0 0 </pose> <mass> 2.5 </mass> <inertia> <ixx> 0.00108333 </ixx> <ixy> 0 </ixy> <ixz> 0 </ixz> <iyy> 0.00108333 </iyy> <iyz> 0 </iyz> <izz> 0.002 </izz> </inertia> </inertial> <collision name= 'front_left_wheel_collision' > <pose> 0 0 0 1.5708 -0 0 </pose> <geometry> <cylinder> <length> 0.02 </length> <radius> 0.04 </radius> </cylinder> </geometry> <surface> <contact> <ode> <kp> 1e+07 </kp> <kd> 1 </kd> </ode> </contact> <friction> <ode> <mu> 1 </mu> <mu2> 1 </mu2> <fdir1> 1 0 0 </fdir1> </ode> </friction> </surface> </collision> <visual name= 'front_left_wheel_visual' > <pose> 0 0 0 1.5708 -0 0 </pose> <geometry> <cylinder> <length> 0.02 </length> <radius> 0.04 </radius> </cylinder> </geometry> <material> <script> <name> Gazebo/Grey </name> <uri> file://media/materials/scripts/gazebo.material </uri> </script> </material> </visual> <gravity> 1 </gravity> <velocity_decay/> </link> <joint name= 'front_right_wheel_joint' type= 'revolute' > <pose relative_to= 'base_footprint' > 0.105 0.085 0.04 0 -0 0 </pose> <parent> base_footprint </parent> <child> front_right_wheel </child> <axis> <xyz> 0 1 0 </xyz> <limit> <lower> -1e+16 </lower> <upper> 1e+16 </upper> </limit> <dynamics> <spring_reference> 0 </spring_reference> <spring_stiffness> 0 </spring_stiffness> </dynamics> </axis> </joint> <link name= 'front_right_wheel' > <pose relative_to= 'front_right_wheel_joint' > 0 0 0 0 -0 0 </pose> <inertial> <pose> 0 0 0 0 -0 0 </pose> <mass> 2.5 </mass> <inertia> <ixx> 0.00108333 </ixx> <ixy> 0 </ixy> <ixz> 0 </ixz> <iyy> 0.00108333 </iyy> <iyz> 0 </iyz> <izz> 0.002 </izz> </inertia> </inertial> <collision name= 'front_right_wheel_collision' > <pose> 0 0 0 1.5708 -0 0 </pose> <geometry> <cylinder> <length> 0.02 </length> <radius> 0.04 </radius> </cylinder> </geometry> <surface> <contact> <ode> <kp> 1e+07 </kp> <kd> 1 </kd> </ode> </contact> <friction> <ode> <mu> 1 </mu> <mu2> 1 </mu2> <fdir1> 1 0 0 </fdir1> </ode> </friction> </surface> </collision> <visual name= 'front_right_wheel_visual' > <pose> 0 0 0 1.5708 -0 0 </pose> <geometry> <cylinder> <length> 0.02 </length> <radius> 0.04 </radius> </cylinder> </geometry> <material> <script> <name> Gazebo/Grey </name> <uri> file://media/materials/scripts/gazebo.material </uri> </script> </material> </visual> <gravity> 1 </gravity> <velocity_decay/> </link> <plugin name= 'gazebo_ros_control' filename= 'libgazebo_ros_control.so' > <robotNamespace> /rrbot </robotNamespace> <robotSimType> gazebo_ros_control/DefaultRobotHWSim </robotSimType> </plugin> <static> 0 </static> <plugin name= 'differential_drive_controller' filename= 'libgazebo_ros_diff_drive.so' > <legacyMode> 1 </legacyMode> <rosDebugLevel> Debug </rosDebugLevel> <publishWheelTF> 0 </publishWheelTF> <robotNamespace> / </robotNamespace> <publishTf> 1 </publishTf> <publishWheelJointState> 0 </publishWheelJointState> <alwaysOn> 1 </alwaysOn> <updateRate> 100.0 </updateRate> <leftJoint> front_left_wheel_joint </leftJoint> <rightJoint> front_right_wheel_joint </rightJoint> <wheelSeparation> 0.3 </wheelSeparation> <wheelDiameter> 0.08 </wheelDiameter> <broadcastTF> 1 </broadcastTF> <wheelTorque> 30 </wheelTorque> <wheelAcceleration> 1.8 </wheelAcceleration> <commandTopic> cmd_vel </commandTopic> <odometryFrame> odom </odometryFrame> <odometryTopic> odom </odometryTopic> <robotBaseFrame> base_footprint </robotBaseFrame> </plugin> </model> </sdf>","title":"Troubleshooting"},{"location":"diffbot_mbf/","text":"DiffBot Move Base Flex As described in the move_base_flex ROS wiki : Move Base Flex (MBF) is a backwards-compatible replacement for move_base. MBF can use existing plugins for move_base, and provides an enhanced version of the planner, controller and recovery plugin ROS interfaces. It exposes action servers for planning, controlling and recovering, providing detailed information of the current state and the plugin\u2019s feedback. An external executive logic can use MBF and its actions to perform smart and flexible navigation strategies. Furthermore, MBF enables the use of other map representations, e.g. meshes or grid_map This package is a meta package and refers to the Move Base Flex stack packages.The abstract core of MBF \u2013 without any binding to a map representation \u2013 is represented by the mbf_abstract_nav and the mbf_abstract_core . For navigation on costmaps see mbf_costmap_nav and mbf_costmap_core . This diffbot_mbf package was created using catkin-tools using the following command: 1 2 3 4 5 catkin create pkg diffbot_mbf Creating package \"diffbot_mbf\" in \"/home/fjp/git/ros_ws/src/diffbot\"... Created file diffbot_mbf/package.xml Created file diffbot_mbf/CMakeLists.txt Successfully created package files in /home/fjp/git/ros_ws/src/diffbot/diffbot_mbf. Additionally the following Ubuntu packages are required dependencies of move_base_flex : 1 sudo apt install ros-noetic-mbf-costmap-nav Another working example for turtlebot3 can be found in the turtlebot3_mbf package.","title":"Move Base Flex"},{"location":"diffbot_mbf/#diffbot-move-base-flex","text":"As described in the move_base_flex ROS wiki : Move Base Flex (MBF) is a backwards-compatible replacement for move_base. MBF can use existing plugins for move_base, and provides an enhanced version of the planner, controller and recovery plugin ROS interfaces. It exposes action servers for planning, controlling and recovering, providing detailed information of the current state and the plugin\u2019s feedback. An external executive logic can use MBF and its actions to perform smart and flexible navigation strategies. Furthermore, MBF enables the use of other map representations, e.g. meshes or grid_map This package is a meta package and refers to the Move Base Flex stack packages.The abstract core of MBF \u2013 without any binding to a map representation \u2013 is represented by the mbf_abstract_nav and the mbf_abstract_core . For navigation on costmaps see mbf_costmap_nav and mbf_costmap_core . This diffbot_mbf package was created using catkin-tools using the following command: 1 2 3 4 5 catkin create pkg diffbot_mbf Creating package \"diffbot_mbf\" in \"/home/fjp/git/ros_ws/src/diffbot\"... Created file diffbot_mbf/package.xml Created file diffbot_mbf/CMakeLists.txt Successfully created package files in /home/fjp/git/ros_ws/src/diffbot/diffbot_mbf. Additionally the following Ubuntu packages are required dependencies of move_base_flex : 1 sudo apt install ros-noetic-mbf-costmap-nav Another working example for turtlebot3 can be found in the turtlebot3_mbf package.","title":"DiffBot Move Base Flex"},{"location":"diffbot_msgs/","text":"DiffBot Messages Package As mentioned before, the nodes in ROS communicate with each other by publishing messages to topics . ROS provides the std_msgs package that includes ROS' common message types to represent primitive data types (see the ROS msg specification for primitive types) and other basic message constructs, such as multiarrays. Note howerver, the following from the std_msgs documentation : Quote The types in std_msgs do not convey semantic meaning about their contents: every message simply has a field called \"data\". Therefore, while the messages in this package can be useful for quick prototyping, they are NOT intended for \"long-term\" usage. For ease of documentation and collaboration, we recommend that existing messages be used, or new messages created, that provide meaningful field name(s). Therefore, we create a package that contains message definitions specific to DiffBot. The following command uses catkin-tools to create the diffbot_msgs package: 1 2 3 4 5 6 ros_ws/src$ catkin create pkg diffbot_msgs --catkin-deps message_generation std_msgs Creating package \"diffbot_msgs\" in \"/home/fjp/git/ros_ws/src\"... WARNING: Packages with messages or services should depend on both message_generation and message_runtime Created file diffbot_msgs/package.xml Created file diffbot_msgs/CMakeLists.txt Successfully created package files in /home/fjp/git/ros_ws/src/diffbot_msgs. Note The following is based on ROS Tutorials Creating Msg And Srv . In this tutorial you can find the required configurations for the package.xml and CMakeLists.txt . Currently there is no encoder message definition in ROS (see the sensor_msgs package) which is why a dedicated message is created for the encoders. For this, a simple msg description file, named Encoder.msg is created in the msg/ subdirectory of this diffbot_msgs package: 1 2 3 4 5 6 7 8 # This is a message to hold number of ticks from Encoders Header header # Use an array of size two of type int32 for the two encoders. # int32 is used instead of int64 because it is not supporte by Arduino/Teensy. # An overflow is also unlikely with the encoders of the DG01D-E # motor with encoder because of its low encoder resolution int32[2] encoders The message includes the message type Header (see also Header msg ) which includes common metadata fileds such as timestamp that is automatically set by ROS client libraries . Having this encoder message description gives semantic meaning to the encoder messages and for example avoids having two separate int32 publishers for each encoder. Combining the encoder message into a single one alleviates additional timing problems. There exists also the common_msgs meta package for common, generic robot-specific message types. From the common_msgs DiffBot uses for example the nav_msgs for navigation with the navigation stack . Other relevant message definitions are the sensor_msgs/Imu and sensor_msgs/LaserScan ](http://docs.ros.org/en/api/sensor_msgs/html/msg/LaserScan.html), where both are definitions from the sensor_msgs package. Using rosmsg After building the package and its messages using catkin build let's make sure that ROS can see it using the rosmsg show command. 1 2 3 4 5 6 $ rosmsg show diffbot_msgs/Encoder std_msgs/Header header uint32 seq time stamp string frame_id int32[2] encoders ROSSerial The generated messages in this packages are used on the Teensy microcontroller, which is using rosserial . Integrating these messages requires the following steps. Generate rosserial libraries in a temporary folder using the make_libraries script: 1 rosrun rosserial_client make_libraries ~/Arduino/tmp/ Copy the generated ~/Arduino/tmp/diffbot_msgs message folder to the src folder of the rosserial Arduino library. When rosserial was installed with the Arduino Library Manager, the location is ~/Arduino/libraries/Rosserial_Arduino_Library/ . Usage The new messages, specific to DiffBot, can be used by including the generated header, for example #include <diffbot_msgs/Encoder.h> . References Tutorials Arduino IDE Setup","title":"Messages"},{"location":"diffbot_msgs/#diffbot-messages-package","text":"As mentioned before, the nodes in ROS communicate with each other by publishing messages to topics . ROS provides the std_msgs package that includes ROS' common message types to represent primitive data types (see the ROS msg specification for primitive types) and other basic message constructs, such as multiarrays. Note howerver, the following from the std_msgs documentation : Quote The types in std_msgs do not convey semantic meaning about their contents: every message simply has a field called \"data\". Therefore, while the messages in this package can be useful for quick prototyping, they are NOT intended for \"long-term\" usage. For ease of documentation and collaboration, we recommend that existing messages be used, or new messages created, that provide meaningful field name(s). Therefore, we create a package that contains message definitions specific to DiffBot. The following command uses catkin-tools to create the diffbot_msgs package: 1 2 3 4 5 6 ros_ws/src$ catkin create pkg diffbot_msgs --catkin-deps message_generation std_msgs Creating package \"diffbot_msgs\" in \"/home/fjp/git/ros_ws/src\"... WARNING: Packages with messages or services should depend on both message_generation and message_runtime Created file diffbot_msgs/package.xml Created file diffbot_msgs/CMakeLists.txt Successfully created package files in /home/fjp/git/ros_ws/src/diffbot_msgs. Note The following is based on ROS Tutorials Creating Msg And Srv . In this tutorial you can find the required configurations for the package.xml and CMakeLists.txt . Currently there is no encoder message definition in ROS (see the sensor_msgs package) which is why a dedicated message is created for the encoders. For this, a simple msg description file, named Encoder.msg is created in the msg/ subdirectory of this diffbot_msgs package: 1 2 3 4 5 6 7 8 # This is a message to hold number of ticks from Encoders Header header # Use an array of size two of type int32 for the two encoders. # int32 is used instead of int64 because it is not supporte by Arduino/Teensy. # An overflow is also unlikely with the encoders of the DG01D-E # motor with encoder because of its low encoder resolution int32[2] encoders The message includes the message type Header (see also Header msg ) which includes common metadata fileds such as timestamp that is automatically set by ROS client libraries . Having this encoder message description gives semantic meaning to the encoder messages and for example avoids having two separate int32 publishers for each encoder. Combining the encoder message into a single one alleviates additional timing problems. There exists also the common_msgs meta package for common, generic robot-specific message types. From the common_msgs DiffBot uses for example the nav_msgs for navigation with the navigation stack . Other relevant message definitions are the sensor_msgs/Imu and sensor_msgs/LaserScan ](http://docs.ros.org/en/api/sensor_msgs/html/msg/LaserScan.html), where both are definitions from the sensor_msgs package.","title":"DiffBot Messages Package"},{"location":"diffbot_msgs/#using-rosmsg","text":"After building the package and its messages using catkin build let's make sure that ROS can see it using the rosmsg show command. 1 2 3 4 5 6 $ rosmsg show diffbot_msgs/Encoder std_msgs/Header header uint32 seq time stamp string frame_id int32[2] encoders","title":"Using rosmsg"},{"location":"diffbot_msgs/#rosserial","text":"The generated messages in this packages are used on the Teensy microcontroller, which is using rosserial . Integrating these messages requires the following steps. Generate rosserial libraries in a temporary folder using the make_libraries script: 1 rosrun rosserial_client make_libraries ~/Arduino/tmp/ Copy the generated ~/Arduino/tmp/diffbot_msgs message folder to the src folder of the rosserial Arduino library. When rosserial was installed with the Arduino Library Manager, the location is ~/Arduino/libraries/Rosserial_Arduino_Library/ .","title":"ROSSerial"},{"location":"diffbot_msgs/#usage","text":"The new messages, specific to DiffBot, can be used by including the generated header, for example #include <diffbot_msgs/Encoder.h> .","title":"Usage"},{"location":"diffbot_msgs/#references","text":"Tutorials Arduino IDE Setup","title":"References"},{"location":"diffbot_navigation/","text":"DiffBot Navigation Package Navigation Stack Overview. 1 2 3 4 5 fjp@diffbot:~/catkin_ws/src/diffbot$ catkin create pkg diffbot_navigation --catkin-deps amcl map_server move_base diffbot_bringup Creating package \"diffbot_navigation\" in \"/home/fjp/git/ros_ws/src/diffbot\"... Created file diffbot_navigation/package.xml Created file diffbot_navigation/CMakeLists.txt Successfully created package files in /home/fjp/git/ros_ws/src/diffbot/diffbot_navigation. We also need the following ROS packages that can be installed from the ROS Ubuntu packages: 1 $ sudo apt install ros-noetic-dwa-local-planner ros-noetic-amcl ros-noetic-map-server ros-noetic-move-base After this we create the required launch files and parameter configurations. These will be used for the simulation and the real robot. First we focus on the simulation in Gazebo. Launch files All launch files are in the folder named launch of the diffbot_navigation package. Inside the move_base.launch it is important to remap the following topics: 1 2 3 4 5 6 7 <!-- Arguments --> <arg name= \"cmd_vel_topic\" default= \"/diffbot/mobile_base_controller/cmd_vel\" /> <arg name= \"odom_topic\" default= \"/diffbot/mobile_base_controller/odom\" /> ... <!-- remappings of move_base node --> <remap from= \"cmd_vel\" to= \"$(arg cmd_vel_topic)\" /> <remap from= \"odom\" to= \"$(arg odom_topic)\" /> Parameter Configuration The parameters for the navigation package go into the config (for some robots named param ) folder. Most of them can be changed during runtime using dynamic reconfigure with the rqt_reconfigure gui . Setup and Configuration of the Navigation Stack on a Robot amcl : amcl is a probabilistic localization system for a robot moving in 2D. It implements the adaptive (or KLD-sampling) Monte Carlo localization approach (as described by Dieter Fox), which uses a particle filter to track the pose of a robot against a known map. map_server : provides the map_server ROS Node, which offers map data as a ROS Service. It also provides the map_saver command-line utility, which allows dynamically generated maps to be saved to file. move_base : The move_base package provides an implementation of an action (see the actionlib package) that, given a goal in the world, will attempt to reach it with a mobile base. The move_base node links together a global and local planner to accomplish its global navigation task. It supports any global planner adhering to the nav_core::BaseGlobalPlanner interface specified in the nav_core package and any local planner adhering to the nav_core::BaseLocalPlanner interface specified in the nav_core package. The move_base node also maintains two costmaps, one for the global planner, and one for a local planner (see the costmap_2d package) that are used to accomplish navigation tasks. gmapping : This package contains a ROS wrapper for OpenSlam's Gmapping. The gmapping package provides laser-based SLAM (Simultaneous Localization and Mapping), as a ROS node called slam_gmapping. Using slam_gmapping, you can create a 2-D occupancy grid map (like a building floorplan) from laser and pose data collected by a mobile robot. ROS cartographer slam_toolbox Examples - TurtleBot3 Navigation Navigation in Gazebo with available Map To navigate the robot in the simulation run the following command but make sure to first download the turtlebot3_world to your ~/.gazebo/models/ folder. This is required because the turtlebot3_world.world file references the turtlebot3_world model. 1 roslaunch diffbot_navigation diffbot.launch world_name:='$(find diffbot_gazebo)/worlds/turtlebot3_world.world' This will spawn DiffBot inside the turtlebot3 world inside Gazebo and visualize the elements of the navigation stack in RViz. Navigation demo of DiffBot (click to view the demo on Youtube). To navigate the robot using the default DWA planner in the known map, coming from the running map_server , you can use the 2D Nav Goal in RViz . Just select the navigation arrow to where the robot should move as shown in the animation above. The DWA local planner is working for differential drive robots, like DiffBot. For other robots such as non-holonomic robots or other types of mobile robots (also differential drive robots) other planners can be used. See for example teb_local_planner . {: .notice } Resources Global Planners: - global_planner Local Planners: - Difference between DWA and Base Local Planner - Difference between DWA and TEB Local Planner","title":"Navigation"},{"location":"diffbot_navigation/#diffbot-navigation-package","text":"Navigation Stack Overview. 1 2 3 4 5 fjp@diffbot:~/catkin_ws/src/diffbot$ catkin create pkg diffbot_navigation --catkin-deps amcl map_server move_base diffbot_bringup Creating package \"diffbot_navigation\" in \"/home/fjp/git/ros_ws/src/diffbot\"... Created file diffbot_navigation/package.xml Created file diffbot_navigation/CMakeLists.txt Successfully created package files in /home/fjp/git/ros_ws/src/diffbot/diffbot_navigation. We also need the following ROS packages that can be installed from the ROS Ubuntu packages: 1 $ sudo apt install ros-noetic-dwa-local-planner ros-noetic-amcl ros-noetic-map-server ros-noetic-move-base After this we create the required launch files and parameter configurations. These will be used for the simulation and the real robot. First we focus on the simulation in Gazebo.","title":"DiffBot Navigation Package"},{"location":"diffbot_navigation/#launch-files","text":"All launch files are in the folder named launch of the diffbot_navigation package. Inside the move_base.launch it is important to remap the following topics: 1 2 3 4 5 6 7 <!-- Arguments --> <arg name= \"cmd_vel_topic\" default= \"/diffbot/mobile_base_controller/cmd_vel\" /> <arg name= \"odom_topic\" default= \"/diffbot/mobile_base_controller/odom\" /> ... <!-- remappings of move_base node --> <remap from= \"cmd_vel\" to= \"$(arg cmd_vel_topic)\" /> <remap from= \"odom\" to= \"$(arg odom_topic)\" />","title":"Launch files"},{"location":"diffbot_navigation/#parameter-configuration","text":"The parameters for the navigation package go into the config (for some robots named param ) folder. Most of them can be changed during runtime using dynamic reconfigure with the rqt_reconfigure gui . Setup and Configuration of the Navigation Stack on a Robot amcl : amcl is a probabilistic localization system for a robot moving in 2D. It implements the adaptive (or KLD-sampling) Monte Carlo localization approach (as described by Dieter Fox), which uses a particle filter to track the pose of a robot against a known map. map_server : provides the map_server ROS Node, which offers map data as a ROS Service. It also provides the map_saver command-line utility, which allows dynamically generated maps to be saved to file. move_base : The move_base package provides an implementation of an action (see the actionlib package) that, given a goal in the world, will attempt to reach it with a mobile base. The move_base node links together a global and local planner to accomplish its global navigation task. It supports any global planner adhering to the nav_core::BaseGlobalPlanner interface specified in the nav_core package and any local planner adhering to the nav_core::BaseLocalPlanner interface specified in the nav_core package. The move_base node also maintains two costmaps, one for the global planner, and one for a local planner (see the costmap_2d package) that are used to accomplish navigation tasks. gmapping : This package contains a ROS wrapper for OpenSlam's Gmapping. The gmapping package provides laser-based SLAM (Simultaneous Localization and Mapping), as a ROS node called slam_gmapping. Using slam_gmapping, you can create a 2-D occupancy grid map (like a building floorplan) from laser and pose data collected by a mobile robot. ROS cartographer slam_toolbox Examples - TurtleBot3 Navigation","title":"Parameter Configuration"},{"location":"diffbot_navigation/#navigation-in-gazebo-with-available-map","text":"To navigate the robot in the simulation run the following command but make sure to first download the turtlebot3_world to your ~/.gazebo/models/ folder. This is required because the turtlebot3_world.world file references the turtlebot3_world model. 1 roslaunch diffbot_navigation diffbot.launch world_name:='$(find diffbot_gazebo)/worlds/turtlebot3_world.world' This will spawn DiffBot inside the turtlebot3 world inside Gazebo and visualize the elements of the navigation stack in RViz. Navigation demo of DiffBot (click to view the demo on Youtube). To navigate the robot using the default DWA planner in the known map, coming from the running map_server , you can use the 2D Nav Goal in RViz . Just select the navigation arrow to where the robot should move as shown in the animation above. The DWA local planner is working for differential drive robots, like DiffBot. For other robots such as non-holonomic robots or other types of mobile robots (also differential drive robots) other planners can be used. See for example teb_local_planner . {: .notice }","title":"Navigation in Gazebo with available Map"},{"location":"diffbot_navigation/#resources","text":"Global Planners: - global_planner Local Planners: - Difference between DWA and Base Local Planner - Difference between DWA and TEB Local Planner","title":"Resources"},{"location":"diffbot_perception/","text":"Perception To provide perception capabilities to your mobile robot you need a sensor like a RGB or RGB-D camera as hardware and software like OpenCV or PCL . Quote OpenCV (Open Source Computer Vision Library) is a library of programming functions mainly aimed at real-time computer vision. Originally developed by Intel, it was later supported by Willow Garage then Itseez (which was later acquired by Intel). The library is cross-platform and free for use under the open-source Apache 2 License. Starting with 2011, OpenCV features GPU acceleration for real-time operations. Quote The Point Cloud Library (PCL) is a standalone, large scale, open project for 2D/3D image and point cloud processing. PCL is released under the terms of the BSD license, and thus free for commercial and research use. When working with OpenCV and ROS we have to install required dependency using ros-noetic-vision-opencv . This will install additional dependencies like OpenCV ( libopencv-dv ) and ros-noetic-cv-bridge . There is no need to install OpenCV from source or the Ubuntu binaries (deb package). If you want to use OpenCV without ROS you should consider installing it from source using CMake . This will allow you to configure what features should be installed, e.g., example code. Otherwise you can install a pre compiled binary libopencv-dev . {: .notice } Similar to work with PCL and ROS you need to install the dependency ros-noetic-perception-pcl , which is the interface between ROS and PCL. To work with PCL without ROS you should consider installing it from source using CMake . This will allow you to configure what features should be installed, e.g., example code. Otherwise you can install a pre compiled binary libpcl-dev . {: .notice } perception_pcl is a meta package for PCL (Point Cloud Library) ROS interface stack. PCL-ROS is the preferred bridge for 3D applications involving n-D Point Clouds and 3D geometry processing in ROS.","title":"Diffbot perception"},{"location":"diffbot_perception/#perception","text":"To provide perception capabilities to your mobile robot you need a sensor like a RGB or RGB-D camera as hardware and software like OpenCV or PCL . Quote OpenCV (Open Source Computer Vision Library) is a library of programming functions mainly aimed at real-time computer vision. Originally developed by Intel, it was later supported by Willow Garage then Itseez (which was later acquired by Intel). The library is cross-platform and free for use under the open-source Apache 2 License. Starting with 2011, OpenCV features GPU acceleration for real-time operations. Quote The Point Cloud Library (PCL) is a standalone, large scale, open project for 2D/3D image and point cloud processing. PCL is released under the terms of the BSD license, and thus free for commercial and research use. When working with OpenCV and ROS we have to install required dependency using ros-noetic-vision-opencv . This will install additional dependencies like OpenCV ( libopencv-dv ) and ros-noetic-cv-bridge . There is no need to install OpenCV from source or the Ubuntu binaries (deb package). If you want to use OpenCV without ROS you should consider installing it from source using CMake . This will allow you to configure what features should be installed, e.g., example code. Otherwise you can install a pre compiled binary libopencv-dev . {: .notice } Similar to work with PCL and ROS you need to install the dependency ros-noetic-perception-pcl , which is the interface between ROS and PCL. To work with PCL without ROS you should consider installing it from source using CMake . This will allow you to configure what features should be installed, e.g., example code. Otherwise you can install a pre compiled binary libpcl-dev . {: .notice } perception_pcl is a meta package for PCL (Point Cloud Library) ROS interface stack. PCL-ROS is the preferred bridge for 3D applications involving n-D Point Clouds and 3D geometry processing in ROS.","title":"Perception"},{"location":"diffbot_robot/","text":"DiffBot Robot This is a ROS metapackage that references all related packages to DiffBot. A metapackage can be created with: 1 2 3 4 fjp@ubuntu:ros_ws/src/$ catkin_create_pkg diffbot_robot --meta Created file diffbot_robot/package.xml Created file diffbot_robot/CMakeLists.txt Successfully created files in /home/fjp/git/ros_ws/src/diffbot/diffbot_robot. Please adjust the values in package.xml. To release a package see the bloom page and the listed tutorials there. Specifically the following ones: To index the package follow the Indexing Your ROS Repository for Documentation Generation . Release a package using bloom, see First Time Release tutorial .","title":"Robot Package"},{"location":"diffbot_robot/#diffbot-robot","text":"This is a ROS metapackage that references all related packages to DiffBot. A metapackage can be created with: 1 2 3 4 fjp@ubuntu:ros_ws/src/$ catkin_create_pkg diffbot_robot --meta Created file diffbot_robot/package.xml Created file diffbot_robot/CMakeLists.txt Successfully created files in /home/fjp/git/ros_ws/src/diffbot/diffbot_robot. Please adjust the values in package.xml. To release a package see the bloom page and the listed tutorials there. Specifically the following ones: To index the package follow the Indexing Your ROS Repository for Documentation Generation . Release a package using bloom, see First Time Release tutorial .","title":"DiffBot Robot"},{"location":"diffbot_slam/","text":"DiffBot Slam Package This package contains launch files and configurations for different simultaneous localization and mapping (SLAM) algorithms to map the environment of the robot in 2D, although some of these algorithms can be used to map in 3D. 1 2 3 4 5 fjp@diffbot:~/catkin_ws/src/diffbot$ catkin create pkg diffbot_slam --catkin-deps diffbot_navigation gmapping Creating package \"diffbot_slam\" in \"/home/fjp/git/ros_ws/src/diffbot\"... Created file diffbot_slam/package.xml Created file diffbot_slam/CMakeLists.txt Successfully created package files in /home/fjp/git/ros_ws/src/diffbot/diffbot_slam. Additional runtime dependencies are: cartographer_ros , hector_slam , frontier_exploration and explore_lite . These are added to this workspace using vcstool (TODO). As you can see this package has lots of dependencies to test different slam implementations and frontier exploration approaches. To run this package these dependencies need to be installed and are set as exec_depend in the package.xml . Currently only gmapping provides a ROS Noetic Ubuntu package that can be installed directly with: 1 sudo apt install ros-noetic-gmapping In case you want to try more advanced SLAM algorithms, such as karto_slam or cartographer_ros you need the following Ubuntu package dependencies. Alternatively you can install from source by building the cloned git repository in your catkin workspace. Take the required installation size into account. For example karto_slam needs approximately 125MB because it will also install ros-noetic-open-karto . {: .notice } 1 sudo apt install ros-noetic-slam-karto SLAM SLAM stands for Simultaneous Localization and Mapping sometimes refered to as Concurrent Localization and Mappping (CLAM). The SLAM algorithm combines localization and mapping, where a robot has access only to its own movement and sensory data. The robot must build a map while simultaneously localizing itself relative to the map. See also this blog post on FastSLAM . To use the following slam algorithms, we need a mobile robot that provides odometry data and is equipped with a horizontally-mounted, fixed, laser range-finder. Viewed on a higher level, every specific slam node of these algorithms will attempt to transform each incoming scan into the odom (odometry) tf frame. Therefore the node will subscribe to the laser /scan and the /tf topics. Transforms are necessary to relate frames for laser, base, and odometry. The only exception is hector_slam which doesn't require odometry for mapping. The following SLAM implementations are offered using the launch files explained in the next section. It is suggested to start with gmapping which is used by default. gmapping : This package contains a ROS wrapper for OpenSlam's Gmapping . The gmapping package provides laser-based SLAM (Simultaneous Localization and Mapping), as a ROS node called slam_gmapping . Using slam_gmapping , you can create a 2-D occupancy grid map (like a building floorplan) from laser and pose data collected by a mobile robot. cartographer : Cartographer is a system that provides real-time simultaneous localization and mapping (SLAM) in 2D and 3D across multiple platforms and sensor configurations. See the documentation for an algorithm walkthrough . karto : This package pulls in the Karto mapping library, and provides a ROS wrapper for using it. Karto is considered more accurate than, for example gmapping (note: for ROS noetic, see slam_karto ) and became open source in 2010 . hector_slam : metapackage that installs hector_mapping and related packages. The hector_mapping is a SLAM approach that can be used without odometry as well as on platforms that exhibit roll/pitch motion (of the sensor, the platform or both), such as drones. It leverages the high update rate of modern LIDAR systems like the Hokuyo UTM-30LX and provides 2D pose estimates at scan rate of the sensors (40Hz for the UTM-30LX). While the system does not provide explicit loop closing ability, it is sufficiently accurate for many real world scenarios. The system has successfully been used on Unmanned Ground Robots, Unmanned Surface Vehicles, Handheld Mapping Devices and logged data from quadrotor UAVs. Unlike gmapping which uses a particle filter , karto , cartographer and hector_slam are all graph-based SLAM algorithms . The least accurate SLAM algorithm is gmapping but it works fine for smaller maps. Use other algorithms, such as karto if you operate your robot in larger environments or you want more accuracy. Another interesing package is slam_toolbox which provides ROS1 and ROS2 support and is based on the easy to use karto algorithm. karto is the basis for many companies because it provides an excellent scan matcher and can operate in large environments. Additionally, slam_toolbox provides tools to edit a generated map and even create a high quality map using stored data (offline). The cartographer package is currently supported by OpenRobotics and not by Google where it was originally developed. It is currently also not setup correctly for DiffBot. Using it will result in errors. {: .notice } Launch files This package provides a main launch file named diffbot_slam.launch which accepts an argument slam_method . Depending on its value, different launch files will be included that execute the specified SLAM algorithm using its configuration in the config folder. As mentioned above, every ROS slam package requries messages from the laser-range finder topic. Usually this topic is named /scan . To distinguish possible multiple lidars, the topic of DiffBot resides in its namespace /diffbot/scan . Therefore, its necessary to remap the /scan topic to /diffbot/scan . The following shows how this was done for the gmapping launch file. Inside this package in the launch/gmapping.launch it is important to map the scan topic to laser scanner topic published by Diffbot. Remappings are done in the node tag . Here, for the gmapping.launch in the gmapping node: 1 2 3 4 5 6 7 8 9 10 11 <launch> <!-- Arguments --> <arg name= \"scan_topic\" default= \"diffbot/scan\" /> ... <!-- Gmapping --> <node pkg= \"gmapping\" type= \"slam_gmapping\" name= \"diffbot_slam_gmapping\" output= \"screen\" > ... <!-- remapping of gmapping node --> <remap from= \"scan\" to= \"$(arg scan_topic)\" /> </node> </launch> Parameter Configurations Most of the configrations are the same as turtlebot3_slam/config . For detailed description of what each parameter does, please check the individual package documentation of the different SLAM methods. Gazebo Simulation Tests To test SLAM in the Gazebo simulator run the following two launch files in separate terminals. First run the simulation with: 1 roslaunch diffbot_gazebo diffbot.launch world_name:='$(find diffbot_gazebo)/worlds/turtlebot3_world.world' and in a second terminal execute the SLAM algorithm: 1 roslaunch diffbot_slam diffbot_slam.launch slam_method:=gmapping Here you can choose between different algorithms by changing the value of the slam_method argument. Possible values are gmapping (the default), karto , hector and cartographer . The ROS node graph will look like the following: ROS Node graph after launching Gazebo and gmapping. In the figure we can see that gmapping subscribes and publishes to tf . It requires the transformation from <the frame attached to incoming scans> to the base_link , which is usually a fixed value, broadcast periodically by the robot_state_publisher . Aditionally, it requires the transform from base_link to odom . This is provided by the odometry system (e.g., the driver for the mobile base). In the case of DiffBot the odometry system consists of EKF fusion data from the motor encoders and the IMU. The provided tf transforms are map to odom that describes the current estimate of the robot's pose within the map frame. You can read more about the required and provided transforms in the documentation. Field Tests In case you get inaccurate maps follow the official ROS troubleshooting guide for navigation . Frontier Exploration The so far described mapping approaches require manually steering the robot in the unknown environment. Frontier exploration is an approach to move a mobile robot on its own to new frontiers to extend its map into new territory until the entire environment has been explored. The ROS wiki provides a good tutorial using Husky robot how to use the frontier_exploration package. A lightweight alternative is the explore_lite package. Other SLAM Packages (for 3D Mapping) hdl_graph_slam : Open source ROS package for real-time 6DOF SLAM using a 3D LIDAR. It is based on 3D Graph SLAM with NDT scan matching-based odometry estimation and loop detection. This method is useful for outdoor. RTAB-Map : stands for Real-Time Appearance-Based Mapping and is a RGB-D SLAM approach based on a global loop closure detector with real-time constraints. This package can be used to generate a 3D point clouds of the environment and/or to create a 2D occupancy grid map for navigation. To do this it requires only a stereo or RGB-D camera for visual odometry. Additional wheel odometry is not required but can improve the result. Loam Velodyne : Laser Odometry and Mapping (Loam) is a realtime method for state estimation and mapping using a 3D lidar, see also the forked Github repository for loam_velodyne . Note that this is not supported officially anymore because it became closed source. ORB-SLAM2 : Real-Time SLAM for Monocular, Stereo and RGB-D Cameras, with Loop Detection and Relocalization Capabilities. See orb_slam2_ros for the ROS wrapper. slam_toolbox : This package provides a sped up improved slam karto with updated SDK and visualization and modification toolsets. It is a ROS drop in replacement to gmapping, cartographer, karto, hector, etc. This package supports ROS1 and ROS2 and is suitable for use in commercial products because it can map large environments. And it provides tools to edit the generated maps. See also the related ROSCon 2019 video . References slam_toolbox , Slam Toolbox ROSCon 2019 pdf Papers: A Tutorial on Graph-Based SLAM cartographer Real-Time Loop Closure in 2D LIDAR SLAM hector_slam A flexible and scalable SLAM system with full 3D motion estimation . A practical introduction to to pose graph slam The Normal Distributions Transform: A New Approach to Laser Scan Matching RTAB-Map as an Open-Source Lidar and Visual SLAM Library for Large-Scale and Long-Term Online Operation LOAM: Lidar Odometry and Mapping in Real-time ORB-SLAM2: an Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras","title":"SLAM"},{"location":"diffbot_slam/#diffbot-slam-package","text":"This package contains launch files and configurations for different simultaneous localization and mapping (SLAM) algorithms to map the environment of the robot in 2D, although some of these algorithms can be used to map in 3D. 1 2 3 4 5 fjp@diffbot:~/catkin_ws/src/diffbot$ catkin create pkg diffbot_slam --catkin-deps diffbot_navigation gmapping Creating package \"diffbot_slam\" in \"/home/fjp/git/ros_ws/src/diffbot\"... Created file diffbot_slam/package.xml Created file diffbot_slam/CMakeLists.txt Successfully created package files in /home/fjp/git/ros_ws/src/diffbot/diffbot_slam. Additional runtime dependencies are: cartographer_ros , hector_slam , frontier_exploration and explore_lite . These are added to this workspace using vcstool (TODO). As you can see this package has lots of dependencies to test different slam implementations and frontier exploration approaches. To run this package these dependencies need to be installed and are set as exec_depend in the package.xml . Currently only gmapping provides a ROS Noetic Ubuntu package that can be installed directly with: 1 sudo apt install ros-noetic-gmapping In case you want to try more advanced SLAM algorithms, such as karto_slam or cartographer_ros you need the following Ubuntu package dependencies. Alternatively you can install from source by building the cloned git repository in your catkin workspace. Take the required installation size into account. For example karto_slam needs approximately 125MB because it will also install ros-noetic-open-karto . {: .notice } 1 sudo apt install ros-noetic-slam-karto","title":"DiffBot Slam Package"},{"location":"diffbot_slam/#slam","text":"SLAM stands for Simultaneous Localization and Mapping sometimes refered to as Concurrent Localization and Mappping (CLAM). The SLAM algorithm combines localization and mapping, where a robot has access only to its own movement and sensory data. The robot must build a map while simultaneously localizing itself relative to the map. See also this blog post on FastSLAM . To use the following slam algorithms, we need a mobile robot that provides odometry data and is equipped with a horizontally-mounted, fixed, laser range-finder. Viewed on a higher level, every specific slam node of these algorithms will attempt to transform each incoming scan into the odom (odometry) tf frame. Therefore the node will subscribe to the laser /scan and the /tf topics. Transforms are necessary to relate frames for laser, base, and odometry. The only exception is hector_slam which doesn't require odometry for mapping. The following SLAM implementations are offered using the launch files explained in the next section. It is suggested to start with gmapping which is used by default. gmapping : This package contains a ROS wrapper for OpenSlam's Gmapping . The gmapping package provides laser-based SLAM (Simultaneous Localization and Mapping), as a ROS node called slam_gmapping . Using slam_gmapping , you can create a 2-D occupancy grid map (like a building floorplan) from laser and pose data collected by a mobile robot. cartographer : Cartographer is a system that provides real-time simultaneous localization and mapping (SLAM) in 2D and 3D across multiple platforms and sensor configurations. See the documentation for an algorithm walkthrough . karto : This package pulls in the Karto mapping library, and provides a ROS wrapper for using it. Karto is considered more accurate than, for example gmapping (note: for ROS noetic, see slam_karto ) and became open source in 2010 . hector_slam : metapackage that installs hector_mapping and related packages. The hector_mapping is a SLAM approach that can be used without odometry as well as on platforms that exhibit roll/pitch motion (of the sensor, the platform or both), such as drones. It leverages the high update rate of modern LIDAR systems like the Hokuyo UTM-30LX and provides 2D pose estimates at scan rate of the sensors (40Hz for the UTM-30LX). While the system does not provide explicit loop closing ability, it is sufficiently accurate for many real world scenarios. The system has successfully been used on Unmanned Ground Robots, Unmanned Surface Vehicles, Handheld Mapping Devices and logged data from quadrotor UAVs. Unlike gmapping which uses a particle filter , karto , cartographer and hector_slam are all graph-based SLAM algorithms . The least accurate SLAM algorithm is gmapping but it works fine for smaller maps. Use other algorithms, such as karto if you operate your robot in larger environments or you want more accuracy. Another interesing package is slam_toolbox which provides ROS1 and ROS2 support and is based on the easy to use karto algorithm. karto is the basis for many companies because it provides an excellent scan matcher and can operate in large environments. Additionally, slam_toolbox provides tools to edit a generated map and even create a high quality map using stored data (offline). The cartographer package is currently supported by OpenRobotics and not by Google where it was originally developed. It is currently also not setup correctly for DiffBot. Using it will result in errors. {: .notice }","title":"SLAM"},{"location":"diffbot_slam/#launch-files","text":"This package provides a main launch file named diffbot_slam.launch which accepts an argument slam_method . Depending on its value, different launch files will be included that execute the specified SLAM algorithm using its configuration in the config folder. As mentioned above, every ROS slam package requries messages from the laser-range finder topic. Usually this topic is named /scan . To distinguish possible multiple lidars, the topic of DiffBot resides in its namespace /diffbot/scan . Therefore, its necessary to remap the /scan topic to /diffbot/scan . The following shows how this was done for the gmapping launch file. Inside this package in the launch/gmapping.launch it is important to map the scan topic to laser scanner topic published by Diffbot. Remappings are done in the node tag . Here, for the gmapping.launch in the gmapping node: 1 2 3 4 5 6 7 8 9 10 11 <launch> <!-- Arguments --> <arg name= \"scan_topic\" default= \"diffbot/scan\" /> ... <!-- Gmapping --> <node pkg= \"gmapping\" type= \"slam_gmapping\" name= \"diffbot_slam_gmapping\" output= \"screen\" > ... <!-- remapping of gmapping node --> <remap from= \"scan\" to= \"$(arg scan_topic)\" /> </node> </launch>","title":"Launch files"},{"location":"diffbot_slam/#parameter-configurations","text":"Most of the configrations are the same as turtlebot3_slam/config . For detailed description of what each parameter does, please check the individual package documentation of the different SLAM methods.","title":"Parameter Configurations"},{"location":"diffbot_slam/#gazebo-simulation-tests","text":"To test SLAM in the Gazebo simulator run the following two launch files in separate terminals. First run the simulation with: 1 roslaunch diffbot_gazebo diffbot.launch world_name:='$(find diffbot_gazebo)/worlds/turtlebot3_world.world' and in a second terminal execute the SLAM algorithm: 1 roslaunch diffbot_slam diffbot_slam.launch slam_method:=gmapping Here you can choose between different algorithms by changing the value of the slam_method argument. Possible values are gmapping (the default), karto , hector and cartographer . The ROS node graph will look like the following: ROS Node graph after launching Gazebo and gmapping. In the figure we can see that gmapping subscribes and publishes to tf . It requires the transformation from <the frame attached to incoming scans> to the base_link , which is usually a fixed value, broadcast periodically by the robot_state_publisher . Aditionally, it requires the transform from base_link to odom . This is provided by the odometry system (e.g., the driver for the mobile base). In the case of DiffBot the odometry system consists of EKF fusion data from the motor encoders and the IMU. The provided tf transforms are map to odom that describes the current estimate of the robot's pose within the map frame. You can read more about the required and provided transforms in the documentation.","title":"Gazebo Simulation Tests"},{"location":"diffbot_slam/#field-tests","text":"In case you get inaccurate maps follow the official ROS troubleshooting guide for navigation .","title":"Field Tests"},{"location":"diffbot_slam/#frontier-exploration","text":"The so far described mapping approaches require manually steering the robot in the unknown environment. Frontier exploration is an approach to move a mobile robot on its own to new frontiers to extend its map into new territory until the entire environment has been explored. The ROS wiki provides a good tutorial using Husky robot how to use the frontier_exploration package. A lightweight alternative is the explore_lite package.","title":"Frontier Exploration"},{"location":"diffbot_slam/#other-slam-packages-for-3d-mapping","text":"hdl_graph_slam : Open source ROS package for real-time 6DOF SLAM using a 3D LIDAR. It is based on 3D Graph SLAM with NDT scan matching-based odometry estimation and loop detection. This method is useful for outdoor. RTAB-Map : stands for Real-Time Appearance-Based Mapping and is a RGB-D SLAM approach based on a global loop closure detector with real-time constraints. This package can be used to generate a 3D point clouds of the environment and/or to create a 2D occupancy grid map for navigation. To do this it requires only a stereo or RGB-D camera for visual odometry. Additional wheel odometry is not required but can improve the result. Loam Velodyne : Laser Odometry and Mapping (Loam) is a realtime method for state estimation and mapping using a 3D lidar, see also the forked Github repository for loam_velodyne . Note that this is not supported officially anymore because it became closed source. ORB-SLAM2 : Real-Time SLAM for Monocular, Stereo and RGB-D Cameras, with Loop Detection and Relocalization Capabilities. See orb_slam2_ros for the ROS wrapper. slam_toolbox : This package provides a sped up improved slam karto with updated SDK and visualization and modification toolsets. It is a ROS drop in replacement to gmapping, cartographer, karto, hector, etc. This package supports ROS1 and ROS2 and is suitable for use in commercial products because it can map large environments. And it provides tools to edit the generated maps. See also the related ROSCon 2019 video .","title":"Other SLAM Packages (for 3D Mapping)"},{"location":"diffbot_slam/#references","text":"slam_toolbox , Slam Toolbox ROSCon 2019 pdf Papers: A Tutorial on Graph-Based SLAM cartographer Real-Time Loop Closure in 2D LIDAR SLAM hector_slam A flexible and scalable SLAM system with full 3D motion estimation . A practical introduction to to pose graph slam The Normal Distributions Transform: A New Approach to Laser Scan Matching RTAB-Map as an Open-Source Lidar and Visual SLAM Library for Large-Scale and Long-Term Online Operation LOAM: Lidar Odometry and Mapping in Real-time ORB-SLAM2: an Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras","title":"References"},{"location":"git-setup/","text":"git setup Install git on Ubuntu via the following command: fjp@ubuntu : ~/git/2wd-robot $ sudo apt install git Set your username and email address that you use on github (when using github to host your repository): fjp@ubuntu : ~ $ git config --global user.name \"fjp\" fjp@ubuntu : ~ $ git config --global user.email \"franz.pucher@gmail.com\" To store your password credentials when pushing and pulling to the remote repository use the following commands: fjp@ubuntu : ~/git/2wd-robot $ git config --global credential.helper store fjp@ubuntu : ~/git/2wd-robot $ git push Username for 'https://github.com': fjp Password for 'https://fjp@github.com': Everything up-to-date fjp@ubuntu : ~/git/2wd-robot $ git push Everything up-to-date","title":"Git Setup"},{"location":"git-setup/#git-setup","text":"Install git on Ubuntu via the following command: fjp@ubuntu : ~/git/2wd-robot $ sudo apt install git Set your username and email address that you use on github (when using github to host your repository): fjp@ubuntu : ~ $ git config --global user.name \"fjp\" fjp@ubuntu : ~ $ git config --global user.email \"franz.pucher@gmail.com\" To store your password credentials when pushing and pulling to the remote repository use the following commands: fjp@ubuntu : ~/git/2wd-robot $ git config --global credential.helper store fjp@ubuntu : ~/git/2wd-robot $ git push Username for 'https://github.com': fjp Password for 'https://fjp@github.com': Everything up-to-date fjp@ubuntu : ~/git/2wd-robot $ git push Everything up-to-date","title":"git setup"},{"location":"grove_motor_driver/","text":"Grove - I2C Motor Driver V1.3 The package for the Grove I2C Motor Driver V1.3 from Seeed Studio is created with catkin create pkg PKG_NAME [--catkin-deps [DEP [DEP ...]]] : 1 2 3 4 5 6 7 fjp@ubuntu:~/git/diffbot/ros/src$ catkin create pkg grove_motor_driver --catkin-deps rospy roscpp geometry_msgs Creating package \"grove_motor_driver\" in \"/home/fjp/git/diffbot/ros/src\"... Created file grove_motor_driver/CMakeLists.txt Created file grove_motor_driver/package.xml Created folder grove_motor_driver/include/grove_motor_driver Created folder grove_motor_driver/src Successfully created package files in /home/fjp/git/diffbot/ros/src/grove_motor_driver. The package depends on the two ROS client libraries rospy and roscpp . To control the two motors the package will use the geometry_msgs/Twist message. The interface to the motor driver is done with the Python library from DexterInd which is a rewerite of the Seeed Studio Arduino library . This library requires the following two python libraries RPi.GPIO smbus SMBus (System Management Bus) is a subset from the I2C protocol These libraries should be installed with pip3 , Python's package manager: 1 2 pip3 install RPi.GPIO pip3 install smbus Note that this will install these packages system wide. This is ok because they are installed on the Raspberry Pi which is dedicated to operate for this purpose. For a development environment it is best practice to use a python virtual environment like venv and install the packages inside it. {: .notice } Connection Afterwards the I2C port of the motor should be connected to the I2C port 1 of the Raspberry Pi 4 B. Don't forget to remove the jumper on the motor driver board which would provide power to the Pi. However, it is also not required to connect VCC and GND of the I2C connector. Only the SDA (data) and SCL (clock) wires are required. Make sure to set the address with the dip switches on the motor driver to 0x0f because this is the default address used in the library files. To test the physical I2C connection use i2cdetect described in Hardware Interfaces : The output should list 0f in the address table: 1 2 3 4 5 6 7 8 9 10 $ i2cdetect -y 1 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- 0f 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: -- -- -- -- -- -- -- -- Test Motor Driver Test the motor driver by running one of the python files: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 fjp@ubuntu:~/git/diffbot/ros/src/grove_motor_driver/src$ python3 motor_example.py Forward Back Stop Speed: 0 Speed: 1 Speed: 2 Speed: 3 Speed: 4 Speed: 5 Speed: 6 Speed: 7 Speed: 8 Speed: 9 Speed: 10 Speed: 11 Speed: 12 ... Speed: 25 Speed: 26 Speed: 27 Speed: 28 Speed: 29 Speed: 30 Speed: 31 Speed: 32 Speed: 33 ... Speed: 55 Speed: 56 Speed: 57 Speed: 58 Speed: 59 Speed: 60 Speed: 61 Speed: 62 Speed: 63 Speed: 64 Speed: 65 Speed: 66 ... Speed: 75 Speed: 76 Speed: 77 Speed: 78 Speed: 79 Speed: 80 Speed: 81 Speed: 82 Speed: 83 Speed: 84 Speed: 85 Speed: 86 Speed: 87 Speed: 88 ... Speed: 97 Speed: 98 Speed: 99 Stop Troubleshooting If you get errors like the following, make sure the I2C cables from the motor driver to the Raspberry Pi are connected (see Hardware Interfaces for more infos) and use the RESET button on the motor driver. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 fjp@ubuntu:~/git/2wd-robot/ros/src/control$ sudo python grove_i2c_motor_driver.py Traceback (most recent call last): File \"grove_i2c_motor_driver.py\", line 68, in <module> m.MotorSpeedSetAB(100,100) File \"grove_i2c_motor_driver.py\", line 57, in MotorSpeedSetAB bus.write_i2c_block_data(self.I2CMotorDriverAdd, self.MotorSpeedSet, [MotorSpeedA,MotorSpeedB]) IOError: [Errno 110] Connection timed out fjp@ubuntu:~/git/2wd-robot/ros/src/control$ sudo python grove_i2c_motor_driver.py Traceback (most recent call last): File \"grove_i2c_motor_driver.py\", line 68, in <module> m.MotorSpeedSetAB(100,100) File \"grove_i2c_motor_driver.py\", line 57, in MotorSpeedSetAB bus.write_i2c_block_data(self.I2CMotorDriverAdd, self.MotorSpeedSet, [MotorSpeedA,MotorSpeedB]) IOError: [Errno 121] Remote I/O error Try pressing the RESET button and release it right before executing one of the scripts. You should also be able to detect the motor driver with i2cdetect -y 1 : 1 2 3 4 5 6 7 8 9 10 fjp@ubuntu:~/git/2wd-robot/ros/src/control/src$ sudo i2cdetect -y 1 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- 0f 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: -- -- -- -- -- -- -- -- As you can see the address of the motor driver is detected at 0x0f . In case of the following output, where every address of the I2C bus seems to be taken it is most likely that the SDA (data) and SCL (clock) signal cables are switched: 1 2 3 4 5 6 7 8 9 10 i2cdetect -y 1 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: 03 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f 10: 10 11 12 13 14 15 16 17 18 19 1a 1b 1c 1d 1e 1f 20: 20 21 22 23 24 25 26 27 28 29 2a 2b 2c 2d 2e 2f 30: 30 31 32 33 34 35 36 37 38 39 3a 3b 3c 3d 3e 3f 40: 40 41 42 43 44 45 46 47 48 49 4a 4b 4c 4d 4e 4f 50: 50 51 52 53 54 55 56 57 58 59 5a 5b 5c 5d 5e 5f 60: 60 61 62 63 64 65 66 67 68 69 6a 6b 6c 6d 6e 6f 70: 70 71 72 73 74 75 76 77 To solve this make sure the SDA and SCL cables are connected to the right pins on the Raspberry Pi. See Hardware Interfaces for more infos. ROS Node for Motor Driver To use the available library of the Grove I2C motor driver in ROS we need to create a wrapper node, called motor_driver . The node subscribes to the topic /2wd_robot/cmd_vel which is of type Twist message from the geometry_msgs header. To send commands to the motor the . These topics can be published with nodes from the navigation stack or with rostopic pub for test purposes. https://en.wikipedia.org/wiki/Differential_wheeled_robot http://wiki.ros.org/differential_drive After the node has been implemented, we need to build the workspace with catkin build : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 fjp@ubuntu:~/git/2wd-robot/ros$ catkin build ---------------------------------------------------------------- Profile: default Extending: [cached] /opt/ros/melodic Workspace: /home/fjp/git/2wd-robot/ros ---------------------------------------------------------------- Build Space: [exists] /home/fjp/git/2wd-robot/ros/build Devel Space: [exists] /home/fjp/git/2wd-robot/ros/devel Install Space: [unused] /home/fjp/git/2wd-robot/ros/install Log Space: [exists] /home/fjp/git/2wd-robot/ros/logs Source Space: [exists] /home/fjp/git/2wd-robot/ros/src DESTDIR: [unused] None ---------------------------------------------------------------- Devel Space Layout: linked Install Space Layout: None ---------------------------------------------------------------- Additional CMake Args: None Additional Make Args: None Additional catkin Make Args: None Internal Make Job Server: True Cache Job Environments: False ---------------------------------------------------------------- Whitelisted Packages: None Blacklisted Packages: None ---------------------------------------------------------------- Workspace configuration appears valid. ---------------------------------------------------------------- [build] Found '2' packages in 0.0 seconds. [build] Updating package table. Starting >>> grove_motor_driver Starting >>> grove_ultrasonic_ranger Finished <<< grove_motor_driver [ 1.0 seconds ] Finished <<< grove_ultrasonic_ranger [ 1.0 seconds ] [build] Summary: All 2 packages succeeded! [build] Ignored: None. [build] Warnings: None. [build] Abandoned: None. [build] Failed: None. [build] Runtime: 2.0 seconds total. As the final note of the build output suggests, we have to source the setup.bash files in the devel space. 1 fjp@ubuntu:~/git/2wd-robot/ros$ source devel/setup.bash To make the ranger node executable we have to modify the ranger.py file: 1 fjp@ubuntu:~/git/2wd-robot/ros/src/grove_ultrasonic_ranger/src$ sudo chmod a+x ranger.py","title":"Motor Driver"},{"location":"grove_motor_driver/#grove-i2c-motor-driver-v13","text":"The package for the Grove I2C Motor Driver V1.3 from Seeed Studio is created with catkin create pkg PKG_NAME [--catkin-deps [DEP [DEP ...]]] : 1 2 3 4 5 6 7 fjp@ubuntu:~/git/diffbot/ros/src$ catkin create pkg grove_motor_driver --catkin-deps rospy roscpp geometry_msgs Creating package \"grove_motor_driver\" in \"/home/fjp/git/diffbot/ros/src\"... Created file grove_motor_driver/CMakeLists.txt Created file grove_motor_driver/package.xml Created folder grove_motor_driver/include/grove_motor_driver Created folder grove_motor_driver/src Successfully created package files in /home/fjp/git/diffbot/ros/src/grove_motor_driver. The package depends on the two ROS client libraries rospy and roscpp . To control the two motors the package will use the geometry_msgs/Twist message. The interface to the motor driver is done with the Python library from DexterInd which is a rewerite of the Seeed Studio Arduino library . This library requires the following two python libraries RPi.GPIO smbus SMBus (System Management Bus) is a subset from the I2C protocol These libraries should be installed with pip3 , Python's package manager: 1 2 pip3 install RPi.GPIO pip3 install smbus Note that this will install these packages system wide. This is ok because they are installed on the Raspberry Pi which is dedicated to operate for this purpose. For a development environment it is best practice to use a python virtual environment like venv and install the packages inside it. {: .notice }","title":"Grove - I2C Motor Driver V1.3"},{"location":"grove_motor_driver/#connection","text":"Afterwards the I2C port of the motor should be connected to the I2C port 1 of the Raspberry Pi 4 B. Don't forget to remove the jumper on the motor driver board which would provide power to the Pi. However, it is also not required to connect VCC and GND of the I2C connector. Only the SDA (data) and SCL (clock) wires are required. Make sure to set the address with the dip switches on the motor driver to 0x0f because this is the default address used in the library files. To test the physical I2C connection use i2cdetect described in Hardware Interfaces : The output should list 0f in the address table: 1 2 3 4 5 6 7 8 9 10 $ i2cdetect -y 1 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- 0f 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: -- -- -- -- -- -- -- --","title":"Connection"},{"location":"grove_motor_driver/#test-motor-driver","text":"Test the motor driver by running one of the python files: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 fjp@ubuntu:~/git/diffbot/ros/src/grove_motor_driver/src$ python3 motor_example.py Forward Back Stop Speed: 0 Speed: 1 Speed: 2 Speed: 3 Speed: 4 Speed: 5 Speed: 6 Speed: 7 Speed: 8 Speed: 9 Speed: 10 Speed: 11 Speed: 12 ... Speed: 25 Speed: 26 Speed: 27 Speed: 28 Speed: 29 Speed: 30 Speed: 31 Speed: 32 Speed: 33 ... Speed: 55 Speed: 56 Speed: 57 Speed: 58 Speed: 59 Speed: 60 Speed: 61 Speed: 62 Speed: 63 Speed: 64 Speed: 65 Speed: 66 ... Speed: 75 Speed: 76 Speed: 77 Speed: 78 Speed: 79 Speed: 80 Speed: 81 Speed: 82 Speed: 83 Speed: 84 Speed: 85 Speed: 86 Speed: 87 Speed: 88 ... Speed: 97 Speed: 98 Speed: 99 Stop","title":"Test Motor Driver"},{"location":"grove_motor_driver/#troubleshooting","text":"If you get errors like the following, make sure the I2C cables from the motor driver to the Raspberry Pi are connected (see Hardware Interfaces for more infos) and use the RESET button on the motor driver. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 fjp@ubuntu:~/git/2wd-robot/ros/src/control$ sudo python grove_i2c_motor_driver.py Traceback (most recent call last): File \"grove_i2c_motor_driver.py\", line 68, in <module> m.MotorSpeedSetAB(100,100) File \"grove_i2c_motor_driver.py\", line 57, in MotorSpeedSetAB bus.write_i2c_block_data(self.I2CMotorDriverAdd, self.MotorSpeedSet, [MotorSpeedA,MotorSpeedB]) IOError: [Errno 110] Connection timed out fjp@ubuntu:~/git/2wd-robot/ros/src/control$ sudo python grove_i2c_motor_driver.py Traceback (most recent call last): File \"grove_i2c_motor_driver.py\", line 68, in <module> m.MotorSpeedSetAB(100,100) File \"grove_i2c_motor_driver.py\", line 57, in MotorSpeedSetAB bus.write_i2c_block_data(self.I2CMotorDriverAdd, self.MotorSpeedSet, [MotorSpeedA,MotorSpeedB]) IOError: [Errno 121] Remote I/O error Try pressing the RESET button and release it right before executing one of the scripts. You should also be able to detect the motor driver with i2cdetect -y 1 : 1 2 3 4 5 6 7 8 9 10 fjp@ubuntu:~/git/2wd-robot/ros/src/control/src$ sudo i2cdetect -y 1 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- 0f 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: -- -- -- -- -- -- -- -- As you can see the address of the motor driver is detected at 0x0f . In case of the following output, where every address of the I2C bus seems to be taken it is most likely that the SDA (data) and SCL (clock) signal cables are switched: 1 2 3 4 5 6 7 8 9 10 i2cdetect -y 1 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: 03 04 05 06 07 08 09 0a 0b 0c 0d 0e 0f 10: 10 11 12 13 14 15 16 17 18 19 1a 1b 1c 1d 1e 1f 20: 20 21 22 23 24 25 26 27 28 29 2a 2b 2c 2d 2e 2f 30: 30 31 32 33 34 35 36 37 38 39 3a 3b 3c 3d 3e 3f 40: 40 41 42 43 44 45 46 47 48 49 4a 4b 4c 4d 4e 4f 50: 50 51 52 53 54 55 56 57 58 59 5a 5b 5c 5d 5e 5f 60: 60 61 62 63 64 65 66 67 68 69 6a 6b 6c 6d 6e 6f 70: 70 71 72 73 74 75 76 77 To solve this make sure the SDA and SCL cables are connected to the right pins on the Raspberry Pi. See Hardware Interfaces for more infos.","title":"Troubleshooting"},{"location":"grove_motor_driver/#ros-node-for-motor-driver","text":"To use the available library of the Grove I2C motor driver in ROS we need to create a wrapper node, called motor_driver . The node subscribes to the topic /2wd_robot/cmd_vel which is of type Twist message from the geometry_msgs header. To send commands to the motor the . These topics can be published with nodes from the navigation stack or with rostopic pub for test purposes. https://en.wikipedia.org/wiki/Differential_wheeled_robot http://wiki.ros.org/differential_drive After the node has been implemented, we need to build the workspace with catkin build : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 fjp@ubuntu:~/git/2wd-robot/ros$ catkin build ---------------------------------------------------------------- Profile: default Extending: [cached] /opt/ros/melodic Workspace: /home/fjp/git/2wd-robot/ros ---------------------------------------------------------------- Build Space: [exists] /home/fjp/git/2wd-robot/ros/build Devel Space: [exists] /home/fjp/git/2wd-robot/ros/devel Install Space: [unused] /home/fjp/git/2wd-robot/ros/install Log Space: [exists] /home/fjp/git/2wd-robot/ros/logs Source Space: [exists] /home/fjp/git/2wd-robot/ros/src DESTDIR: [unused] None ---------------------------------------------------------------- Devel Space Layout: linked Install Space Layout: None ---------------------------------------------------------------- Additional CMake Args: None Additional Make Args: None Additional catkin Make Args: None Internal Make Job Server: True Cache Job Environments: False ---------------------------------------------------------------- Whitelisted Packages: None Blacklisted Packages: None ---------------------------------------------------------------- Workspace configuration appears valid. ---------------------------------------------------------------- [build] Found '2' packages in 0.0 seconds. [build] Updating package table. Starting >>> grove_motor_driver Starting >>> grove_ultrasonic_ranger Finished <<< grove_motor_driver [ 1.0 seconds ] Finished <<< grove_ultrasonic_ranger [ 1.0 seconds ] [build] Summary: All 2 packages succeeded! [build] Ignored: None. [build] Warnings: None. [build] Abandoned: None. [build] Failed: None. [build] Runtime: 2.0 seconds total. As the final note of the build output suggests, we have to source the setup.bash files in the devel space. 1 fjp@ubuntu:~/git/2wd-robot/ros$ source devel/setup.bash To make the ranger node executable we have to modify the ranger.py file: 1 fjp@ubuntu:~/git/2wd-robot/ros/src/grove_ultrasonic_ranger/src$ sudo chmod a+x ranger.py","title":"ROS Node for Motor Driver"},{"location":"grove_ultrasonic_ranger/","text":"Grove - Ultrasonic Ranger To avoid obstacles the Grove ultrasonic ranger from Seeed Studio is used. We will create a ROS package with catkin create pkg PKG_NAME [--catkin-deps [DEP [DEP ...]]] : 1 2 3 4 5 6 fjp@ubuntu:~/git/2wd-robot/ros/src$ catkin create pkg grove_ultrasonic_ranger --catkin-deps rospy roscpp sensor_msgs Creating package \"grove_ultrasonic_ranger\" in \"/home/fjp/git/2wd-robot/ros/src\" ... Created file grove_ultrasonic_ranger/CMakeLists.txt Created file grove_ultrasonic_ranger/package.xml Created folder grove_ultrasonic_ranger/include/grove_ultrasonic_ranger Created folder grove_ultrasonic_ranger/src The package depends on the two ROS client libraries rospy and roscpp . To signalise the current distance to obstacles the sensor_msgs/Range message is used. After connecting the signal pin of the sensor to (physical) GPIO 11 of the Raspberry Pi 4 B and power it with 5V and ground, we can test its functionality with the available python script ultrasonic.py from Seed Studio. The following shows the truncated output of the ultrasonic.py script when moving an obstacle in front of the sensor. We see that the distance value changes as expected. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 fjp@ubuntu:~/git/2wd-robot/ros/src/grove_ultrasonic_ranger/src$ sudo python ultrasonic.py SeeedStudio Grove Ultrasonic get data and print Ultrasonic Measurement Distance : 2 .0 CM Ultrasonic Measurement Distance : 2 .1 CM Ultrasonic Measurement Distance : 2 .0 CM Ultrasonic Measurement Distance : 3 .5 CM Ultrasonic Measurement Distance : 3 .5 CM Ultrasonic Measurement Distance : 2 .4 CM Ultrasonic Measurement Distance : 12 .9 CM ... Ultrasonic Measurement Distance : 45 .8 CM Ultrasonic Measurement Distance : 510 .7 CM Ultrasonic Measurement Distance : 510 .7 CM Ultrasonic Measurement Distance : 45 .7 CM Ultrasonic Measurement Distance : 161 .8 CM Ultrasonic Measurement Distance : 26 .9 CM Ultrasonic Measurement Distance : 18 .3 CM Ultrasonic Measurement Distance : 160 .9 CM Ultrasonic Measurement Distance : 158 .3 CM Ultrasonic Measurement Distance : 159 .4 CM ... Modified GroveUltrasonicRanger Library To use the ultrasonic ranger as a ROS node it is convenient to wrap the sensor functionality in a class. This provides an easy to extend interface for the ultrasonic ranger ( API ) Therefore I copied the core functionality of the ultrasonic.py script from Seeed Studio in a class named GroveUltrasonicRanger . Executing the grove_ultrasonic_ranger.py will result in the following output: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 fjp@ubuntu:~/git/2wd-robot/ros/src/grove_ultrasonic_ranger/src$ sudo python grove_ultrasonic_ranger.py SeeedStudio Grove Ultrasonic get data and print Distance : 0 .051 m Distance : 0 .069 m Distance : 0 .098 m Distance : 0 .131 m Distance : 0 .153 m Distance : 0 .172 m Distance : 0 .207 m Distance : 0 .210 m Distance : 0 .234 m Distance : 0 .256 m GPIO.cleanup () GPIO.cleanup () done ROS Node for Ultrasonic Ranger ROS provides the Range Message in the sensor_msgs header . This message type can be used to write a wrapper that will act as a ROS node for the Grove ultrasonic sensor. To design this node we will send out measurements periodically over a topic of type sensor_msgs/Range . The code for this node is in ranger.py . After writing the node we need to build the packages in the workspace with catkin build . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 fjp@ubuntu:~/git/2wd-robot/ros$ catkin build ---------------------------------------------------------------- Profile: default Extending: [ env ] /opt/ros/melodic Workspace: /home/fjp/git/2wd-robot/ros ---------------------------------------------------------------- Build Space: [ exists ] /home/fjp/git/2wd-robot/ros/build Devel Space: [ exists ] /home/fjp/git/2wd-robot/ros/devel Install Space: [ unused ] /home/fjp/git/2wd-robot/ros/install Log Space: [ missing ] /home/fjp/git/2wd-robot/ros/logs Source Space: [ exists ] /home/fjp/git/2wd-robot/ros/src DESTDIR: [ unused ] None ---------------------------------------------------------------- Devel Space Layout: linked Install Space Layout: None ---------------------------------------------------------------- Additional CMake Args: None Additional Make Args: None Additional catkin Make Args: None Internal Make Job Server: True Cache Job Environments: False ---------------------------------------------------------------- Whitelisted Packages: None Blacklisted Packages: None ---------------------------------------------------------------- Workspace configuration appears valid. NOTE: Forcing CMake to run for each package. ---------------------------------------------------------------- [ build ] Found '1' package in 0 .0 seconds. [ build ] Updating package table. Starting >>> catkin_tools_prebuild Finished <<< catkin_tools_prebuild [ 9 .9 seconds ] Starting >>> grove_ultrasonic_ranger Finished <<< grove_ultrasonic_ranger [ 12 .0 seconds ] [ build ] Summary: All 2 packages succeeded! [ build ] Ignored: None. [ build ] Warnings: None. [ build ] Abandoned: None. [ build ] Failed: None. [ build ] Runtime: 21 .9 seconds total. [ build ] Note: Workspace packages have changed, please re-source setup files to use them. As the final note of the build output suggests, we have to source the setup.bash files in the devel space. 1 fjp@ubuntu:~/git/2wd-robot/ros$ source devel/setup.bash To make the ranger node executable we have to modify the ranger.py file: 1 fjp@ubuntu:~/git/2wd-robot/ros/src/grove_ultrasonic_ranger/src$ sudo chmod a+x ranger.py Then we can test the node using rosrun : 1 2 3 4 5 6 7 8 9 10 fjp@ubuntu:~/git/2wd-robot/ros$ sudo su [ sudo ] password for fjp: root@ubuntu:/home/fjp/git/2wd-robot/ros# source devel/setup.bash root@ubuntu:/home/fjp/git/2wd-robot/ros# rosrun grove_ultrasonic_ranger ranger.py Distance : 1 .617 m Distance : 1 .617 m Distance : 1 .617 m Distance : 0 .108 m Distance : 0 .092 m Distance : 0 .099 m This lets the node publish range messages which we can capture in another terminal window using rostopic . First we use rostopic list to find the name of the topic we are interested in: 1 2 3 4 fjp@ubuntu:~/git/2wd-robot/ros$ rostopic list /distance /rosout /rosout_agg We named our topic /distance which we can use with the rostopic echo command to see the published messages: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 fjp@ubuntu:~/git/2wd-robot/ros$ rostopic echo /distance header: seq: 1 stamp: secs: 1576778377 nsecs: 746809959 frame_id: \"ranger_distance\" radiation_type: 0 field_of_view: 0 .261799007654 min_range: 0 .019999999553 max_range: 3 .5 range: 1 .61674261093 --- header: seq: 2 stamp: secs: 1576778378 nsecs: 459048986 frame_id: \"ranger_distance\" radiation_type: 0 field_of_view: 0 .261799007654 min_range: 0 .019999999553 max_range: 3 .5 range: 1 .61261284351 --- header: seq: 3 stamp: secs: 1576778379 nsecs: 172172069 frame_id: \"ranger_distance\" radiation_type: 0 field_of_view: 0 .261799007654 min_range: 0 .019999999553 max_range: 3 .5 range: 1 .61657905579 --- header: seq: 4 stamp: secs: 1576778379 nsecs: 884002923 frame_id: \"ranger_distance\" radiation_type: 0 field_of_view: 0 .261799007654 min_range: 0 .019999999553 max_range: 3 .5 range: 1 .61277639866 --- header: seq: 5 stamp: secs: 1576778380 nsecs: 596549034 frame_id: \"ranger_distance\" radiation_type: 0 field_of_view: 0 .261799007654 min_range: 0 .019999999553 max_range: 3 .5 range: 1 .67693090439 --- Informational Distance Measurements To provide additional information when an obstacle is too close or the robot has no obstacle in front of it we use REP-117 as guideline. To successfully implement this REP, checks for valid measurements should use floating-point standards and follow this form: 1 2 3 4 5 6 7 8 9 10 11 if ( minimum_range <= value && value <= maximum_range ){ // Represents expected pre-REP logic and is the only necessary condition for most applications. // This is a valid measurement. } else if ( ! isfinite ( value ) && value < 0 ){ // Object too close to measure. } else if ( ! isfinite ( value ) && value > 0 ){ // No objects detected in range. } else if ( isnan ( value ) ){ // This is an erroneous, invalid, or missing measurement. } else { // The sensor reported these measurements as valid, but they are discarded per the limits defined by minimum_range and maximum_range. }","title":"Grove ultrasonic ranger"},{"location":"grove_ultrasonic_ranger/#grove-ultrasonic-ranger","text":"To avoid obstacles the Grove ultrasonic ranger from Seeed Studio is used. We will create a ROS package with catkin create pkg PKG_NAME [--catkin-deps [DEP [DEP ...]]] : 1 2 3 4 5 6 fjp@ubuntu:~/git/2wd-robot/ros/src$ catkin create pkg grove_ultrasonic_ranger --catkin-deps rospy roscpp sensor_msgs Creating package \"grove_ultrasonic_ranger\" in \"/home/fjp/git/2wd-robot/ros/src\" ... Created file grove_ultrasonic_ranger/CMakeLists.txt Created file grove_ultrasonic_ranger/package.xml Created folder grove_ultrasonic_ranger/include/grove_ultrasonic_ranger Created folder grove_ultrasonic_ranger/src The package depends on the two ROS client libraries rospy and roscpp . To signalise the current distance to obstacles the sensor_msgs/Range message is used. After connecting the signal pin of the sensor to (physical) GPIO 11 of the Raspberry Pi 4 B and power it with 5V and ground, we can test its functionality with the available python script ultrasonic.py from Seed Studio. The following shows the truncated output of the ultrasonic.py script when moving an obstacle in front of the sensor. We see that the distance value changes as expected. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 fjp@ubuntu:~/git/2wd-robot/ros/src/grove_ultrasonic_ranger/src$ sudo python ultrasonic.py SeeedStudio Grove Ultrasonic get data and print Ultrasonic Measurement Distance : 2 .0 CM Ultrasonic Measurement Distance : 2 .1 CM Ultrasonic Measurement Distance : 2 .0 CM Ultrasonic Measurement Distance : 3 .5 CM Ultrasonic Measurement Distance : 3 .5 CM Ultrasonic Measurement Distance : 2 .4 CM Ultrasonic Measurement Distance : 12 .9 CM ... Ultrasonic Measurement Distance : 45 .8 CM Ultrasonic Measurement Distance : 510 .7 CM Ultrasonic Measurement Distance : 510 .7 CM Ultrasonic Measurement Distance : 45 .7 CM Ultrasonic Measurement Distance : 161 .8 CM Ultrasonic Measurement Distance : 26 .9 CM Ultrasonic Measurement Distance : 18 .3 CM Ultrasonic Measurement Distance : 160 .9 CM Ultrasonic Measurement Distance : 158 .3 CM Ultrasonic Measurement Distance : 159 .4 CM ...","title":"Grove - Ultrasonic Ranger"},{"location":"grove_ultrasonic_ranger/#modified-groveultrasonicranger-library","text":"To use the ultrasonic ranger as a ROS node it is convenient to wrap the sensor functionality in a class. This provides an easy to extend interface for the ultrasonic ranger ( API ) Therefore I copied the core functionality of the ultrasonic.py script from Seeed Studio in a class named GroveUltrasonicRanger . Executing the grove_ultrasonic_ranger.py will result in the following output: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 fjp@ubuntu:~/git/2wd-robot/ros/src/grove_ultrasonic_ranger/src$ sudo python grove_ultrasonic_ranger.py SeeedStudio Grove Ultrasonic get data and print Distance : 0 .051 m Distance : 0 .069 m Distance : 0 .098 m Distance : 0 .131 m Distance : 0 .153 m Distance : 0 .172 m Distance : 0 .207 m Distance : 0 .210 m Distance : 0 .234 m Distance : 0 .256 m GPIO.cleanup () GPIO.cleanup () done","title":"Modified GroveUltrasonicRanger Library"},{"location":"grove_ultrasonic_ranger/#ros-node-for-ultrasonic-ranger","text":"ROS provides the Range Message in the sensor_msgs header . This message type can be used to write a wrapper that will act as a ROS node for the Grove ultrasonic sensor. To design this node we will send out measurements periodically over a topic of type sensor_msgs/Range . The code for this node is in ranger.py . After writing the node we need to build the packages in the workspace with catkin build . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 fjp@ubuntu:~/git/2wd-robot/ros$ catkin build ---------------------------------------------------------------- Profile: default Extending: [ env ] /opt/ros/melodic Workspace: /home/fjp/git/2wd-robot/ros ---------------------------------------------------------------- Build Space: [ exists ] /home/fjp/git/2wd-robot/ros/build Devel Space: [ exists ] /home/fjp/git/2wd-robot/ros/devel Install Space: [ unused ] /home/fjp/git/2wd-robot/ros/install Log Space: [ missing ] /home/fjp/git/2wd-robot/ros/logs Source Space: [ exists ] /home/fjp/git/2wd-robot/ros/src DESTDIR: [ unused ] None ---------------------------------------------------------------- Devel Space Layout: linked Install Space Layout: None ---------------------------------------------------------------- Additional CMake Args: None Additional Make Args: None Additional catkin Make Args: None Internal Make Job Server: True Cache Job Environments: False ---------------------------------------------------------------- Whitelisted Packages: None Blacklisted Packages: None ---------------------------------------------------------------- Workspace configuration appears valid. NOTE: Forcing CMake to run for each package. ---------------------------------------------------------------- [ build ] Found '1' package in 0 .0 seconds. [ build ] Updating package table. Starting >>> catkin_tools_prebuild Finished <<< catkin_tools_prebuild [ 9 .9 seconds ] Starting >>> grove_ultrasonic_ranger Finished <<< grove_ultrasonic_ranger [ 12 .0 seconds ] [ build ] Summary: All 2 packages succeeded! [ build ] Ignored: None. [ build ] Warnings: None. [ build ] Abandoned: None. [ build ] Failed: None. [ build ] Runtime: 21 .9 seconds total. [ build ] Note: Workspace packages have changed, please re-source setup files to use them. As the final note of the build output suggests, we have to source the setup.bash files in the devel space. 1 fjp@ubuntu:~/git/2wd-robot/ros$ source devel/setup.bash To make the ranger node executable we have to modify the ranger.py file: 1 fjp@ubuntu:~/git/2wd-robot/ros/src/grove_ultrasonic_ranger/src$ sudo chmod a+x ranger.py Then we can test the node using rosrun : 1 2 3 4 5 6 7 8 9 10 fjp@ubuntu:~/git/2wd-robot/ros$ sudo su [ sudo ] password for fjp: root@ubuntu:/home/fjp/git/2wd-robot/ros# source devel/setup.bash root@ubuntu:/home/fjp/git/2wd-robot/ros# rosrun grove_ultrasonic_ranger ranger.py Distance : 1 .617 m Distance : 1 .617 m Distance : 1 .617 m Distance : 0 .108 m Distance : 0 .092 m Distance : 0 .099 m This lets the node publish range messages which we can capture in another terminal window using rostopic . First we use rostopic list to find the name of the topic we are interested in: 1 2 3 4 fjp@ubuntu:~/git/2wd-robot/ros$ rostopic list /distance /rosout /rosout_agg We named our topic /distance which we can use with the rostopic echo command to see the published messages: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 fjp@ubuntu:~/git/2wd-robot/ros$ rostopic echo /distance header: seq: 1 stamp: secs: 1576778377 nsecs: 746809959 frame_id: \"ranger_distance\" radiation_type: 0 field_of_view: 0 .261799007654 min_range: 0 .019999999553 max_range: 3 .5 range: 1 .61674261093 --- header: seq: 2 stamp: secs: 1576778378 nsecs: 459048986 frame_id: \"ranger_distance\" radiation_type: 0 field_of_view: 0 .261799007654 min_range: 0 .019999999553 max_range: 3 .5 range: 1 .61261284351 --- header: seq: 3 stamp: secs: 1576778379 nsecs: 172172069 frame_id: \"ranger_distance\" radiation_type: 0 field_of_view: 0 .261799007654 min_range: 0 .019999999553 max_range: 3 .5 range: 1 .61657905579 --- header: seq: 4 stamp: secs: 1576778379 nsecs: 884002923 frame_id: \"ranger_distance\" radiation_type: 0 field_of_view: 0 .261799007654 min_range: 0 .019999999553 max_range: 3 .5 range: 1 .61277639866 --- header: seq: 5 stamp: secs: 1576778380 nsecs: 596549034 frame_id: \"ranger_distance\" radiation_type: 0 field_of_view: 0 .261799007654 min_range: 0 .019999999553 max_range: 3 .5 range: 1 .67693090439 ---","title":"ROS Node for Ultrasonic Ranger"},{"location":"grove_ultrasonic_ranger/#informational-distance-measurements","text":"To provide additional information when an obstacle is too close or the robot has no obstacle in front of it we use REP-117 as guideline. To successfully implement this REP, checks for valid measurements should use floating-point standards and follow this form: 1 2 3 4 5 6 7 8 9 10 11 if ( minimum_range <= value && value <= maximum_range ){ // Represents expected pre-REP logic and is the only necessary condition for most applications. // This is a valid measurement. } else if ( ! isfinite ( value ) && value < 0 ){ // Object too close to measure. } else if ( ! isfinite ( value ) && value > 0 ){ // No objects detected in range. } else if ( isnan ( value ) ){ // This is an erroneous, invalid, or missing measurement. } else { // The sensor reported these measurements as valid, but they are discarded per the limits defined by minimum_range and maximum_range. }","title":"Informational Distance Measurements"},{"location":"hardware-interfaces/","text":"The hardware interfaces provide an interface between the components (sensors and actuators) of the 2WD robot and the main processing unit, the Raspberry Pi 4 B. GPIO Currently, three GPIO pins are used to connect the ultrasonic ranger and two speed sensors. The ultrasonic ranger uses just a single GPIO pin for communicating its measured distances. Therefore, we can use one of the GPIO pins such as physical pin 11 . The LM393 speed sensors also use a single digital GPIO pin each. These pins will be setup using software interrupts with the RPi.GPIO library. Prepare I2C Connection The I2C connections are used for multiple components such as the motor driver and the oled display. I2C Pinout on Raspberry Pi 4 B. Using these ports on the Raspberry Pi 4 B, requires that we enable the I2C interface. To do so, we will use the tool i2cdetect which requires that we install a tool on Ubuntu called i2c-tools : 1 2 3 4 5 6 7 fjp@ubuntu:~/git/2wd-robot$ i2cdetect Command 'i2cdetect' not found, but can be installed with: sudo apt install i2c-tools fjp@ubuntu:~/git/2wd-robot$ sudo apt install i2c-tools This i2cdetect tool is a userspace program to scan an I2C bus for devices given a specific i2cbus argument which indicates the number or name of the I2C bus to be scanned, and should correspond to one of the busses listed by i2cdetect -l . See also info i2cdetect for the manual page. To test if the i2c ports are working we use the following commands: 1 2 3 4 $ i2cdetect -y 0 Error: Could not open file '/dev/i2c-0' or '/dev/i2c/0': No such file or directory $ i2cdetect -y 1 Error: Could not open file '/dev/i2c-1' or '/dev/i2c/1': No such file or directory The ports are not setup correctly yet, which is why we need to enable the following two lines in the /boot/firmware/config.txt file: 1 2 dtparam = i2c0 = on dtparam = i2c1 = on After rebooting the Raspberry Pi and entering the command again the following output will appear: 1 2 3 $ i2cdetect -y 0 Error: Could not open file `/dev/i2c-0': Permission denied Run as root? Running as root using sudo will work (please read on, there is a better way): 1 2 3 4 5 6 7 8 9 10 11 $ sudo i2cdetect -y 0 [sudo] password for fjp: 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- -- 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: -- -- -- -- -- -- -- -- As mentioned there is a better way to access the i2c devices without using power user privileges. When issuing the following: 1 2 ls -l /dev/i2c-0 crw-rw---- 1 root i2c 89, 0 Apr 1 2020 /dev/i2c-0 we see that the /dev/i2c-0 device belongs to user root and i2c user group. To get access without sudo we can add other users, requiering access to the i2c group with: 1 2 3 4 sudo adduser fjp i2c Adding user `fjp' to group `i2c' ... Adding user fjp to group i2c Done. After logging out and back in again the access will be granted and following output will come up: 1 2 3 4 5 6 7 8 9 10 $ i2cdetect -y 0 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- -- 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: -- -- -- -- -- -- -- -- It outputs a table with the list of detected devices on the specified bus. In this case there are no connected devices on I2C bus 0. Alternative setup using raspi-config On Raspian Buster, the official Raspberry OS, we could use the `raspi-config` tool: fjp@ubuntu : ~/git/2wd-robot $ sudo raspi-config Raspberry Pi 4 Model B Rev 1.1 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 Raspberry Pi Software Configuration Tool (raspi-config) \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 1 Change User Password Change password for the current user \u2502 \u2502 2 Network Options Configure network settings \u2502 \u2502 3 Boot Options Configure options for start-up \u2502 \u2502 4 Localisation Options Set up language and regional settings to match your location \u2502 \u2502 5 Interfacing Options Configure connections to peripherals \u2502 \u2502 6 Overclock Configure overclocking for your Pi \u2502 \u2502 7 Advanced Options Configure advanced settings \u2502 \u2502 8 Update Update this tool to the latest version \u2502 \u2502 9 About raspi-config Information about this configuration tool \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 <Select> <Finish> \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Select the i2c option: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 Raspberry Pi Software Configuration Tool (raspi-config) \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 P1 Camera Enable/Disable connection to the Raspberry Pi Camera \u2502 \u2502 P2 SSH Enable/Disable remote command line access to your Pi using SSH \u2502 \u2502 P3 VNC Enable/Disable graphical remote access to your Pi using RealVNC \u2502 \u2502 P4 SPI Enable/Disable automatic loading of SPI kernel module \u2502 \u2502 P5 I2C Enable/Disable automatic loading of I2C kernel module \u2502 \u2502 P6 Serial Enable/Disable shell and kernel messages on the serial connection \u2502 \u2502 P7 1-Wire Enable/Disable one-wire interface \u2502 \u2502 P8 Remote GPIO Enable/Disable remote access to GPIO pins \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 <Select> <Back> \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 And enable the interface: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 Would you like the ARM I2C interface to be enabled? \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 <Yes> <No> \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Confirm the activation and restart the RPi: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 The ARM I2C interface is enabled \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 <Ok> \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 USB Devices Similar to accessing i2c devices, a non root user can use usb connections by adding it to the the dialout group: 1 2 3 4 sudo adduser fjp dialout Adding user `fjp' to group `dialout' ... Adding user fjp to group dialout Done.","title":"Hardware Interfaces"},{"location":"hardware-interfaces/#gpio","text":"Currently, three GPIO pins are used to connect the ultrasonic ranger and two speed sensors. The ultrasonic ranger uses just a single GPIO pin for communicating its measured distances. Therefore, we can use one of the GPIO pins such as physical pin 11 . The LM393 speed sensors also use a single digital GPIO pin each. These pins will be setup using software interrupts with the RPi.GPIO library.","title":"GPIO"},{"location":"hardware-interfaces/#prepare-i2c-connection","text":"The I2C connections are used for multiple components such as the motor driver and the oled display. I2C Pinout on Raspberry Pi 4 B. Using these ports on the Raspberry Pi 4 B, requires that we enable the I2C interface. To do so, we will use the tool i2cdetect which requires that we install a tool on Ubuntu called i2c-tools : 1 2 3 4 5 6 7 fjp@ubuntu:~/git/2wd-robot$ i2cdetect Command 'i2cdetect' not found, but can be installed with: sudo apt install i2c-tools fjp@ubuntu:~/git/2wd-robot$ sudo apt install i2c-tools This i2cdetect tool is a userspace program to scan an I2C bus for devices given a specific i2cbus argument which indicates the number or name of the I2C bus to be scanned, and should correspond to one of the busses listed by i2cdetect -l . See also info i2cdetect for the manual page. To test if the i2c ports are working we use the following commands: 1 2 3 4 $ i2cdetect -y 0 Error: Could not open file '/dev/i2c-0' or '/dev/i2c/0': No such file or directory $ i2cdetect -y 1 Error: Could not open file '/dev/i2c-1' or '/dev/i2c/1': No such file or directory The ports are not setup correctly yet, which is why we need to enable the following two lines in the /boot/firmware/config.txt file: 1 2 dtparam = i2c0 = on dtparam = i2c1 = on After rebooting the Raspberry Pi and entering the command again the following output will appear: 1 2 3 $ i2cdetect -y 0 Error: Could not open file `/dev/i2c-0': Permission denied Run as root? Running as root using sudo will work (please read on, there is a better way): 1 2 3 4 5 6 7 8 9 10 11 $ sudo i2cdetect -y 0 [sudo] password for fjp: 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- -- 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: -- -- -- -- -- -- -- -- As mentioned there is a better way to access the i2c devices without using power user privileges. When issuing the following: 1 2 ls -l /dev/i2c-0 crw-rw---- 1 root i2c 89, 0 Apr 1 2020 /dev/i2c-0 we see that the /dev/i2c-0 device belongs to user root and i2c user group. To get access without sudo we can add other users, requiering access to the i2c group with: 1 2 3 4 sudo adduser fjp i2c Adding user `fjp' to group `i2c' ... Adding user fjp to group i2c Done. After logging out and back in again the access will be granted and following output will come up: 1 2 3 4 5 6 7 8 9 10 $ i2cdetect -y 0 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- -- 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: -- -- -- -- -- -- -- -- It outputs a table with the list of detected devices on the specified bus. In this case there are no connected devices on I2C bus 0. Alternative setup using raspi-config On Raspian Buster, the official Raspberry OS, we could use the `raspi-config` tool: fjp@ubuntu : ~/git/2wd-robot $ sudo raspi-config Raspberry Pi 4 Model B Rev 1.1 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 Raspberry Pi Software Configuration Tool (raspi-config) \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 1 Change User Password Change password for the current user \u2502 \u2502 2 Network Options Configure network settings \u2502 \u2502 3 Boot Options Configure options for start-up \u2502 \u2502 4 Localisation Options Set up language and regional settings to match your location \u2502 \u2502 5 Interfacing Options Configure connections to peripherals \u2502 \u2502 6 Overclock Configure overclocking for your Pi \u2502 \u2502 7 Advanced Options Configure advanced settings \u2502 \u2502 8 Update Update this tool to the latest version \u2502 \u2502 9 About raspi-config Information about this configuration tool \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 <Select> <Finish> \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Select the i2c option: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 Raspberry Pi Software Configuration Tool (raspi-config) \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 P1 Camera Enable/Disable connection to the Raspberry Pi Camera \u2502 \u2502 P2 SSH Enable/Disable remote command line access to your Pi using SSH \u2502 \u2502 P3 VNC Enable/Disable graphical remote access to your Pi using RealVNC \u2502 \u2502 P4 SPI Enable/Disable automatic loading of SPI kernel module \u2502 \u2502 P5 I2C Enable/Disable automatic loading of I2C kernel module \u2502 \u2502 P6 Serial Enable/Disable shell and kernel messages on the serial connection \u2502 \u2502 P7 1-Wire Enable/Disable one-wire interface \u2502 \u2502 P8 Remote GPIO Enable/Disable remote access to GPIO pins \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 <Select> <Back> \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 And enable the interface: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 Would you like the ARM I2C interface to be enabled? \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 <Yes> <No> \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Confirm the activation and restart the RPi: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 The ARM I2C interface is enabled \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 <Ok> \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"Prepare I2C Connection"},{"location":"hardware-interfaces/#usb-devices","text":"Similar to accessing i2c devices, a non root user can use usb connections by adding it to the the dialout group: 1 2 3 4 sudo adduser fjp dialout Adding user `fjp' to group `dialout' ... Adding user fjp to group dialout Done.","title":"USB Devices"},{"location":"laser-range-scanner/","text":"Laser Range Scanner For SLAM (simultaneous localization and mapping) and navigation a laser range scanner is a cruical requirement to use the ROS navigation stack (see also diffbot_navigation ). DiffBot uses the 360 field of view RPLidar A2 M8 with 12 meter range. Mounting When installing the RPLidar on DiffBot make sure it is located high enough to avoid occluding the laser. It is also important to mount the laser in the correct location and that its direction and orientation matches the origin alignment in the robot description. In the figure below we can see, that the side with the communication cable is pointing in the positive x-direction. In case the cable should be mounted in the opposite direction, it is necessary to adapt the origin of the laser link in the robot description. Installation manual for RPLidar A2 (source: [robopeak/rplidar_ros/](https://github.com/robopeak/rplidar_ros/wiki/How-to-use-rplidar).","title":"Laser Range Scanner"},{"location":"laser-range-scanner/#laser-range-scanner","text":"For SLAM (simultaneous localization and mapping) and navigation a laser range scanner is a cruical requirement to use the ROS navigation stack (see also diffbot_navigation ). DiffBot uses the 360 field of view RPLidar A2 M8 with 12 meter range.","title":"Laser Range Scanner"},{"location":"laser-range-scanner/#mounting","text":"When installing the RPLidar on DiffBot make sure it is located high enough to avoid occluding the laser. It is also important to mount the laser in the correct location and that its direction and orientation matches the origin alignment in the robot description. In the figure below we can see, that the side with the communication cable is pointing in the positive x-direction. In case the cable should be mounted in the opposite direction, it is necessary to adapt the origin of the laser link in the robot description. Installation manual for RPLidar A2 (source: [robopeak/rplidar_ros/](https://github.com/robopeak/rplidar_ros/wiki/How-to-use-rplidar).","title":"Mounting"},{"location":"lm393_speed_sensor/","text":"LM393 Speed Sensor - Odometry Note that this sensor is deprecated in the current version of DiffBot because it isn't a quadrature encoder. Instead, the DG01D-E motor includes a quadrature encoder that can measure ticks (can be converted to speed) and the direction the motor is turning (clock-wise or anti-clock-wise). The LM393 speed sensor could be used in combination with an additional information about the current driving direction, for example coming from the software. Howerver, this won't be as accurate as using a quadrature encoder that provides this information. {: .notice } To measure how far the robot has driven, we use the LM393 speed sensor from Joy-IT as odometry sensor. First, we will create a ROS package with catkin create pkg PKG_NAME [--catkin-deps [DEP [DEP ...]]] : 1 2 3 4 5 6 7 fjp@ubuntu:~/git/2wd-robot/ros/src$ catkin create pkg lm393_speed_sensor --catkin-deps rospy roscpp nav_msgs Creating package \"lm393_speed_sensor\" in \"/home/fjp/git/2wd-robot/ros/src\" ... Created file lm393_speed_sensor/CMakeLists.txt Created file lm393_speed_sensor/package.xml Created folder lm393_speed_sensor/include/lm393_speed_sensor Created folder lm393_speed_sensor/src Successfully created package files in /home/fjp/git/2wd-robot/ros/src/lm393_speed_sensor. The package depends on the two ROS client libraries rospy and roscpp . The current implementation uses python and the RPi.GPIO library for interrupts. To achieve more percise results, C++ should be used instead. To signalise the current pose of the robot in the odometry frame, the nav_msgs/Range message is used. Connection To get the speed sensors working, we connect the signal pins to (physical) GPIO 15 and (physical) GPIO 16 of the Raspberry Pi 4 B and power them with 3.3V. The ground pins are connected to ground of the Pi. LM393 Speed Sensor Library To use the LM393 speed sensor as a ROS node the sensor functionality is wraped in a class. This provides an easy to extend interface for the speed sensor ( API ) The code consists of a class LM393SpeedSensor which has two interrupt service routines (ISR) methods. Using the RPi.GPIO interrupt capabilities, these ISR methods are used as callback functions when the sensor measures a falling edge. This is the case when the rotary disk spins and the optocoupler measures a high to low signal due to the spinning disk. The sensor API is implemented in the lm393_speed_sensor.py python module. Executing the module will result in the following output when the motors are spinning freely with full speed and using some force to slow them down. We see that the rotational speed \\(\\omega\\) , measured in RPM ( revolutions per minute ) changes. 1 2 fjp@ubuntu:~/git/2wd-robot/ros/src/lm393_speed_sensor/src$ sudo python lm393_speed_sensor.py TODO The rotational speed \\(\\omega\\) values will be converted to tangential velocity values using the radius \\(r\\) of the wheels. \\[ v = \\omega \\cdot r = 2 \\pi n \\cdot r \\] ROS Node for LM393 Speed Sensor ROS provides the Odometry Message in the nav_msgs header . This message type can be used to write a wrapper that will act as a ROS node for the LM393 speed sensor. To design this node we will send out measurements periodically over a topic of type nav_msgs/Odometry . The code for this node is in speed_sensor.py . After writing the node we need to build the packages in the workspace with catkin build . 1 2 fjp@ubuntu:~/git/2wd-robot/ros$ catkin build TODO As the final note of the build output suggests, we have to source the setup.bash files in the devel space. 1 fjp@ubuntu:~/git/2wd-robot/ros$ source devel/setup.bash To make the speed_sensor node executable we have to modify the speed_sensor.py file: 1 fjp@ubuntu:~/git/2wd-robot/ros/src/grove_ultrasonic_ranger/src$ sudo chmod a+x speed_sensor.py Then we can test the node using rosrun : 1 2 3 4 fjp@ubuntu:~/git/2wd-robot/ros$ sudo su [ sudo ] password for fjp: root@ubuntu:/home/fjp/git/2wd-robot/ros# source devel/setup.bash root@ubuntu:/home/fjp/git/2wd-robot/ros# rosrun lm393_speed_sensor speed_sensor.py This lets the node publish range messages which we can capture in another terminal window using rostopic . First we use rostopic list to find the name of the topic we are interested in: 1 2 fjp@ubuntu:~/git/2wd-robot/ros$ rostopic list TODO We named our topic /odom which we can use with the rostopic echo command to see the published messages:","title":"Lm393 speed sensor"},{"location":"lm393_speed_sensor/#lm393-speed-sensor-odometry","text":"Note that this sensor is deprecated in the current version of DiffBot because it isn't a quadrature encoder. Instead, the DG01D-E motor includes a quadrature encoder that can measure ticks (can be converted to speed) and the direction the motor is turning (clock-wise or anti-clock-wise). The LM393 speed sensor could be used in combination with an additional information about the current driving direction, for example coming from the software. Howerver, this won't be as accurate as using a quadrature encoder that provides this information. {: .notice } To measure how far the robot has driven, we use the LM393 speed sensor from Joy-IT as odometry sensor. First, we will create a ROS package with catkin create pkg PKG_NAME [--catkin-deps [DEP [DEP ...]]] : 1 2 3 4 5 6 7 fjp@ubuntu:~/git/2wd-robot/ros/src$ catkin create pkg lm393_speed_sensor --catkin-deps rospy roscpp nav_msgs Creating package \"lm393_speed_sensor\" in \"/home/fjp/git/2wd-robot/ros/src\" ... Created file lm393_speed_sensor/CMakeLists.txt Created file lm393_speed_sensor/package.xml Created folder lm393_speed_sensor/include/lm393_speed_sensor Created folder lm393_speed_sensor/src Successfully created package files in /home/fjp/git/2wd-robot/ros/src/lm393_speed_sensor. The package depends on the two ROS client libraries rospy and roscpp . The current implementation uses python and the RPi.GPIO library for interrupts. To achieve more percise results, C++ should be used instead. To signalise the current pose of the robot in the odometry frame, the nav_msgs/Range message is used.","title":"LM393 Speed Sensor - Odometry"},{"location":"lm393_speed_sensor/#connection","text":"To get the speed sensors working, we connect the signal pins to (physical) GPIO 15 and (physical) GPIO 16 of the Raspberry Pi 4 B and power them with 3.3V. The ground pins are connected to ground of the Pi.","title":"Connection"},{"location":"lm393_speed_sensor/#lm393-speed-sensor-library","text":"To use the LM393 speed sensor as a ROS node the sensor functionality is wraped in a class. This provides an easy to extend interface for the speed sensor ( API ) The code consists of a class LM393SpeedSensor which has two interrupt service routines (ISR) methods. Using the RPi.GPIO interrupt capabilities, these ISR methods are used as callback functions when the sensor measures a falling edge. This is the case when the rotary disk spins and the optocoupler measures a high to low signal due to the spinning disk. The sensor API is implemented in the lm393_speed_sensor.py python module. Executing the module will result in the following output when the motors are spinning freely with full speed and using some force to slow them down. We see that the rotational speed \\(\\omega\\) , measured in RPM ( revolutions per minute ) changes. 1 2 fjp@ubuntu:~/git/2wd-robot/ros/src/lm393_speed_sensor/src$ sudo python lm393_speed_sensor.py TODO The rotational speed \\(\\omega\\) values will be converted to tangential velocity values using the radius \\(r\\) of the wheels. \\[ v = \\omega \\cdot r = 2 \\pi n \\cdot r \\]","title":"LM393 Speed Sensor Library"},{"location":"lm393_speed_sensor/#ros-node-for-lm393-speed-sensor","text":"ROS provides the Odometry Message in the nav_msgs header . This message type can be used to write a wrapper that will act as a ROS node for the LM393 speed sensor. To design this node we will send out measurements periodically over a topic of type nav_msgs/Odometry . The code for this node is in speed_sensor.py . After writing the node we need to build the packages in the workspace with catkin build . 1 2 fjp@ubuntu:~/git/2wd-robot/ros$ catkin build TODO As the final note of the build output suggests, we have to source the setup.bash files in the devel space. 1 fjp@ubuntu:~/git/2wd-robot/ros$ source devel/setup.bash To make the speed_sensor node executable we have to modify the speed_sensor.py file: 1 fjp@ubuntu:~/git/2wd-robot/ros/src/grove_ultrasonic_ranger/src$ sudo chmod a+x speed_sensor.py Then we can test the node using rosrun : 1 2 3 4 fjp@ubuntu:~/git/2wd-robot/ros$ sudo su [ sudo ] password for fjp: root@ubuntu:/home/fjp/git/2wd-robot/ros# source devel/setup.bash root@ubuntu:/home/fjp/git/2wd-robot/ros# rosrun lm393_speed_sensor speed_sensor.py This lets the node publish range messages which we can capture in another terminal window using rostopic . First we use rostopic list to find the name of the topic we are interested in: 1 2 fjp@ubuntu:~/git/2wd-robot/ros$ rostopic list TODO We named our topic /odom which we can use with the rostopic echo command to see the published messages:","title":"ROS Node for LM393 Speed Sensor"},{"location":"power-supply/","text":"Power Supply DiffBot uses two different power sources. One USB type C power bank providing 5V to the Raspberry Pi 4B. The other source comes from battery holder, mounted at the bottom of the base plate which can hold up to four AA sized batteries. The standard AA battery has a charge of 2700 mAh and a nominal Voltage of 1.5 V. This means that with four batteries in series the total Voltage supplied to the motor driver would be 6 V. The voltage drop on the motor driver is roughly 2.12 V which means that each motor would operate on 3.88 V. According to the datasheet the DG01D-E motor requires a voltage between 3-9 V, has a gearbox ratio of 1:48 and a speed of 90RPM at 4.5V. Therefore 3.88 V is at the lower operating range of this motor, hence a higher input source to the motor driver is requried to provide higher output voltage to the motors. The maximum allowed input Voltage to the motor driver is 15 V, hence a power source between 6 V to 15 V should be used, in the range of 10 V to 15 V to drive with different speeds. In order to reuse the battery holder on the bottom plate a battery with the same AA form factor but higher voltage is what we are looking for. A helpful overview of different batteris with their exact naming can be found in the List of battery sizes on Wikipedia. Here we can see that the 14500 recharchable Lithium-ion battery provides 3.7 V with a capacity of approximately 900 mAh. This means that four batteries of these provide 14.8 V which is right below the maximum 15 V that the motor driver can handle. Although the capacity is lower compared to non recharchable AA batteries (2700 mAh), these type of batteries can be reused multiple times when charged with a suitable battery charchger such as the Nitecore UMS4 or the XTAR VC4 . DiffBot uses four 14500 batteries from Trustfire with 3.7 V and 900 mAh. Note that four standard AA bateries with 1.5 V each is perfectly fine. Only the motor driver has to provide 100% of its input voltage to avoid stalling the motors. With higher Voltage rated batteries more different speeds will be possible. {: .notice }","title":"Power Supply"},{"location":"power-supply/#power-supply","text":"DiffBot uses two different power sources. One USB type C power bank providing 5V to the Raspberry Pi 4B. The other source comes from battery holder, mounted at the bottom of the base plate which can hold up to four AA sized batteries. The standard AA battery has a charge of 2700 mAh and a nominal Voltage of 1.5 V. This means that with four batteries in series the total Voltage supplied to the motor driver would be 6 V. The voltage drop on the motor driver is roughly 2.12 V which means that each motor would operate on 3.88 V. According to the datasheet the DG01D-E motor requires a voltage between 3-9 V, has a gearbox ratio of 1:48 and a speed of 90RPM at 4.5V. Therefore 3.88 V is at the lower operating range of this motor, hence a higher input source to the motor driver is requried to provide higher output voltage to the motors. The maximum allowed input Voltage to the motor driver is 15 V, hence a power source between 6 V to 15 V should be used, in the range of 10 V to 15 V to drive with different speeds. In order to reuse the battery holder on the bottom plate a battery with the same AA form factor but higher voltage is what we are looking for. A helpful overview of different batteris with their exact naming can be found in the List of battery sizes on Wikipedia. Here we can see that the 14500 recharchable Lithium-ion battery provides 3.7 V with a capacity of approximately 900 mAh. This means that four batteries of these provide 14.8 V which is right below the maximum 15 V that the motor driver can handle. Although the capacity is lower compared to non recharchable AA batteries (2700 mAh), these type of batteries can be reused multiple times when charged with a suitable battery charchger such as the Nitecore UMS4 or the XTAR VC4 . DiffBot uses four 14500 batteries from Trustfire with 3.7 V and 900 mAh. Note that four standard AA bateries with 1.5 V each is perfectly fine. Only the motor driver has to provide 100% of its input voltage to avoid stalling the motors. With higher Voltage rated batteries more different speeds will be possible. {: .notice }","title":"Power Supply"},{"location":"robot-description/","text":"DiffBot Robot Description The description of the 2WD robot will be created in its own package named diffbot_description . The description uses URDF and xacro . For this we create a package with catkin create pkg PKG_NAME [--catkin-deps [DEP [DEP ...]]] : 1 2 3 4 5 fjp@ubuntu:~/git/diffbot/ros/src$ catkin create pkg diffbot_description Creating package \"diffbot_description\" in \"/home/fjp/git/diffbot/ros/src\"... Created file diffbot_description/CMakeLists.txt Created file diffbot_description/package.xml Successfully created package files in /home/fjp/git/diffbot/ros/src/diffbot_description. Because this package contains only descriptions and launch files it doesn't require any dependencies. According to ROS conventions we create the following folders where the individual files realted to the robot description will be placed: 1 fjp@ubuntu:~/git/diffbot/ros/src/robot_description$ mkdir urdf meshes launch The urdf folder will be used to keep the urdf and xacro files. The meshes folder keeps the meshes that are included in the urdf file, and the launch folder keeps the ROS launch files. Robot Model To model the two wheeled differential drive robot we follow REP-120 . It states to use a base_link and a base_footprint . The resulting description files can be found in the diffbot_description package. Required Tools To check a urdf file we can make use of the tools check_urdf and urdf_to_graphiz in the liburdfdom-tools debian package. Install it with the following command: 1 sudo apt install liburdfdom-tools First we need to convert the robot description of DiffBot, which is present as xacro file, to a urdf file by issuing the following command: 1 fjp@ubuntu:~/git/diffbot/ros$ rosrun xacro xacro ` rospack find diffbot_description ` /urdf/diffbot.urdf.xacro -o /tmp/diffbot.urdf After we've created the urdf from the xacro file we can check the urdf files for errors with: 1 2 3 4 5 6 7 8 fjp@ubuntu:/tmp$ check_urdf diffbot.urdf robot name is: diffbot ---------- Successfully Parsed XML --------------- root Link: base_footprint has 1 child(ren) child(1): base_link child(1): caster_link child(2): front_left_wheel child(3): front_right_wheel It is also helpful to output a graphviz diagram of the robot model: 1 2 3 4 fjp@ubuntu:/tmp$ urdf_to_graphiz diffbot.urdf Created file diffbot.gv Created file diffbot.pdf fjp@ubuntu:/tmp$ evince diffbot.pdf Graphviz diagram of DiffBot URDF robot description. To visualize the 3D model in RViz we first need to install the joint-state-publisher-gui which was separated from non-gui joint-state-publisher . There exists a debian package which can be installed with the following command: 1 fjp@ubuntu:~/git/diffbot/ros$ sudo apt install ros-noetic-joint-state-publisher-gui After installing the required dependency, the view_diffbot.launch launch file can be executed using roslaunch command: 1 fjp@ubuntu:~/git/diffbot/ros$ roslaunch diffbot_description view_diffbot.launch According to the launch file's configuration, this will show the robot in RViz together with the joint-state-publisher-gui to set the joint values: DiffBot displayed in RViz. With the robot descripton loaded on the ROS parameter server, it's possible to use the TF Tree rqt plugin to display the transformation tree (see image above). In the next section, Gazebo Simulation , the robot model is prepared for simulation inside of Gazebo .","title":"Robot Description"},{"location":"robot-description/#diffbot-robot-description","text":"The description of the 2WD robot will be created in its own package named diffbot_description . The description uses URDF and xacro . For this we create a package with catkin create pkg PKG_NAME [--catkin-deps [DEP [DEP ...]]] : 1 2 3 4 5 fjp@ubuntu:~/git/diffbot/ros/src$ catkin create pkg diffbot_description Creating package \"diffbot_description\" in \"/home/fjp/git/diffbot/ros/src\"... Created file diffbot_description/CMakeLists.txt Created file diffbot_description/package.xml Successfully created package files in /home/fjp/git/diffbot/ros/src/diffbot_description. Because this package contains only descriptions and launch files it doesn't require any dependencies. According to ROS conventions we create the following folders where the individual files realted to the robot description will be placed: 1 fjp@ubuntu:~/git/diffbot/ros/src/robot_description$ mkdir urdf meshes launch The urdf folder will be used to keep the urdf and xacro files. The meshes folder keeps the meshes that are included in the urdf file, and the launch folder keeps the ROS launch files.","title":"DiffBot Robot Description"},{"location":"robot-description/#robot-model","text":"To model the two wheeled differential drive robot we follow REP-120 . It states to use a base_link and a base_footprint . The resulting description files can be found in the diffbot_description package.","title":"Robot Model"},{"location":"robot-description/#required-tools","text":"To check a urdf file we can make use of the tools check_urdf and urdf_to_graphiz in the liburdfdom-tools debian package. Install it with the following command: 1 sudo apt install liburdfdom-tools First we need to convert the robot description of DiffBot, which is present as xacro file, to a urdf file by issuing the following command: 1 fjp@ubuntu:~/git/diffbot/ros$ rosrun xacro xacro ` rospack find diffbot_description ` /urdf/diffbot.urdf.xacro -o /tmp/diffbot.urdf After we've created the urdf from the xacro file we can check the urdf files for errors with: 1 2 3 4 5 6 7 8 fjp@ubuntu:/tmp$ check_urdf diffbot.urdf robot name is: diffbot ---------- Successfully Parsed XML --------------- root Link: base_footprint has 1 child(ren) child(1): base_link child(1): caster_link child(2): front_left_wheel child(3): front_right_wheel It is also helpful to output a graphviz diagram of the robot model: 1 2 3 4 fjp@ubuntu:/tmp$ urdf_to_graphiz diffbot.urdf Created file diffbot.gv Created file diffbot.pdf fjp@ubuntu:/tmp$ evince diffbot.pdf Graphviz diagram of DiffBot URDF robot description. To visualize the 3D model in RViz we first need to install the joint-state-publisher-gui which was separated from non-gui joint-state-publisher . There exists a debian package which can be installed with the following command: 1 fjp@ubuntu:~/git/diffbot/ros$ sudo apt install ros-noetic-joint-state-publisher-gui After installing the required dependency, the view_diffbot.launch launch file can be executed using roslaunch command: 1 fjp@ubuntu:~/git/diffbot/ros$ roslaunch diffbot_description view_diffbot.launch According to the launch file's configuration, this will show the robot in RViz together with the joint-state-publisher-gui to set the joint values: DiffBot displayed in RViz. With the robot descripton loaded on the ROS parameter server, it's possible to use the TF Tree rqt plugin to display the transformation tree (see image above). In the next section, Gazebo Simulation , the robot model is prepared for simulation inside of Gazebo .","title":"Required Tools"},{"location":"ros-network-setup/","text":"ROS Network Setup ROS is a distributed computing environment. This allows running compute expensive tasks such as visualization or path planning for navigation on machines with more performance and sending goals to robots operating on less performant hardware like DiffBot with its Raspberry Pi 4 B. For detailed instructions see ROS Network Setup and the ROS Environment Variables . The setup between the work machine that handles compute heavy tasks and DiffBot is as follows: TODO image On DiffBot we configure the ROS_MASTER_URI to be the IP address of the work machine. 1 export ROS_MASTER_URI=http://192.168.0.9:11311/ To test run the master on the work machine using the roscore command. Then run the the listener from the roscpp_tutorials package in another terminal: 1 fjp@workmachine:~/git/diffbot/ros$ rosrun roscpp_tutorials listener Then switch to a terminal on DiffBot's Raspberry Pi and run the talker from roscpp_tutorials : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 fjp@diffbot:~/git/diffbot/ros$ rosrun roscpp_tutorials talker [ INFO] [1602018325.633133449]: hello world 0 [ INFO] [1602018325.733137152]: hello world 1 [ INFO] [1602018325.833112540]: hello world 2 [ INFO] [1602018325.933114483]: hello world 3 [ INFO] [1602018326.033114093]: hello world 4 [ INFO] [1602018326.133112684]: hello world 5 [ INFO] [1602018326.233112183]: hello world 6 [ INFO] [1602018326.333113126]: hello world 7 [ INFO] [1602018326.433113680]: hello world 8 [ INFO] [1602018326.533113031]: hello world 9 [ INFO] [1602018326.633110140]: hello world 10 [ INFO] [1602018326.733108954]: hello world 11 [ INFO] [1602018326.833113267]: hello world 12 [ INFO] [1602018326.933164505]: hello world 13 [ INFO] [1602018327.033119135]: hello world 14 [ INFO] [1602018327.133113559]: hello world 15 [ INFO] [1602018327.233111003]: hello world 16 [ INFO] [1602018327.333110705]: hello world 17 [ INFO] [1602018327.433126425]: hello world 18 [ INFO] [1602018327.533111498]: hello world 19 [ INFO] [1602018327.633107978]: hello world 20 [ INFO] [1602018327.733110736]: hello world 21 [ INFO] [1602018327.833107605]: hello world 22 [ INFO] [1602018327.933111659]: hello world 23 [ INFO] [1602018328.033108065]: hello world 24 [ INFO] [1602018328.133110379]: hello world 25 [ INFO] [1602018328.233150191]: hello world 26 [ INFO] [1602018328.333135986]: hello world 27 [ INFO] [1602018328.433153558]: hello world 28 [ INFO] [1602018328.533154557]: hello world 29 [ INFO] [1602018328.633151667]: hello world 30 [ INFO] [1602018328.733128777]: hello world 31 [ INFO] [1602018328.833170108]: hello world 32 [ INFO] [1602018328.933172402]: hello world 33 Looking back at the terminal on the work machine you should see the output: 1 2 3 4 5 6 7 8 fjp.github.io git:(master) rosrun roscpp_tutorials listener [ INFO] [1602018328.330070016]: I heard: [hello world 27] [ INFO] [1602018328.430244670]: I heard: [hello world 28] [ INFO] [1602018328.530173113]: I heard: [hello world 29] [ INFO] [1602018328.630251690]: I heard: [hello world 30] [ INFO] [1602018328.730334064]: I heard: [hello world 31] [ INFO] [1602018328.830346566]: I heard: [hello world 32] [ INFO] [1602018328.930009032]: I heard: [hello world 33] Note that it can take some time receiving the messages from DiffBot on the work machine, which we can see from the time stamps in the outputs above.","title":"ROS Network Setup"},{"location":"ros-network-setup/#ros-network-setup","text":"ROS is a distributed computing environment. This allows running compute expensive tasks such as visualization or path planning for navigation on machines with more performance and sending goals to robots operating on less performant hardware like DiffBot with its Raspberry Pi 4 B. For detailed instructions see ROS Network Setup and the ROS Environment Variables . The setup between the work machine that handles compute heavy tasks and DiffBot is as follows: TODO image On DiffBot we configure the ROS_MASTER_URI to be the IP address of the work machine. 1 export ROS_MASTER_URI=http://192.168.0.9:11311/ To test run the master on the work machine using the roscore command. Then run the the listener from the roscpp_tutorials package in another terminal: 1 fjp@workmachine:~/git/diffbot/ros$ rosrun roscpp_tutorials listener Then switch to a terminal on DiffBot's Raspberry Pi and run the talker from roscpp_tutorials : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 fjp@diffbot:~/git/diffbot/ros$ rosrun roscpp_tutorials talker [ INFO] [1602018325.633133449]: hello world 0 [ INFO] [1602018325.733137152]: hello world 1 [ INFO] [1602018325.833112540]: hello world 2 [ INFO] [1602018325.933114483]: hello world 3 [ INFO] [1602018326.033114093]: hello world 4 [ INFO] [1602018326.133112684]: hello world 5 [ INFO] [1602018326.233112183]: hello world 6 [ INFO] [1602018326.333113126]: hello world 7 [ INFO] [1602018326.433113680]: hello world 8 [ INFO] [1602018326.533113031]: hello world 9 [ INFO] [1602018326.633110140]: hello world 10 [ INFO] [1602018326.733108954]: hello world 11 [ INFO] [1602018326.833113267]: hello world 12 [ INFO] [1602018326.933164505]: hello world 13 [ INFO] [1602018327.033119135]: hello world 14 [ INFO] [1602018327.133113559]: hello world 15 [ INFO] [1602018327.233111003]: hello world 16 [ INFO] [1602018327.333110705]: hello world 17 [ INFO] [1602018327.433126425]: hello world 18 [ INFO] [1602018327.533111498]: hello world 19 [ INFO] [1602018327.633107978]: hello world 20 [ INFO] [1602018327.733110736]: hello world 21 [ INFO] [1602018327.833107605]: hello world 22 [ INFO] [1602018327.933111659]: hello world 23 [ INFO] [1602018328.033108065]: hello world 24 [ INFO] [1602018328.133110379]: hello world 25 [ INFO] [1602018328.233150191]: hello world 26 [ INFO] [1602018328.333135986]: hello world 27 [ INFO] [1602018328.433153558]: hello world 28 [ INFO] [1602018328.533154557]: hello world 29 [ INFO] [1602018328.633151667]: hello world 30 [ INFO] [1602018328.733128777]: hello world 31 [ INFO] [1602018328.833170108]: hello world 32 [ INFO] [1602018328.933172402]: hello world 33 Looking back at the terminal on the work machine you should see the output: 1 2 3 4 5 6 7 8 fjp.github.io git:(master) rosrun roscpp_tutorials listener [ INFO] [1602018328.330070016]: I heard: [hello world 27] [ INFO] [1602018328.430244670]: I heard: [hello world 28] [ INFO] [1602018328.530173113]: I heard: [hello world 29] [ INFO] [1602018328.630251690]: I heard: [hello world 30] [ INFO] [1602018328.730334064]: I heard: [hello world 31] [ INFO] [1602018328.830346566]: I heard: [hello world 32] [ INFO] [1602018328.930009032]: I heard: [hello world 33] Note that it can take some time receiving the messages from DiffBot on the work machine, which we can see from the time stamps in the outputs above.","title":"ROS Network Setup"},{"location":"ros-setup/","text":"ROS Installation The robot setup is supposed to run on Ubuntu Mate 20.04 Focal Fossa. ROS Noetic is intended to run with this Ubuntu version. Another program that is required to run ROS nodes written with the rospy client library is python-is-python3 . Install it with: 1 sudo apt install python-is-python3 Build Tool: catkin_tools To work with ROS we will use catkin_tools instead of catkin_make . catkin_tools provide commands such as catkin build which we will use instead of catkin_make because the catkin_tools are more actively developed than catkin_make ref . It is recommended to use catkin_tools instead of the default catkin when building ROS workspaces. catkin_tools provides a number of benefits over regular catkin_make and will be used in the documentation. All packages can be built using catkin_make however: use catkin_make in place of catkin build where appropriate. {: .notice} Follow the instructions to insall catkin_tools from a Ubuntu package repository. After sucessfully installing catkin_tools we can create and initialize a workspace (called ros for this project) with the commands listed in the build_tools documentation : Note that we already source d the setup.bash while following the ROS installation instructions . {: .notice} 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 fjp@ubuntu:~/git/2wd-robot/ros$ mkdir -p ~/git/2wd-robot/ros/src # Make a new workspace and source space fjp@ubuntu:~/git/2wd-robot/ros$ cd ~/git/2wd-robot/ros # Navigate to the workspace root fjp@ubuntu:~/git/2wd-robot/ros$ catkin init # Initialize it with a hidden marker file Initializing catkin workspace in ` /home/fjp/git/2wd-robot/ros ` . ---------------------------------------------------------------- Profile: default Extending: [ env ] /opt/ros/melodic Workspace: /home/fjp/git/2wd-robot/ros ---------------------------------------------------------------- Build Space: [ missing ] /home/fjp/git/2wd-robot/ros/build Devel Space: [ missing ] /home/fjp/git/2wd-robot/ros/devel Install Space: [ unused ] /home/fjp/git/2wd-robot/ros/install Log Space: [ missing ] /home/fjp/git/2wd-robot/ros/logs Source Space: [ exists ] /home/fjp/git/2wd-robot/ros/src DESTDIR: [ unused ] None ---------------------------------------------------------------- Devel Space Layout: linked Install Space Layout: None ---------------------------------------------------------------- Additional CMake Args: None Additional Make Args: None Additional catkin Make Args: None Internal Make Job Server: True Cache Job Environments: False ---------------------------------------------------------------- Whitelisted Packages: None Blacklisted Packages: None ---------------------------------------------------------------- Workspace configuration appears valid. ---------------------------------------------------------------- Command Overview of catkin_tools To create packages , which will be covered in the next posts in more depth, we will use catkin create pkg PKG_NAME . Building the workspace is done with catkin build . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 fjp@ubuntu:~/git/2wd-robot/ros$ catkin build ---------------------------------------------------------------- Profile: default Extending: [ env ] /opt/ros/melodic Workspace: /home/fjp/git/2wd-robot/ros ---------------------------------------------------------------- Build Space: [ exists ] /home/fjp/git/2wd-robot/ros/build Devel Space: [ exists ] /home/fjp/git/2wd-robot/ros/devel Install Space: [ unused ] /home/fjp/git/2wd-robot/ros/install Log Space: [ missing ] /home/fjp/git/2wd-robot/ros/logs Source Space: [ exists ] /home/fjp/git/2wd-robot/ros/src DESTDIR: [ unused ] None ---------------------------------------------------------------- Devel Space Layout: linked Install Space Layout: None ---------------------------------------------------------------- Additional CMake Args: None Additional Make Args: None Additional catkin Make Args: None Internal Make Job Server: True Cache Job Environments: False ---------------------------------------------------------------- Whitelisted Packages: None Blacklisted Packages: None ---------------------------------------------------------------- Workspace configuration appears valid. NOTE: Forcing CMake to run for each package. ---------------------------------------------------------------- [ build ] No packages were found in the source space '/home/fjp/git/2wd-robot/ros/src' [ build ] No packages to be built. [ build ] Package table is up to date. Starting >>> catkin_tools_prebuild Finished <<< catkin_tools_prebuild [ 10 .0 seconds ] [ build ] Summary: All 1 packages succeeded! [ build ] Ignored: None. [ build ] Warnings: None. [ build ] Abandoned: None. [ build ] Failed: None. [ build ] Runtime: 10 .1 seconds total. Finally the newly built packages have to be loaded in the environment using source . 1 fjp@ubuntu:~/git/2wd-robot/ros$ source ~/git/2wd-robot/ros/devel/setup.bash # Load the workspace's environment Resources Although the catkin tutorial uses catkin_make it provides a helpful guide to create a workspace","title":"ROS Setup"},{"location":"ros-setup/#ros-installation","text":"The robot setup is supposed to run on Ubuntu Mate 20.04 Focal Fossa. ROS Noetic is intended to run with this Ubuntu version. Another program that is required to run ROS nodes written with the rospy client library is python-is-python3 . Install it with: 1 sudo apt install python-is-python3","title":"ROS Installation"},{"location":"ros-setup/#build-tool-catkin_tools","text":"To work with ROS we will use catkin_tools instead of catkin_make . catkin_tools provide commands such as catkin build which we will use instead of catkin_make because the catkin_tools are more actively developed than catkin_make ref . It is recommended to use catkin_tools instead of the default catkin when building ROS workspaces. catkin_tools provides a number of benefits over regular catkin_make and will be used in the documentation. All packages can be built using catkin_make however: use catkin_make in place of catkin build where appropriate. {: .notice} Follow the instructions to insall catkin_tools from a Ubuntu package repository. After sucessfully installing catkin_tools we can create and initialize a workspace (called ros for this project) with the commands listed in the build_tools documentation : Note that we already source d the setup.bash while following the ROS installation instructions . {: .notice} 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 fjp@ubuntu:~/git/2wd-robot/ros$ mkdir -p ~/git/2wd-robot/ros/src # Make a new workspace and source space fjp@ubuntu:~/git/2wd-robot/ros$ cd ~/git/2wd-robot/ros # Navigate to the workspace root fjp@ubuntu:~/git/2wd-robot/ros$ catkin init # Initialize it with a hidden marker file Initializing catkin workspace in ` /home/fjp/git/2wd-robot/ros ` . ---------------------------------------------------------------- Profile: default Extending: [ env ] /opt/ros/melodic Workspace: /home/fjp/git/2wd-robot/ros ---------------------------------------------------------------- Build Space: [ missing ] /home/fjp/git/2wd-robot/ros/build Devel Space: [ missing ] /home/fjp/git/2wd-robot/ros/devel Install Space: [ unused ] /home/fjp/git/2wd-robot/ros/install Log Space: [ missing ] /home/fjp/git/2wd-robot/ros/logs Source Space: [ exists ] /home/fjp/git/2wd-robot/ros/src DESTDIR: [ unused ] None ---------------------------------------------------------------- Devel Space Layout: linked Install Space Layout: None ---------------------------------------------------------------- Additional CMake Args: None Additional Make Args: None Additional catkin Make Args: None Internal Make Job Server: True Cache Job Environments: False ---------------------------------------------------------------- Whitelisted Packages: None Blacklisted Packages: None ---------------------------------------------------------------- Workspace configuration appears valid. ----------------------------------------------------------------","title":"Build Tool: catkin_tools"},{"location":"ros-setup/#command-overview-of-catkin_tools","text":"To create packages , which will be covered in the next posts in more depth, we will use catkin create pkg PKG_NAME . Building the workspace is done with catkin build . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 fjp@ubuntu:~/git/2wd-robot/ros$ catkin build ---------------------------------------------------------------- Profile: default Extending: [ env ] /opt/ros/melodic Workspace: /home/fjp/git/2wd-robot/ros ---------------------------------------------------------------- Build Space: [ exists ] /home/fjp/git/2wd-robot/ros/build Devel Space: [ exists ] /home/fjp/git/2wd-robot/ros/devel Install Space: [ unused ] /home/fjp/git/2wd-robot/ros/install Log Space: [ missing ] /home/fjp/git/2wd-robot/ros/logs Source Space: [ exists ] /home/fjp/git/2wd-robot/ros/src DESTDIR: [ unused ] None ---------------------------------------------------------------- Devel Space Layout: linked Install Space Layout: None ---------------------------------------------------------------- Additional CMake Args: None Additional Make Args: None Additional catkin Make Args: None Internal Make Job Server: True Cache Job Environments: False ---------------------------------------------------------------- Whitelisted Packages: None Blacklisted Packages: None ---------------------------------------------------------------- Workspace configuration appears valid. NOTE: Forcing CMake to run for each package. ---------------------------------------------------------------- [ build ] No packages were found in the source space '/home/fjp/git/2wd-robot/ros/src' [ build ] No packages to be built. [ build ] Package table is up to date. Starting >>> catkin_tools_prebuild Finished <<< catkin_tools_prebuild [ 10 .0 seconds ] [ build ] Summary: All 1 packages succeeded! [ build ] Ignored: None. [ build ] Warnings: None. [ build ] Abandoned: None. [ build ] Failed: None. [ build ] Runtime: 10 .1 seconds total. Finally the newly built packages have to be loaded in the environment using source . 1 fjp@ubuntu:~/git/2wd-robot/ros$ source ~/git/2wd-robot/ros/devel/setup.bash # Load the workspace's environment","title":"Command Overview of catkin_tools"},{"location":"ros-setup/#resources","text":"Although the catkin tutorial uses catkin_make it provides a helpful guide to create a workspace","title":"Resources"},{"location":"teensy-mcu/","text":"Teensy Setup The Teensy 3.2 microcontroller (MCU) is used to get the ticks from the encoders attached to the motors and send this information (counts) as a message over the /diffbot/ticks_left and /diffbot/ticks_right ropics. For this rosserial is running on the Teensy MCU which allows it to create a node on the Teensy that can communicate with the ROS Master running on the Raspberry Pi. To setup rosserial on the work PC and the Raspberry Pi the following package has to be installed: 1 sudo apt install ros-noetic-rosserial To program the Teensy board with the work PC the Arduino IDE with the Teensyduino add-on can be used. Other options are to use PlatformIO plugin for VSCode. How to install the Arduino IDE and Teensyduino is listed in the instructions on the Teensy website . Here the instructions to setup Teensyduino in Linux are listed: Download the Linux udev rules (link at the top of this page) and copy the file to /etc/udev/rules.d. sudo cp 49-teensy.rules /etc/udev/rules.d/ Download and extract one of Arduino's Linux packages. Note: Arduino from Linux distro packages is not supported. Download the corresponding Teensyduino installer. Run the installer by adding execute permission and then execute it. chmod 755 TeensyduinoInstall.linux64 ./TeensyduinoInstall.linux64 The first step can be used on the work PC and the Raspberry Pi to enable the USB communication with the Teensy board. Step two of these instructions are only necessary on the work PC to actually program the Teensy board. Note Make sure to download the Arduino IDE from the website and don't install it from the Ubuntu repositories. The following video shows installation process, more instructions to setup the Arduino IDE can be found in the ROS wiki . To check if the connection to the Teensy board works use these commands on the Raspberry Pi: 1 2 3 4 5 $ lsusb Bus 002 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub Bus 001 Device 003: ID 16c0:0483 Van Ooijen Technische Informatica Teensyduino Serial Bus 001 Device 002: ID 2109:3431 VIA Labs, Inc. Hub Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub And to see on which serial port it is connected use: 1 2 $ ls /dev | grep ttyACM0 ttyACM0 If the output is empty it might be the case that the board is connected to another port like ttyUSB0 . Encoder Program When installing Teensyduino new example programs are provided. One of them is to test Encoders. The code for the motor encoders uses it as basis together with a pubsub example from rosserial: TODO link to code encoders.ino After the program is flashed to the Teensy board it can be tested with the following procedure: Start a ROS master by executing roscore in a new terminal. Create a rosserial node using rosserial_python package: 1 2 3 4 5 6 7 8 9 $ rosrun rosserial_python serial_node.py _port: = /dev/ttyACM0 _baud: = 115200 [INFO] [1602784903.659869]: ROS Serial Python Node [INFO] [1602784903.692366]: Connecting to /dev/ttyACM0 at 115200 baud [INFO] [1602784905.809722]: Requesting topics... [INFO] [1602784905.824418]: Note: publish buffer size is 512 bytes [INFO] [1602784905.829712]: Setup publisher on /diffbot/ticks_left [std_msgs/Int32] [INFO] [1602784905.839914]: Setup publisher on /diffbot/ticks_right [std_msgs/Int32] [INFO] [1602784905.856772]: Note: subscribe buffer size is 512 bytes [INFO] [1602784905.861749]: Setup subscriber on /reset [std_msgs/Empty] In case of the following error, probably the wrong program is flashed to the Teensy board: 1 [ERROR] [1602782376.724880]: Unable to sync with device; possible link problem or link software version mismatch such as hydro rosserial_python with groovy Arduino Note Note that the rosserial node needs to be stopped to flash new sketches to the Teensy board. Each DG01D-E motor has two signal pins for its built-in encoder. For these, the Teensy pins 5, 6 are used for the left encoder and 7, 8 are used for the right one, see also the Teensy pinout . Teensy 4.0 Pins. TODO add connection schematic With one motor encoder connected to pins 5, 6, echo the /encoder_ticks topic: 1 rostopic echo /encoder_ticks Rotating a wheel attached to the motor shaft 360 degree (one full turn) will increase the first value of the encoders array: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 --- header: seq: 190323 stamp: secs: 0 nsecs: 0 frame_id: '' encoders: [0, 0] --- header: seq: 190324 stamp: secs: 0 nsecs: 0 frame_id: '' encoders: [230, 0] --- header: seq: 190325 stamp: secs: 0 nsecs: 0 frame_id: '' encoders: [350, 0] --- header: seq: 190326 stamp: secs: 0 nsecs: 0 frame_id: '' encoders: [480, 0] --- header: seq: 190327 stamp: secs: 0 nsecs: 0 frame_id: '' encoders: [540, 0] The found value 540 for a full turn of the wheel is important for the hardware interface.","title":"Teensy MCU"},{"location":"teensy-mcu/#teensy-setup","text":"The Teensy 3.2 microcontroller (MCU) is used to get the ticks from the encoders attached to the motors and send this information (counts) as a message over the /diffbot/ticks_left and /diffbot/ticks_right ropics. For this rosserial is running on the Teensy MCU which allows it to create a node on the Teensy that can communicate with the ROS Master running on the Raspberry Pi. To setup rosserial on the work PC and the Raspberry Pi the following package has to be installed: 1 sudo apt install ros-noetic-rosserial To program the Teensy board with the work PC the Arduino IDE with the Teensyduino add-on can be used. Other options are to use PlatformIO plugin for VSCode. How to install the Arduino IDE and Teensyduino is listed in the instructions on the Teensy website . Here the instructions to setup Teensyduino in Linux are listed: Download the Linux udev rules (link at the top of this page) and copy the file to /etc/udev/rules.d. sudo cp 49-teensy.rules /etc/udev/rules.d/ Download and extract one of Arduino's Linux packages. Note: Arduino from Linux distro packages is not supported. Download the corresponding Teensyduino installer. Run the installer by adding execute permission and then execute it. chmod 755 TeensyduinoInstall.linux64 ./TeensyduinoInstall.linux64 The first step can be used on the work PC and the Raspberry Pi to enable the USB communication with the Teensy board. Step two of these instructions are only necessary on the work PC to actually program the Teensy board. Note Make sure to download the Arduino IDE from the website and don't install it from the Ubuntu repositories. The following video shows installation process, more instructions to setup the Arduino IDE can be found in the ROS wiki . To check if the connection to the Teensy board works use these commands on the Raspberry Pi: 1 2 3 4 5 $ lsusb Bus 002 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub Bus 001 Device 003: ID 16c0:0483 Van Ooijen Technische Informatica Teensyduino Serial Bus 001 Device 002: ID 2109:3431 VIA Labs, Inc. Hub Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub And to see on which serial port it is connected use: 1 2 $ ls /dev | grep ttyACM0 ttyACM0 If the output is empty it might be the case that the board is connected to another port like ttyUSB0 .","title":"Teensy Setup"},{"location":"teensy-mcu/#encoder-program","text":"When installing Teensyduino new example programs are provided. One of them is to test Encoders. The code for the motor encoders uses it as basis together with a pubsub example from rosserial: TODO link to code encoders.ino After the program is flashed to the Teensy board it can be tested with the following procedure: Start a ROS master by executing roscore in a new terminal. Create a rosserial node using rosserial_python package: 1 2 3 4 5 6 7 8 9 $ rosrun rosserial_python serial_node.py _port: = /dev/ttyACM0 _baud: = 115200 [INFO] [1602784903.659869]: ROS Serial Python Node [INFO] [1602784903.692366]: Connecting to /dev/ttyACM0 at 115200 baud [INFO] [1602784905.809722]: Requesting topics... [INFO] [1602784905.824418]: Note: publish buffer size is 512 bytes [INFO] [1602784905.829712]: Setup publisher on /diffbot/ticks_left [std_msgs/Int32] [INFO] [1602784905.839914]: Setup publisher on /diffbot/ticks_right [std_msgs/Int32] [INFO] [1602784905.856772]: Note: subscribe buffer size is 512 bytes [INFO] [1602784905.861749]: Setup subscriber on /reset [std_msgs/Empty] In case of the following error, probably the wrong program is flashed to the Teensy board: 1 [ERROR] [1602782376.724880]: Unable to sync with device; possible link problem or link software version mismatch such as hydro rosserial_python with groovy Arduino Note Note that the rosserial node needs to be stopped to flash new sketches to the Teensy board. Each DG01D-E motor has two signal pins for its built-in encoder. For these, the Teensy pins 5, 6 are used for the left encoder and 7, 8 are used for the right one, see also the Teensy pinout . Teensy 4.0 Pins. TODO add connection schematic With one motor encoder connected to pins 5, 6, echo the /encoder_ticks topic: 1 rostopic echo /encoder_ticks Rotating a wheel attached to the motor shaft 360 degree (one full turn) will increase the first value of the encoders array: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 --- header: seq: 190323 stamp: secs: 0 nsecs: 0 frame_id: '' encoders: [0, 0] --- header: seq: 190324 stamp: secs: 0 nsecs: 0 frame_id: '' encoders: [230, 0] --- header: seq: 190325 stamp: secs: 0 nsecs: 0 frame_id: '' encoders: [350, 0] --- header: seq: 190326 stamp: secs: 0 nsecs: 0 frame_id: '' encoders: [480, 0] --- header: seq: 190327 stamp: secs: 0 nsecs: 0 frame_id: '' encoders: [540, 0] The found value 540 for a full turn of the wheel is important for the hardware interface.","title":"Encoder Program"},{"location":"theory/actuation/","text":"Actuators Motor Encoder Gearbox Gearboxes can be used to fine-tune the performance characteristics of a motion axis. They're most commonly considered torque multipliers but actually serve several other functions, including speed matching, inertia reduction, and resolution increase ref .","title":"Actuation"},{"location":"theory/actuation/#actuators","text":"","title":"Actuators"},{"location":"theory/actuation/#motor","text":"","title":"Motor"},{"location":"theory/actuation/#encoder","text":"","title":"Encoder"},{"location":"theory/actuation/#gearbox","text":"Gearboxes can be used to fine-tune the performance characteristics of a motion axis. They're most commonly considered torque multipliers but actually serve several other functions, including speed matching, inertia reduction, and resolution increase ref .","title":"Gearbox"},{"location":"theory/motion-and-odometry/","text":"Robotic Motion and Odometry The following section describes the theory of robotic motion and odometry, which is part of the book Elements of Robotics . The section focuses on a detailed look on the quadrature encoders that are attached to the robot wheels. For DiffBot the encoders are part of the motors DG01D-E . This section reviews the basic concepts of distance, time, velocity and acceleration. The physics of motion can be described using calculus, but a computer cannot deal with continuous functions; instead, discrete approximations must be used. Odometry, the fundamental algorithm for computing robotic motion. An approximation of the location of a robot can be obtained by repeatedly computing the distance moved and the change direction from the velocity of the wheels in a short period of time. Unfortunately, odometry is subject to serious errors. It is important to understand that errors in direction are much more significant than errors in distance. See the following video explaining odometry: In the simplest implementation, the speed of the wheels of a robot is assumed to be proportional to the power applied by the motors. To improve the accuracy of odometry wheel encoders can be used, which measure the actual number of revolutions of the wheels. The following video from Sparkfun gives an overview of Encoders Distance, Velocity and Time In general, if a robot moves at a constant applied to the motors it causes the wheels to rotate, which in turn causes the robot velocity \\(v\\) for a period of time \\(t\\) , the distance \\(s\\) it moves is \\(s = v \\cdot t\\) . When power is to move at some velocity. However, we cannot specify that a certain power causes a certain velocity No two electrical or mechanical components are ever precisely identical. The environment affects the velocity of a robot because of different friction of the surface External forces can affect the velocity of a robot. It needs more power to sustain a specific velocity when moving uphill and less power when moving downhill, because the force of gravity decreases and increases the velocity. Note Velocity is speed in a direction. A robot can be moving 10cm/s forwards or backwards; in both cases, the speed is the same but the velocity is different. Acceleration as Change in Velocity To get a true picture of the motion of a robot, we need to divide its motion into small segments \\(s_1,s_2,\\dots\\) and measure the distance and time for each segment individually. Then, we can compute the velocities for each segment. In symbols, if we denote the length of the segment si by \\(\\Delta s_i = x_{i+1} \u2212 x_i\\) and the time it takes the robot to cross segment si by \\(\\Delta t_i = t_{i+1} \u2212 t_i\\) , then \\(v_i\\) , the velocity in segment \\(s_i\\) is given by: \\[ v_i = \\frac{\\Delta s_i}{\\Delta t_i} \\] Acceleration is defined as the change in velocity over a period of time \\[a_i = \\frac{\\Delta v_i}{\\Delta t_i}\\] References Kinematics equations for Differential Drive and Articulated Steering","title":"Motion and Odometry"},{"location":"theory/motion-and-odometry/#robotic-motion-and-odometry","text":"The following section describes the theory of robotic motion and odometry, which is part of the book Elements of Robotics . The section focuses on a detailed look on the quadrature encoders that are attached to the robot wheels. For DiffBot the encoders are part of the motors DG01D-E . This section reviews the basic concepts of distance, time, velocity and acceleration. The physics of motion can be described using calculus, but a computer cannot deal with continuous functions; instead, discrete approximations must be used. Odometry, the fundamental algorithm for computing robotic motion. An approximation of the location of a robot can be obtained by repeatedly computing the distance moved and the change direction from the velocity of the wheels in a short period of time. Unfortunately, odometry is subject to serious errors. It is important to understand that errors in direction are much more significant than errors in distance. See the following video explaining odometry: In the simplest implementation, the speed of the wheels of a robot is assumed to be proportional to the power applied by the motors. To improve the accuracy of odometry wheel encoders can be used, which measure the actual number of revolutions of the wheels. The following video from Sparkfun gives an overview of Encoders","title":"Robotic Motion and Odometry"},{"location":"theory/motion-and-odometry/#distance-velocity-and-time","text":"In general, if a robot moves at a constant applied to the motors it causes the wheels to rotate, which in turn causes the robot velocity \\(v\\) for a period of time \\(t\\) , the distance \\(s\\) it moves is \\(s = v \\cdot t\\) . When power is to move at some velocity. However, we cannot specify that a certain power causes a certain velocity No two electrical or mechanical components are ever precisely identical. The environment affects the velocity of a robot because of different friction of the surface External forces can affect the velocity of a robot. It needs more power to sustain a specific velocity when moving uphill and less power when moving downhill, because the force of gravity decreases and increases the velocity. Note Velocity is speed in a direction. A robot can be moving 10cm/s forwards or backwards; in both cases, the speed is the same but the velocity is different.","title":"Distance, Velocity and Time"},{"location":"theory/motion-and-odometry/#acceleration-as-change-in-velocity","text":"To get a true picture of the motion of a robot, we need to divide its motion into small segments \\(s_1,s_2,\\dots\\) and measure the distance and time for each segment individually. Then, we can compute the velocities for each segment. In symbols, if we denote the length of the segment si by \\(\\Delta s_i = x_{i+1} \u2212 x_i\\) and the time it takes the robot to cross segment si by \\(\\Delta t_i = t_{i+1} \u2212 t_i\\) , then \\(v_i\\) , the velocity in segment \\(s_i\\) is given by: \\[ v_i = \\frac{\\Delta s_i}{\\Delta t_i} \\] Acceleration is defined as the change in velocity over a period of time \\[a_i = \\frac{\\Delta v_i}{\\Delta t_i}\\]","title":"Acceleration as Change in Velocity"},{"location":"theory/motion-and-odometry/#references","text":"Kinematics equations for Differential Drive and Articulated Steering","title":"References"}]}